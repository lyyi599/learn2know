{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 22: Scaling Laws for Neural Language Models\n",
    "## Jared Kaplan et al. (2020)\n",
    "\n",
    "### Predictable Scaling: Loss as Function of Compute, Data, Parameters\n",
    "\n",
    "Empirical analysis showing power-law relationships in neural network scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Law Formulation\n",
    "\n",
    "Key finding: Loss follows power laws:\n",
    "$$L(N) = \\left(\\frac{N_c}{N}\\right)^{\\alpha_N}$$\n",
    "\n",
    "where:\n",
    "- N = number of parameters\n",
    "- D = dataset size\n",
    "- C = compute budget (FLOPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_law(x, a, b, c):\n",
    "    \"\"\"Power law: y = a * x^(-b) + c\"\"\"\n",
    "    return a * np.power(x, -b) + c\n",
    "\n",
    "def scaling_law_params(x, a, b):\n",
    "    \"\"\"Simplified: L = a * N^(-b)\"\"\"\n",
    "    return a * np.power(x, -b)\n",
    "\n",
    "# Theoretical scaling law constants (from paper)\n",
    "# These are approximate values from Kaplan et al.\n",
    "alpha_N = 0.076  # Parameters scaling exponent\n",
    "alpha_D = 0.095  # Data scaling exponent  \n",
    "alpha_C = 0.050  # Compute scaling exponent\n",
    "\n",
    "N_c = 8.8e13     # Critical parameter count\n",
    "D_c = 5.4e13     # Critical dataset size\n",
    "C_c = 3.1e8      # Critical compute\n",
    "\n",
    "print(\"Scaling Law Parameters (from paper):\")\n",
    "print(f\"  α_N (params): {alpha_N}\")\n",
    "print(f\"  α_D (data): {alpha_D}\")\n",
    "print(f\"  α_C (compute): {alpha_C}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Model Training at Different Scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLanguageModel:\n",
    "    \"\"\"\n",
    "    Toy language model to demonstrate scaling behavior\n",
    "    \"\"\"\n",
    "    def __init__(self, num_params, vocab_size=100, embed_dim=32):\n",
    "        self.num_params = num_params\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Calculate capacity from parameter count\n",
    "        self.capacity = np.log(num_params) / 10.0\n",
    "    \n",
    "    def train(self, dataset_size, num_steps):\n",
    "        \"\"\"\n",
    "        Simulate training and return final loss\n",
    "        \n",
    "        Loss decreases with:\n",
    "        - More parameters (more capacity)\n",
    "        - More data (better learning)\n",
    "        - More training (convergence)\n",
    "        \"\"\"\n",
    "        # Base loss (vocabulary perplexity)\n",
    "        base_loss = np.log(self.vocab_size)\n",
    "        \n",
    "        # Parameter scaling (more params = lower loss)\n",
    "        param_factor = 1.0 / (1.0 + self.capacity)\n",
    "        \n",
    "        # Data scaling (more data = lower loss)\n",
    "        data_factor = 1.0 / (1.0 + np.log(dataset_size) / 15.0)\n",
    "        \n",
    "        # Training convergence\n",
    "        train_factor = np.exp(-num_steps / 1000.0)\n",
    "        \n",
    "        # Combined loss with noise\n",
    "        loss = base_loss * param_factor * data_factor * (0.5 + 0.5 * train_factor)\n",
    "        loss += np.random.randn() * 0.05  # Add noise\n",
    "        \n",
    "        return max(loss, 1.0)  # Floor at 1.0\n",
    "\n",
    "print(\"Simple Language Model for scaling experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Scaling with Model Size (Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed dataset and training\n",
    "dataset_size = 100000\n",
    "num_steps = 1000\n",
    "\n",
    "# Vary model size\n",
    "param_counts = np.array([1e3, 5e3, 1e4, 5e4, 1e5, 5e5, 1e6, 5e6, 1e7])\n",
    "losses_by_params = []\n",
    "\n",
    "for N in param_counts:\n",
    "    model = SimpleLanguageModel(num_params=int(N))\n",
    "    loss = model.train(dataset_size, num_steps)\n",
    "    losses_by_params.append(loss)\n",
    "\n",
    "losses_by_params = np.array(losses_by_params)\n",
    "\n",
    "# Fit power law\n",
    "params_fit, _ = curve_fit(scaling_law_params, param_counts, losses_by_params)\n",
    "a_params, b_params = params_fit\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(param_counts, losses_by_params, 'o', markersize=10, label='Measured Loss')\n",
    "plt.loglog(param_counts, scaling_law_params(param_counts, *params_fit), \n",
    "           '--', linewidth=2, label=f'Power Law Fit: L ∝ N^{-b_params:.3f}')\n",
    "plt.xlabel('Number of Parameters (N)')\n",
    "plt.ylabel('Loss (L)')\n",
    "plt.title('Scaling Law: Loss vs Model Size')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nParameter Scaling:\")\n",
    "print(f\"  Fitted exponent: {b_params:.4f}\")\n",
    "print(f\"  Interpretation: Doubling params reduces loss by {(1 - 2**(-b_params))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Scaling with Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed model size and training\n",
    "num_params = 1e6\n",
    "num_steps = 1000\n",
    "\n",
    "# Vary dataset size\n",
    "dataset_sizes = np.array([1e3, 5e3, 1e4, 5e4, 1e5, 5e5, 1e6, 5e6, 1e7])\n",
    "losses_by_data = []\n",
    "\n",
    "for D in dataset_sizes:\n",
    "    model = SimpleLanguageModel(num_params=int(num_params))\n",
    "    loss = model.train(int(D), num_steps)\n",
    "    losses_by_data.append(loss)\n",
    "\n",
    "losses_by_data = np.array(losses_by_data)\n",
    "\n",
    "# Fit power law\n",
    "data_fit, _ = curve_fit(scaling_law_params, dataset_sizes, losses_by_data)\n",
    "a_data, b_data = data_fit\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(dataset_sizes, losses_by_data, 's', markersize=10, \n",
    "           color='orange', label='Measured Loss')\n",
    "plt.loglog(dataset_sizes, scaling_law_params(dataset_sizes, *data_fit), \n",
    "           '--', linewidth=2, color='red', label=f'Power Law Fit: L ∝ D^{-b_data:.3f}')\n",
    "plt.xlabel('Dataset Size (D)')\n",
    "plt.ylabel('Loss (L)')\n",
    "plt.title('Scaling Law: Loss vs Dataset Size')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nDataset Scaling:\")\n",
    "print(f\"  Fitted exponent: {b_data:.4f}\")\n",
    "print(f\"  Interpretation: Doubling data reduces loss by {(1 - 2**(-b_data))*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Compute-Optimal Training\n",
    "\n",
    "Chinchilla finding: For a given compute budget, scale model and data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute budget (in arbitrary units)\n",
    "compute_budgets = np.array([1e6, 5e6, 1e7, 5e7, 1e8, 5e8, 1e9])\n",
    "\n",
    "# For each compute budget, find optimal N and D allocation\n",
    "optimal_results = []\n",
    "\n",
    "for C in compute_budgets:\n",
    "    # Chinchilla: N and D should scale equally with compute\n",
    "    # C ≈ 6 * N * D (6 FLOPs per parameter per token)\n",
    "    # Optimal: N ∝ C^0.5, D ∝ C^0.5\n",
    "    \n",
    "    N_opt = int(np.sqrt(C / 6))\n",
    "    D_opt = int(np.sqrt(C / 6))\n",
    "    \n",
    "    model = SimpleLanguageModel(num_params=N_opt)\n",
    "    loss = model.train(D_opt, num_steps=1000)\n",
    "    \n",
    "    optimal_results.append({\n",
    "        'compute': C,\n",
    "        'params': N_opt,\n",
    "        'data': D_opt,\n",
    "        'loss': loss\n",
    "    })\n",
    "\n",
    "compute_vals = [r['compute'] for r in optimal_results]\n",
    "losses_optimal = [r['loss'] for r in optimal_results]\n",
    "\n",
    "# Fit\n",
    "compute_fit, _ = curve_fit(scaling_law_params, compute_vals, losses_optimal)\n",
    "a_compute, b_compute = compute_fit\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Loss vs Compute\n",
    "ax1.loglog(compute_vals, losses_optimal, '^', markersize=10, \n",
    "           color='green', label='Measured Loss')\n",
    "ax1.loglog(compute_vals, scaling_law_params(compute_vals, *compute_fit), \n",
    "           '--', linewidth=2, color='darkgreen', \n",
    "           label=f'Power Law Fit: L ∝ C^{-b_compute:.3f}')\n",
    "ax1.set_xlabel('Compute Budget (C)')\n",
    "ax1.set_ylabel('Loss (L)')\n",
    "ax1.set_title('Scaling Law: Loss vs Compute (Optimal Allocation)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# Optimal N and D vs Compute\n",
    "params_vals = [r['params'] for r in optimal_results]\n",
    "data_vals = [r['data'] for r in optimal_results]\n",
    "\n",
    "ax2.loglog(compute_vals, params_vals, 'o-', label='Optimal N (params)', linewidth=2)\n",
    "ax2.loglog(compute_vals, data_vals, 's-', label='Optimal D (data)', linewidth=2)\n",
    "ax2.set_xlabel('Compute Budget (C)')\n",
    "ax2.set_ylabel('N or D')\n",
    "ax2.set_title('Compute-Optimal Scaling: N ∝ C^0.5, D ∝ C^0.5')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCompute-Optimal Scaling:\")\n",
    "print(f\"  Loss exponent: {b_compute:.4f}\")\n",
    "print(f\"  For 10x more compute, loss reduces by {(1 - 10**(-b_compute))*100:.1f}%\")\n",
    "print(f\"\\n  Chinchilla insight: Scale model AND data together!\")\n",
    "print(f\"  N_optimal ∝ C^0.5\")\n",
    "print(f\"  D_optimal ∝ C^0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Different Scaling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare strategies for same compute budget\n",
    "C = 1e8\n",
    "\n",
    "# Strategy 1: Large model, small data\n",
    "N_large = int(C / 1000)\n",
    "D_small = 1000\n",
    "model_large = SimpleLanguageModel(num_params=N_large)\n",
    "loss_large_model = model_large.train(D_small, 1000)\n",
    "\n",
    "# Strategy 2: Small model, large data\n",
    "N_small = 1000\n",
    "D_large = int(C / 1000)\n",
    "model_small = SimpleLanguageModel(num_params=N_small)\n",
    "loss_small_model = model_small.train(D_large, 1000)\n",
    "\n",
    "# Strategy 3: Balanced (Chinchilla)\n",
    "N_balanced = int(np.sqrt(C / 6))\n",
    "D_balanced = int(np.sqrt(C / 6))\n",
    "model_balanced = SimpleLanguageModel(num_params=N_balanced)\n",
    "loss_balanced = model_balanced.train(D_balanced, 1000)\n",
    "\n",
    "# Visualize\n",
    "strategies = ['Large Model\\nSmall Data', 'Small Model\\nLarge Data', 'Balanced\\n(Chinchilla)']\n",
    "losses = [loss_large_model, loss_small_model, loss_balanced]\n",
    "colors = ['red', 'orange', 'green']\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss comparison\n",
    "ax1.bar(strategies, losses, color=colors, alpha=0.7)\n",
    "ax1.set_ylabel('Final Loss')\n",
    "ax1.set_title(f'Training Strategies (Same Compute Budget: {C:.0e})')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Resource allocation\n",
    "x = np.arange(3)\n",
    "width = 0.35\n",
    "\n",
    "params = [N_large, N_small, N_balanced]\n",
    "data = [D_small, D_large, D_balanced]\n",
    "\n",
    "ax2.bar(x - width/2, np.log10(params), width, label='log₁₀(Params)', alpha=0.7)\n",
    "ax2.bar(x + width/2, np.log10(data), width, label='log₁₀(Data)', alpha=0.7)\n",
    "ax2.set_ylabel('log₁₀(Count)')\n",
    "ax2.set_title('Resource Allocation')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(strategies)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nStrategy Comparison (Compute = {C:.0e}):\")\n",
    "print(f\"\\n1. Large Model (N={N_large:.0e}), Small Data (D={D_small:.0e}):\")\n",
    "print(f\"   Loss = {loss_large_model:.4f}\")\n",
    "print(f\"\\n2. Small Model (N={N_small:.0e}), Large Data (D={D_large:.0e}):\")\n",
    "print(f\"   Loss = {loss_small_model:.4f}\")\n",
    "print(f\"\\n3. Balanced (N={N_balanced:.0e}), (D={D_balanced:.0e}):\")\n",
    "print(f\"   Loss = {loss_balanced:.4f} ← BEST\")\n",
    "print(f\"\\nKey Insight: Balanced scaling is compute-optimal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extrapolation: Predict Larger Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fitted scaling laws to predict performance of future models\n",
    "future_params = np.array([1e8, 1e9, 1e10, 1e11, 1e12])  # 100M to 1T params\n",
    "predicted_losses = scaling_law_params(future_params, *params_fit)\n",
    "\n",
    "# Plot extrapolation\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Historical data\n",
    "plt.loglog(param_counts, losses_by_params, 'o', markersize=10, \n",
    "           label='Measured (smaller models)', color='blue')\n",
    "\n",
    "# Fitted curve\n",
    "extended_params = np.logspace(3, 12, 100)\n",
    "plt.loglog(extended_params, scaling_law_params(extended_params, *params_fit), \n",
    "           '--', linewidth=2, label='Power Law Extrapolation', color='blue', alpha=0.5)\n",
    "\n",
    "# Future predictions\n",
    "plt.loglog(future_params, predicted_losses, 's', markersize=12, \n",
    "           label='Predicted (larger models)', color='red', zorder=5)\n",
    "\n",
    "# Annotate famous model sizes\n",
    "famous_models = [\n",
    "    (1.5e8, 'GPT-2'),\n",
    "    (1.75e9, 'GPT-3'),\n",
    "    (1.75e11, 'GPT-3.5'),\n",
    "]\n",
    "\n",
    "for params, name in famous_models:\n",
    "    loss_pred = scaling_law_params(params, *params_fit)\n",
    "    plt.plot(params, loss_pred, 'r*', markersize=15)\n",
    "    plt.annotate(name, (params, loss_pred), \n",
    "                xytext=(10, 10), textcoords='offset points', fontsize=10)\n",
    "\n",
    "plt.xlabel('Number of Parameters (N)')\n",
    "plt.ylabel('Predicted Loss (L)')\n",
    "plt.title('Scaling Law Extrapolation to Larger Models')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, which='both')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPredicted Performance:\")\n",
    "for N, L in zip(future_params, predicted_losses):\n",
    "    print(f\"  {N:.0e} params → Loss = {L:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Main Findings (Kaplan et al. 2020):\n",
    "\n",
    "1. **Power Law Scaling**: Loss follows power laws with N, D, C\n",
    "   - L(N) ∝ N^(-α_N)\n",
    "   - L(D) ∝ D^(-α_D)\n",
    "   - L(C) ∝ C^(-α_C)\n",
    "\n",
    "2. **Smooth & Predictable**: Can extrapolate across 7+ orders of magnitude\n",
    "\n",
    "3. **Early Stopping**: Optimal training stops before convergence\n",
    "\n",
    "4. **Transfer**: Scaling laws transfer across tasks\n",
    "\n",
    "### Chinchilla Findings (Hoffmann et al. 2022):\n",
    "\n",
    "1. **Compute-Optimal**: For budget C, use\n",
    "   - N ∝ C^0.5\n",
    "   - D ∝ C^0.5\n",
    "   \n",
    "2. **Previous models were under-trained**: \n",
    "   - GPT-3: 175B params, 300B tokens\n",
    "   - Optimal: 70B params, 1.4T tokens (Chinchilla)\n",
    "\n",
    "3. **Data matters as much as parameters**\n",
    "\n",
    "### Practical Implications:\n",
    "\n",
    "1. **Resource Allocation**: Balance model size and training data\n",
    "2. **Performance Prediction**: Estimate SOTA before training\n",
    "3. **Research Planning**: Know where gains will come from\n",
    "4. **Cost Optimization**: Avoid over-parameterization\n",
    "\n",
    "### Scaling Law Exponents:\n",
    "- **Parameters**: α_N ≈ 0.076\n",
    "- **Data**: α_D ≈ 0.095  \n",
    "- **Compute**: α_C ≈ 0.050\n",
    "\n",
    "### Why Power Laws?\n",
    "- Underlying statistical structure of language\n",
    "- Consistent with information theory\n",
    "- Reflects learning difficulty at different scales\n",
    "\n",
    "### Future Directions:\n",
    "- Scaling to multi-modal models\n",
    "- Architectural innovations (MoE, etc.)\n",
    "- Data quality vs quantity\n",
    "- Emergent capabilities at scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
