{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 5: Keeping Neural Networks Simple by Minimizing the Description Length\n",
    "## Hinton & Van Camp (1993) + Modern Pruning Techniques\n",
    "\n",
    "### Network Pruning & Compression\n",
    "\n",
    "Key insight: Remove unnecessary weights to get simpler, more generalizable networks. Smaller = better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "class SimpleNN:\n",
    "    \"\"\"Simple 2-layer neural network\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.1\n",
    "        self.b1 = np.zeros(hidden_dim)\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) * 0.1\n",
    "        self.b2 = np.zeros(output_dim)\n",
    "        \n",
    "        # Keep track of masks for pruning\n",
    "        self.mask1 = np.ones_like(self.W1)\n",
    "        self.mask2 = np.ones_like(self.W2)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # Apply masks (for pruned weights)\n",
    "        W1_masked = self.W1 * self.mask1\n",
    "        W2_masked = self.W2 * self.mask2\n",
    "        \n",
    "        # Hidden layer\n",
    "        self.h = relu(np.dot(X, W1_masked) + self.b1)\n",
    "        \n",
    "        # Output layer\n",
    "        logits = np.dot(self.h, W2_masked) + self.b2\n",
    "        probs = softmax(logits)\n",
    "        \n",
    "        return probs\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\"\"\"\n",
    "        probs = self.forward(X)\n",
    "        return np.argmax(probs, axis=1)\n",
    "    \n",
    "    def accuracy(self, X, y):\n",
    "        \"\"\"Compute accuracy\"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        \"\"\"Count total and active (non-pruned) parameters\"\"\"\n",
    "        total = self.W1.size + self.b1.size + self.W2.size + self.b2.size\n",
    "        active = int(np.sum(self.mask1) + self.b1.size + np.sum(self.mask2) + self.b2.size)\n",
    "        return total, active\n",
    "\n",
    "# Test network\n",
    "nn = SimpleNN(input_dim=10, hidden_dim=20, output_dim=3)\n",
    "X_test = np.random.randn(5, 10)\n",
    "y_test = nn.forward(X_test)\n",
    "print(f\"Network output shape: {y_test.shape}\")\n",
    "total, active = nn.count_parameters()\n",
    "print(f\"Parameters: {total} total, {active} active\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_classification_data(n_samples=1000, n_features=20, n_classes=3):\n",
    "    \"\"\"\n",
    "    Generate synthetic classification dataset\n",
    "    Each class is a Gaussian blob\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    samples_per_class = n_samples // n_classes\n",
    "    \n",
    "    for c in range(n_classes):\n",
    "        # Random center for this class\n",
    "        center = np.random.randn(n_features) * 3\n",
    "        \n",
    "        # Generate samples around center\n",
    "        X_class = np.random.randn(samples_per_class, n_features) + center\n",
    "        y_class = np.full(samples_per_class, c)\n",
    "        \n",
    "        X.append(X_class)\n",
    "        y.append(y_class)\n",
    "    \n",
    "    X = np.vstack(X)\n",
    "    y = np.concatenate(y)\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.random.permutation(len(X))\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X_train, y_train = generate_classification_data(n_samples=1000, n_features=20, n_classes=3)\n",
    "X_test, y_test = generate_classification_data(n_samples=300, n_features=20, n_classes=3)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}, {y_test.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Baseline Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, X_train, y_train, X_test, y_test, epochs=100, lr=0.01):\n",
    "    \"\"\"\n",
    "    Simple training loop\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Forward pass\n",
    "        probs = model.forward(X_train)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        y_one_hot = np.zeros((len(y_train), model.output_dim))\n",
    "        y_one_hot[np.arange(len(y_train)), y_train] = 1\n",
    "        loss = -np.mean(np.sum(y_one_hot * np.log(probs + 1e-8), axis=1))\n",
    "        \n",
    "        # Backward pass (simplified)\n",
    "        batch_size = len(X_train)\n",
    "        dL_dlogits = (probs - y_one_hot) / batch_size\n",
    "        \n",
    "        # Gradients for W2, b2\n",
    "        dL_dW2 = np.dot(model.h.T, dL_dlogits)\n",
    "        dL_db2 = np.sum(dL_dlogits, axis=0)\n",
    "        \n",
    "        # Gradients for W1, b1\n",
    "        dL_dh = np.dot(dL_dlogits, (model.W2 * model.mask2).T)\n",
    "        dL_dh[model.h <= 0] = 0  # ReLU derivative\n",
    "        dL_dW1 = np.dot(X_train.T, dL_dh)\n",
    "        dL_db1 = np.sum(dL_dh, axis=0)\n",
    "        \n",
    "        # Update weights (only where mask is active)\n",
    "        model.W1 -= lr * dL_dW1 * model.mask1\n",
    "        model.b1 -= lr * dL_db1\n",
    "        model.W2 -= lr * dL_dW2 * model.mask2\n",
    "        model.b2 -= lr * dL_db2\n",
    "        \n",
    "        # Track metrics\n",
    "        train_losses.append(loss)\n",
    "        test_acc = model.accuracy(X_test, y_test)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}, Test Acc: {test_acc:.2%}\")\n",
    "    \n",
    "    return train_losses, test_accuracies\n",
    "\n",
    "# Train baseline model\n",
    "print(\"Training baseline network...\\n\")\n",
    "baseline_model = SimpleNN(input_dim=20, hidden_dim=50, output_dim=3)\n",
    "train_losses, test_accs = train_network(baseline_model, X_train, y_train, X_test, y_test, epochs=100)\n",
    "\n",
    "baseline_acc = baseline_model.accuracy(X_test, y_test)\n",
    "total_params, active_params = baseline_model.count_parameters()\n",
    "print(f\"\\nBaseline: {baseline_acc:.2%} accuracy, {active_params} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnitude-Based Pruning\n",
    "\n",
    "Remove weights with smallest absolute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune_by_magnitude(model, pruning_rate):\n",
    "    \"\"\"\n",
    "    Prune weights with smallest magnitudes\n",
    "    \n",
    "    pruning_rate: fraction of weights to remove (0-1)\n",
    "    \"\"\"\n",
    "    # Collect all weights\n",
    "    all_weights = np.concatenate([model.W1.flatten(), model.W2.flatten()])\n",
    "    all_magnitudes = np.abs(all_weights)\n",
    "    \n",
    "    # Find threshold\n",
    "    threshold = np.percentile(all_magnitudes, pruning_rate * 100)\n",
    "    \n",
    "    # Create new masks\n",
    "    model.mask1 = (np.abs(model.W1) > threshold).astype(float)\n",
    "    model.mask2 = (np.abs(model.W2) > threshold).astype(float)\n",
    "    \n",
    "    print(f\"Pruning threshold: {threshold:.6f}\")\n",
    "    print(f\"Pruned {pruning_rate:.1%} of weights\")\n",
    "    \n",
    "    total, active = model.count_parameters()\n",
    "    print(f\"Remaining parameters: {active}/{total} ({active/total:.1%})\")\n",
    "\n",
    "# Test pruning\n",
    "import copy\n",
    "pruned_model = copy.deepcopy(baseline_model)\n",
    "\n",
    "print(\"Before pruning:\")\n",
    "acc_before = pruned_model.accuracy(X_test, y_test)\n",
    "print(f\"Accuracy: {acc_before:.2%}\\n\")\n",
    "\n",
    "print(\"Pruning 50% of weights...\")\n",
    "prune_by_magnitude(pruned_model, pruning_rate=0.5)\n",
    "\n",
    "print(\"\\nAfter pruning (before retraining):\")\n",
    "acc_after = pruned_model.accuracy(X_test, y_test)\n",
    "print(f\"Accuracy: {acc_after:.2%}\")\n",
    "print(f\"Accuracy drop: {(acc_before - acc_after):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning After Pruning\n",
    "\n",
    "Retrain remaining weights to recover accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fine-tuning pruned network...\\n\")\n",
    "finetune_losses, finetune_accs = train_network(\n",
    "    pruned_model, X_train, y_train, X_test, y_test, epochs=50, lr=0.005\n",
    ")\n",
    "\n",
    "acc_finetuned = pruned_model.accuracy(X_test, y_test)\n",
    "total, active = pruned_model.count_parameters()\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RESULTS:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Baseline:     {baseline_acc:.2%} accuracy, {total_params} params\")\n",
    "print(f\"Pruned 50%:   {acc_finetuned:.2%} accuracy, {active} params\")\n",
    "print(f\"Compression:  {total_params/active:.1f}x smaller\")\n",
    "print(f\"Acc. change:  {(acc_finetuned - baseline_acc):+.2%}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Pruning\n",
    "\n",
    "Gradually increase pruning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_pruning(model, X_train, y_train, X_test, y_test, \n",
    "                     target_sparsity=0.9, num_iterations=5):\n",
    "    \"\"\"\n",
    "    Iteratively prune and finetune\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Initial state\n",
    "    total, active = model.count_parameters()\n",
    "    acc = model.accuracy(X_test, y_test)\n",
    "    results.append({\n",
    "        'iteration': 0,\n",
    "        'sparsity': 0.0,\n",
    "        'active_params': active,\n",
    "        'accuracy': acc\n",
    "    })\n",
    "    \n",
    "    # Gradually increase sparsity\n",
    "    for i in range(num_iterations):\n",
    "        # Sparsity for this iteration\n",
    "        current_sparsity = target_sparsity * (i + 1) / num_iterations\n",
    "        \n",
    "        print(f\"\\nIteration {i+1}/{num_iterations}: Target sparsity {current_sparsity:.1%}\")\n",
    "        \n",
    "        # Prune\n",
    "        prune_by_magnitude(model, pruning_rate=current_sparsity)\n",
    "        \n",
    "        # Finetune\n",
    "        train_network(model, X_train, y_train, X_test, y_test, epochs=30, lr=0.005)\n",
    "        \n",
    "        # Record results\n",
    "        total, active = model.count_parameters()\n",
    "        acc = model.accuracy(X_test, y_test)\n",
    "        results.append({\n",
    "            'iteration': i + 1,\n",
    "            'sparsity': current_sparsity,\n",
    "            'active_params': active,\n",
    "            'accuracy': acc\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run iterative pruning\n",
    "iterative_model = copy.deepcopy(baseline_model)\n",
    "results = iterative_pruning(iterative_model, X_train, y_train, X_test, y_test, \n",
    "                           target_sparsity=0.95, num_iterations=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Pruning Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "sparsities = [r['sparsity'] for r in results]\n",
    "accuracies = [r['accuracy'] for r in results]\n",
    "active_params = [r['active_params'] for r in results]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy vs Sparsity\n",
    "ax1.plot(sparsities, accuracies, 'o-', linewidth=2, markersize=10, color='steelblue')\n",
    "ax1.axhline(y=baseline_acc, color='red', linestyle='--', linewidth=2, label='Baseline')\n",
    "ax1.set_xlabel('Sparsity (Fraction Pruned)', fontsize=12)\n",
    "ax1.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax1.set_title('Accuracy vs Sparsity', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.legend(fontsize=11)\n",
    "ax1.set_ylim([0, 1])\n",
    "\n",
    "# Parameters vs Accuracy\n",
    "ax2.plot(active_params, accuracies, 's-', linewidth=2, markersize=10, color='darkgreen')\n",
    "ax2.axhline(y=baseline_acc, color='red', linestyle='--', linewidth=2, label='Baseline')\n",
    "ax2.set_xlabel('Active Parameters', fontsize=12)\n",
    "ax2.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax2.set_title('Accuracy vs Model Size', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.legend(fontsize=11)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.invert_xaxis()  # Fewer params on right\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey observation: Can remove 90%+ of weights with minimal accuracy loss!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Weight Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Baseline weights\n",
    "axes[0, 0].hist(baseline_model.W1.flatten(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Baseline W1 Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Weight Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].hist(baseline_model.W2.flatten(), bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('Baseline W2 Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Weight Value')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Pruned weights (only active)\n",
    "pruned_W1 = iterative_model.W1[iterative_model.mask1 > 0]\n",
    "pruned_W2 = iterative_model.W2[iterative_model.mask2 > 0]\n",
    "\n",
    "axes[1, 0].hist(pruned_W1.flatten(), bins=50, color='darkgreen', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_title('Pruned W1 Distribution (Active Weights Only)', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Weight Value')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].hist(pruned_W2.flatten(), bins=50, color='darkgreen', alpha=0.7, edgecolor='black')\n",
    "axes[1, 1].set_title('Pruned W2 Distribution (Active Weights Only)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Weight Value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Pruned weights have larger magnitudes (small weights removed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sparsity Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# W1 sparsity pattern\n",
    "im1 = ax1.imshow(iterative_model.mask1.T, cmap='RdYlGn', aspect='auto', interpolation='nearest')\n",
    "ax1.set_xlabel('Input Dimension', fontsize=12)\n",
    "ax1.set_ylabel('Hidden Dimension', fontsize=12)\n",
    "ax1.set_title('W1 Sparsity Pattern (Green=Active, Red=Pruned)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# W2 sparsity pattern\n",
    "im2 = ax2.imshow(iterative_model.mask2.T, cmap='RdYlGn', aspect='auto', interpolation='nearest')\n",
    "ax2.set_xlabel('Hidden Dimension', fontsize=12)\n",
    "ax2.set_ylabel('Output Dimension', fontsize=12)\n",
    "ax2.set_title('W2 Sparsity Pattern (Green=Active, Red=Pruned)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "total, active = iterative_model.count_parameters()\n",
    "print(f\"\\nFinal sparsity: {(total - active) / total:.1%}\")\n",
    "print(f\"Compression ratio: {total / active:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDL Principle\n",
    "\n",
    "Minimum Description Length: Simpler models generalize better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mdl(model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Simplified MDL computation\n",
    "    \n",
    "    MDL = Model Cost + Data Cost\n",
    "    - Model Cost: Bits to encode weights\n",
    "    - Data Cost: Bits to encode errors\n",
    "    \"\"\"\n",
    "    # Model cost: number of parameters (simplified)\n",
    "    total, active = model.count_parameters()\n",
    "    model_cost = active  # Each param = 1 \"bit\" (simplified)\n",
    "    \n",
    "    # Data cost: cross-entropy loss\n",
    "    probs = model.forward(X_train)\n",
    "    y_one_hot = np.zeros((len(y_train), model.output_dim))\n",
    "    y_one_hot[np.arange(len(y_train)), y_train] = 1\n",
    "    data_cost = -np.sum(y_one_hot * np.log(probs + 1e-8))\n",
    "    \n",
    "    total_cost = model_cost + data_cost\n",
    "    \n",
    "    return {\n",
    "        'model_cost': model_cost,\n",
    "        'data_cost': data_cost,\n",
    "        'total_cost': total_cost\n",
    "    }\n",
    "\n",
    "# Compare MDL for different models\n",
    "baseline_mdl = compute_mdl(baseline_model, X_train, y_train)\n",
    "pruned_mdl = compute_mdl(iterative_model, X_train, y_train)\n",
    "\n",
    "print(\"MDL Comparison:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"{'Model':<20} {'Model Cost':<15} {'Data Cost':<15} {'Total'}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'Baseline':<20} {baseline_mdl['model_cost']:<15.0f} {baseline_mdl['data_cost']:<15.2f} {baseline_mdl['total_cost']:.2f}\")\n",
    "print(f\"{'Pruned (95%)':<20} {pruned_mdl['model_cost']:<15.0f} {pruned_mdl['data_cost']:<15.2f} {pruned_mdl['total_cost']:.2f}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nPruned model has LOWER total cost → Better generalization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Neural Network Pruning:\n",
    "\n",
    "**Core Idea**: Remove unnecessary weights to create simpler, smaller networks\n",
    "\n",
    "### Magnitude-Based Pruning:\n",
    "\n",
    "1. **Train** network normally\n",
    "2. **Identify** low-magnitude weights: $|w| < \\text{threshold}$\n",
    "3. **Remove** these weights (set to 0, mask out)\n",
    "4. **Fine-tune** remaining weights\n",
    "\n",
    "### Iterative Pruning:\n",
    "\n",
    "Better than one-shot:\n",
    "```\n",
    "for iteration in 1..N:\n",
    "    prune small fraction (e.g., 20%)\n",
    "    finetune\n",
    "```\n",
    "\n",
    "Allows network to adapt gradually.\n",
    "\n",
    "### Results (Typical):\n",
    "\n",
    "- **50% sparsity**: Usually no accuracy loss\n",
    "- **90% sparsity**: Slight accuracy loss (<2%)\n",
    "- **95%+ sparsity**: Noticeable degradation\n",
    "\n",
    "Modern networks (ResNets, Transformers) can often be pruned to **90-95% sparsity** with minimal impact!\n",
    "\n",
    "### MDL Principle:\n",
    "\n",
    "$$\n",
    "\\text{MDL} = \\underbrace{L(\\text{Model})}_\\text{complexity} + \\underbrace{L(\\text{Data | Model})}_\\text{errors}\n",
    "$$\n",
    "\n",
    "**Occam's Razor**: Simplest explanation (smallest network) that fits data is best.\n",
    "\n",
    "### Benefits of Pruning:\n",
    "\n",
    "1. **Smaller models**: Less memory, faster inference\n",
    "2. **Better generalization**: Removing overfitting parameters\n",
    "3. **Energy efficiency**: Fewer operations\n",
    "4. **Interpretability**: Simpler structure\n",
    "\n",
    "### Types of Pruning:\n",
    "\n",
    "| Type | What's Removed | Speedup |\n",
    "|------|----------------|----------|\n",
    "| **Unstructured** | Individual weights | Low (sparse ops) |\n",
    "| **Structured** | Entire neurons/filters | High (dense ops) |\n",
    "| **Channel** | Entire channels | High |\n",
    "| **Layer** | Entire layers | Very High |\n",
    "\n",
    "### Modern Techniques:\n",
    "\n",
    "1. **Lottery Ticket Hypothesis**: \n",
    "   - Pruned networks can be retrained from initialization\n",
    "   - \"Winning tickets\" exist in random init\n",
    "\n",
    "2. **Dynamic Sparse Training**:\n",
    "   - Prune during training (not after)\n",
    "   - Regrow connections\n",
    "\n",
    "3. **Magnitude + Gradient**:\n",
    "   - Use gradient info, not just magnitude\n",
    "   - Remove weights with small magnitude AND small gradient\n",
    "\n",
    "4. **Learnable Sparsity**:\n",
    "   - L0/L1 regularization\n",
    "   - Automatic sparsity discovery\n",
    "\n",
    "### Practical Tips:\n",
    "\n",
    "1. **Start high, prune gradually**: Don't prune 90% immediately\n",
    "2. **Fine-tune after pruning**: Critical for recovery\n",
    "3. **Layer-wise pruning rates**: Different layers have different redundancy\n",
    "4. **Structured pruning for speed**: Unstructured needs special hardware\n",
    "\n",
    "### When to Prune:\n",
    "\n",
    "✅ **Good for**:\n",
    "- Deployment (edge devices, mobile)\n",
    "- Reducing inference cost\n",
    "- Model compression\n",
    "\n",
    "❌ **Not ideal for**:\n",
    "- Very small models (already efficient)\n",
    "- Training speedup (structured pruning only)\n",
    "\n",
    "### Compression Rates in Practice:\n",
    "\n",
    "- **AlexNet**: 9x compression (no accuracy loss)\n",
    "- **VGG-16**: 13x compression\n",
    "- **ResNet-50**: 5-7x compression\n",
    "- **BERT**: 10-40x compression (with quantization)\n",
    "\n",
    "### Key Insight:\n",
    "\n",
    "**Neural networks are massively over-parameterized!**\n",
    "\n",
    "Most weights contribute little to final performance. Pruning reveals the \"core\" network that does the real work.\n",
    "\n",
    "**\"The best model is the simplest one that fits the data\"** - MDL Principle"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
