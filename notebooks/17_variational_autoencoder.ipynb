{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 17: Variational Lossy Autoencoder\n",
    "## Xi Chen, Diederik P. Kingma, et al. (2016)\n",
    "\n",
    "### VAE: Generative Model with Learned Latent Space\n",
    "\n",
    "Combines deep learning with variational inference for generative modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE) Basics\n",
    "\n",
    "VAE learns:\n",
    "- **Encoder**: q(z|x) - approximate posterior\n",
    "- **Decoder**: p(x|z) - generative model\n",
    "\n",
    "**Loss**: ELBO = Reconstruction Loss + KL Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "class VAE:\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Encoder: x -> h -> (mu, log_var)\n",
    "        self.W_enc_h = np.random.randn(input_dim, hidden_dim) * 0.1\n",
    "        self.b_enc_h = np.zeros(hidden_dim)\n",
    "        \n",
    "        self.W_mu = np.random.randn(hidden_dim, latent_dim) * 0.1\n",
    "        self.b_mu = np.zeros(latent_dim)\n",
    "        \n",
    "        self.W_logvar = np.random.randn(hidden_dim, latent_dim) * 0.1\n",
    "        self.b_logvar = np.zeros(latent_dim)\n",
    "        \n",
    "        # Decoder: z -> h -> x_recon\n",
    "        self.W_dec_h = np.random.randn(latent_dim, hidden_dim) * 0.1\n",
    "        self.b_dec_h = np.zeros(hidden_dim)\n",
    "        \n",
    "        self.W_recon = np.random.randn(hidden_dim, input_dim) * 0.1\n",
    "        self.b_recon = np.zeros(input_dim)\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encode input to latent distribution parameters\n",
    "        \n",
    "        Returns: mu, log_var of q(z|x)\n",
    "        \"\"\"\n",
    "        h = relu(np.dot(x, self.W_enc_h) + self.b_enc_h)\n",
    "        mu = np.dot(h, self.W_mu) + self.b_mu\n",
    "        log_var = np.dot(h, self.W_logvar) + self.b_logvar\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = mu + sigma * epsilon\n",
    "        where epsilon ~ N(0, I)\n",
    "        \"\"\"\n",
    "        std = np.exp(0.5 * log_var)\n",
    "        epsilon = np.random.randn(*mu.shape)\n",
    "        z = mu + std * epsilon\n",
    "        return z\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"\n",
    "        Decode latent code to reconstruction\n",
    "        \n",
    "        Returns: reconstructed x\n",
    "        \"\"\"\n",
    "        h = relu(np.dot(z, self.W_dec_h) + self.b_dec_h)\n",
    "        x_recon = sigmoid(np.dot(h, self.W_recon) + self.b_recon)\n",
    "        return x_recon\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Full forward pass\n",
    "        \"\"\"\n",
    "        # Encode\n",
    "        mu, log_var = self.encode(x)\n",
    "        \n",
    "        # Sample latent\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        \n",
    "        # Decode\n",
    "        x_recon = self.decode(z)\n",
    "        \n",
    "        return x_recon, mu, log_var, z\n",
    "    \n",
    "    def loss(self, x, x_recon, mu, log_var):\n",
    "        \"\"\"\n",
    "        VAE loss = Reconstruction Loss + KL Divergence\n",
    "        \"\"\"\n",
    "        # Reconstruction loss (binary cross-entropy)\n",
    "        recon_loss = -np.sum(\n",
    "            x * np.log(x_recon + 1e-8) + \n",
    "            (1 - x) * np.log(1 - x_recon + 1e-8)\n",
    "        )\n",
    "        \n",
    "        # KL divergence: KL(q(z|x) || p(z))\n",
    "        # where p(z) = N(0, I)\n",
    "        # KL = -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        kl_loss = -0.5 * np.sum(1 + log_var - mu**2 - np.exp(log_var))\n",
    "        \n",
    "        return recon_loss + kl_loss, recon_loss, kl_loss\n",
    "\n",
    "# Create VAE\n",
    "input_dim = 16  # e.g., 4x4 image flattened\n",
    "hidden_dim = 32\n",
    "latent_dim = 2  # 2D for visualization\n",
    "\n",
    "vae = VAE(input_dim, hidden_dim, latent_dim)\n",
    "print(f\"VAE created:\")\n",
    "print(f\"  Input: {input_dim}\")\n",
    "print(f\"  Hidden: {hidden_dim}\")\n",
    "print(f\"  Latent: {latent_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Data\n",
    "\n",
    "Simple 4x4 patterns for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_patterns(num_samples=100):\n",
    "    \"\"\"\n",
    "    Generate simple 4x4 binary patterns\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        pattern = np.zeros((4, 4))\n",
    "        \n",
    "        if i % 4 == 0:\n",
    "            # Horizontal line\n",
    "            pattern[1:2, :] = 1\n",
    "        elif i % 4 == 1:\n",
    "            # Vertical line\n",
    "            pattern[:, 2:3] = 1\n",
    "        elif i % 4 == 2:\n",
    "            # Diagonal\n",
    "            np.fill_diagonal(pattern, 1)\n",
    "        else:\n",
    "            # Corner square\n",
    "            pattern[:2, :2] = 1\n",
    "        \n",
    "        # Add small noise\n",
    "        noise = np.random.randn(4, 4) * 0.05\n",
    "        pattern = np.clip(pattern + noise, 0, 1)\n",
    "        \n",
    "        data.append(pattern.flatten())\n",
    "    \n",
    "    return np.array(data)\n",
    "\n",
    "# Generate training data\n",
    "X_train = generate_patterns(200)\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(1, 4, figsize=(12, 3))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(X_train[i].reshape(4, 4), cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Pattern {i}')\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Training Data Samples')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {len(X_train)} training samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Forward Pass and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a single example\n",
    "x = X_train[0:1]\n",
    "x_recon, mu, log_var, z = vae.forward(x)\n",
    "\n",
    "total_loss, recon_loss, kl_loss = vae.loss(x, x_recon, mu, log_var)\n",
    "\n",
    "print(f\"Forward pass:\")\n",
    "print(f\"  Input shape: {x.shape}\")\n",
    "print(f\"  Latent mu: {mu}\")\n",
    "print(f\"  Latent log_var: {log_var}\")\n",
    "print(f\"  Latent z: {z}\")\n",
    "print(f\"  Reconstruction shape: {x_recon.shape}\")\n",
    "print(f\"\\nLosses:\")\n",
    "print(f\"  Total: {total_loss:.4f}\")\n",
    "print(f\"  Reconstruction: {recon_loss:.4f}\")\n",
    "print(f\"  KL Divergence: {kl_loss:.4f}\")\n",
    "\n",
    "# Visualize reconstruction\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax1.imshow(x.reshape(4, 4), cmap='gray', vmin=0, vmax=1)\n",
    "ax1.set_title('Original')\n",
    "ax1.axis('off')\n",
    "\n",
    "ax2.imshow(x_recon.reshape(4, 4), cmap='gray', vmin=0, vmax=1)\n",
    "ax2.set_title('Reconstruction (Untrained)')\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Latent Space\n",
    "\n",
    "Since latent_dim=2, we can visualize the learned representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode all training data\n",
    "latent_codes = []\n",
    "pattern_types = []\n",
    "\n",
    "for i, x in enumerate(X_train):\n",
    "    mu, log_var = vae.encode(x.reshape(1, -1))\n",
    "    latent_codes.append(mu[0])\n",
    "    pattern_types.append(i % 4)\n",
    "\n",
    "latent_codes = np.array(latent_codes)\n",
    "pattern_types = np.array(pattern_types)\n",
    "\n",
    "# Plot latent space\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(\n",
    "    latent_codes[:, 0], \n",
    "    latent_codes[:, 1], \n",
    "    c=pattern_types, \n",
    "    cmap='tab10', \n",
    "    alpha=0.6,\n",
    "    s=50\n",
    ")\n",
    "plt.colorbar(scatter, label='Pattern Type')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.title('Latent Space (Untrained VAE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Latent space visualization shows distribution of encoded patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample from Prior and Generate\n",
    "\n",
    "Sample z ~ N(0, I) and decode to generate new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from standard normal prior\n",
    "num_samples = 8\n",
    "z_samples = np.random.randn(num_samples, latent_dim)\n",
    "\n",
    "# Generate samples\n",
    "generated = []\n",
    "for z in z_samples:\n",
    "    x_gen = vae.decode(z.reshape(1, -1))\n",
    "    generated.append(x_gen[0])\n",
    "\n",
    "# Visualize generated samples\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(generated[i].reshape(4, 4), cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title(f'z={z_samples[i][:2]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Generated Samples from Prior p(z) = N(0, I)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolation in Latent Space\n",
    "\n",
    "Smoothly interpolate between two points in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode two different patterns\n",
    "x1 = X_train[0:1]  # Pattern type 0\n",
    "x2 = X_train[1:2]  # Pattern type 1\n",
    "\n",
    "mu1, _ = vae.encode(x1)\n",
    "mu2, _ = vae.encode(x2)\n",
    "\n",
    "# Interpolate\n",
    "num_steps = 8\n",
    "interpolated = []\n",
    "\n",
    "for alpha in np.linspace(0, 1, num_steps):\n",
    "    z_interp = (1 - alpha) * mu1 + alpha * mu2\n",
    "    x_interp = vae.decode(z_interp)\n",
    "    interpolated.append(x_interp[0])\n",
    "\n",
    "# Visualize interpolation\n",
    "fig, axes = plt.subplots(1, num_steps, figsize=(16, 2))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(interpolated[i].reshape(4, 4), cmap='gray', vmin=0, vmax=1)\n",
    "    ax.set_title(f'α={i/(num_steps-1):.2f}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Latent Space Interpolation', fontsize=14, y=1.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Smooth transitions show continuity in latent space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reparameterization Trick Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show multiple samples from same distribution\n",
    "x = X_train[0:1]\n",
    "mu, log_var = vae.encode(x)\n",
    "\n",
    "# Sample multiple times\n",
    "num_samples = 100\n",
    "z_samples = []\n",
    "for _ in range(num_samples):\n",
    "    z = vae.reparameterize(mu, log_var)\n",
    "    z_samples.append(z[0])\n",
    "\n",
    "z_samples = np.array(z_samples)\n",
    "\n",
    "# Plot distribution\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(z_samples[:, 0], z_samples[:, 1], alpha=0.3, s=20)\n",
    "plt.scatter(mu[0, 0], mu[0, 1], color='red', s=200, marker='*', label='μ', zorder=5)\n",
    "\n",
    "# Draw ellipse for 2 standard deviations\n",
    "std = np.exp(0.5 * log_var[0])\n",
    "theta = np.linspace(0, 2*np.pi, 100)\n",
    "ellipse_x = mu[0, 0] + 2 * std[0] * np.cos(theta)\n",
    "ellipse_y = mu[0, 1] + 2 * std[1] * np.sin(theta)\n",
    "plt.plot(ellipse_x, ellipse_y, 'r--', label='2σ boundary', linewidth=2)\n",
    "\n",
    "plt.xlabel('z₁')\n",
    "plt.ylabel('z₂')\n",
    "plt.title('Reparameterization Trick: z = μ + σ ⊙ ε, where ε ~ N(0,I)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "print(f\"μ = {mu[0]}\")\n",
    "print(f\"σ = {std}\")\n",
    "print(f\"Sample mean: {z_samples.mean(axis=0)}\")\n",
    "print(f\"Sample std: {z_samples.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### VAE Architecture:\n",
    "1. **Encoder**: q_φ(z|x) - Maps input to latent distribution\n",
    "2. **Reparameterization**: z = μ + σ ⊙ ε (enables backprop)\n",
    "3. **Decoder**: p_θ(x|z) - Generates output from latent code\n",
    "\n",
    "### Loss Function (ELBO):\n",
    "```\n",
    "L = E[log p(x|z)] - KL(q(z|x) || p(z))\n",
    "  = Reconstruction Loss - KL Divergence\n",
    "```\n",
    "\n",
    "### KL Divergence:\n",
    "- Regularizes latent space to be close to prior p(z) = N(0, I)\n",
    "- Prevents overfitting\n",
    "- Ensures smooth latent space\n",
    "\n",
    "### Reparameterization Trick:\n",
    "- Makes sampling differentiable\n",
    "- z = μ(x) + σ(x) ⊙ ε, where ε ~ N(0, I)\n",
    "- Gradients flow through μ and σ\n",
    "\n",
    "### Properties:\n",
    "- **Generative**: Can sample new data\n",
    "- **Continuous latent space**: Smooth interpolations\n",
    "- **Probabilistic**: Models uncertainty\n",
    "- **Disentangled representations**: (with β-VAE, etc.)\n",
    "\n",
    "### Applications:\n",
    "- Image generation\n",
    "- Dimensionality reduction\n",
    "- Semi-supervised learning\n",
    "- Anomaly detection\n",
    "- Data augmentation\n",
    "\n",
    "### Variants:\n",
    "- **β-VAE**: Weighted KL for disentanglement\n",
    "- **Conditional VAE**: Conditioned generation\n",
    "- **Hierarchical VAE**: Multiple latent levels\n",
    "- **VQ-VAE**: Discrete latents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
