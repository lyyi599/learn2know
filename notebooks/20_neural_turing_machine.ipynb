{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 20: Neural Turing Machines\n",
    "## Alex Graves, Greg Wayne, Ivo Danihelka (2014)\n",
    "\n",
    "### External Memory with Differentiable Read/Write\n",
    "\n",
    "NTM augments neural networks with external memory that can be read from and written to via attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Memory Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    def __init__(self, num_slots, slot_size):\n",
    "        \"\"\"\n",
    "        External memory bank\n",
    "        \n",
    "        num_slots: Number of memory locations (N)\n",
    "        slot_size: Size of each memory vector (M)\n",
    "        \"\"\"\n",
    "        self.num_slots = num_slots\n",
    "        self.slot_size = slot_size\n",
    "        \n",
    "        # Initialize memory to small random values\n",
    "        self.memory = np.random.randn(num_slots, slot_size) * 0.01\n",
    "    \n",
    "    def read(self, weights):\n",
    "        \"\"\"\n",
    "        Read from memory using attention weights\n",
    "        \n",
    "        weights: (num_slots,) attention distribution\n",
    "        Returns: (slot_size,) weighted combination of memory rows\n",
    "        \"\"\"\n",
    "        return np.dot(weights, self.memory)\n",
    "    \n",
    "    def write(self, weights, erase_vector, add_vector):\n",
    "        \"\"\"\n",
    "        Write to memory using erase and add operations\n",
    "        \n",
    "        weights: (num_slots,) where to write\n",
    "        erase_vector: (slot_size,) what to erase\n",
    "        add_vector: (slot_size,) what to add\n",
    "        \"\"\"\n",
    "        # Erase: M_t = M_{t-1} * (1 - w_t ⊗ e_t)\n",
    "        erase = np.outer(weights, erase_vector)\n",
    "        self.memory = self.memory * (1 - erase)\n",
    "        \n",
    "        # Add: M_t = M_t + w_t ⊗ a_t\n",
    "        add = np.outer(weights, add_vector)\n",
    "        self.memory = self.memory + add\n",
    "    \n",
    "    def get_memory(self):\n",
    "        return self.memory.copy()\n",
    "\n",
    "# Test memory\n",
    "memory = Memory(num_slots=8, slot_size=4)\n",
    "print(f\"Memory initialized: {memory.num_slots} slots × {memory.slot_size} dimensions\")\n",
    "print(f\"Memory shape: {memory.memory.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content-Based Addressing\n",
    "\n",
    "Attend to memory locations based on content similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u, v):\n",
    "    \"\"\"Cosine similarity between vectors\"\"\"\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v) + 1e-8)\n",
    "\n",
    "def softmax(x, beta=1.0):\n",
    "    \"\"\"Softmax with temperature beta\"\"\"\n",
    "    x = beta * x\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def content_addressing(memory, key, beta):\n",
    "    \"\"\"\n",
    "    Content-based addressing\n",
    "    \n",
    "    memory: (num_slots, slot_size)\n",
    "    key: (slot_size,) query vector\n",
    "    beta: sharpness parameter (> 0)\n",
    "    \n",
    "    Returns: (num_slots,) attention weights\n",
    "    \"\"\"\n",
    "    # Compute cosine similarity with each memory row\n",
    "    similarities = np.array([\n",
    "        cosine_similarity(key, memory[i]) \n",
    "        for i in range(len(memory))\n",
    "    ])\n",
    "    \n",
    "    # Apply softmax with sharpness\n",
    "    weights = softmax(similarities, beta=beta)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Test content addressing\n",
    "key = np.random.randn(memory.slot_size)\n",
    "beta = 2.0\n",
    "\n",
    "weights = content_addressing(memory.memory, key, beta)\n",
    "print(f\"\\nContent-based addressing:\")\n",
    "print(f\"Key shape: {key.shape}\")\n",
    "print(f\"Attention weights: {weights}\")\n",
    "print(f\"Sum of weights: {weights.sum():.4f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(range(len(weights)), weights)\n",
    "plt.xlabel('Memory Slot')\n",
    "plt.ylabel('Attention Weight')\n",
    "plt.title('Content-Based Addressing Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location-Based Addressing\n",
    "\n",
    "Shift attention based on relative positions (for sequential access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolation(weights_content, weights_prev, g):\n",
    "    \"\"\"\n",
    "    Interpolate between content and previous weights\n",
    "    \n",
    "    g: gate in [0, 1]\n",
    "      g=1: use only content weights\n",
    "      g=0: use only previous weights\n",
    "    \"\"\"\n",
    "    return g * weights_content + (1 - g) * weights_prev\n",
    "\n",
    "def convolutional_shift(weights, shift_weights):\n",
    "    \"\"\"\n",
    "    Rotate attention weights by shift distribution\n",
    "    \n",
    "    shift_weights: distribution over [-1, 0, +1] shifts\n",
    "    \"\"\"\n",
    "    num_slots = len(weights)\n",
    "    shifted = np.zeros_like(weights)\n",
    "    \n",
    "    # Apply each shift\n",
    "    for shift_idx, shift_amount in enumerate([-1, 0, 1]):\n",
    "        rolled = np.roll(weights, shift_amount)\n",
    "        shifted += shift_weights[shift_idx] * rolled\n",
    "    \n",
    "    return shifted\n",
    "\n",
    "def sharpening(weights, gamma):\n",
    "    \"\"\"\n",
    "    Sharpen attention distribution\n",
    "    \n",
    "    gamma >= 1: larger values = sharper distribution\n",
    "    \"\"\"\n",
    "    weights = weights ** gamma\n",
    "    return weights / (np.sum(weights) + 1e-8)\n",
    "\n",
    "# Test location-based operations\n",
    "weights_prev = np.array([0.05, 0.1, 0.2, 0.3, 0.2, 0.1, 0.04, 0.01])\n",
    "weights_content = content_addressing(memory.memory, key, beta=2.0)\n",
    "\n",
    "# Interpolation\n",
    "g = 0.7  # Favor content\n",
    "weights_gated = interpolation(weights_content, weights_prev, g)\n",
    "\n",
    "# Shift\n",
    "shift_weights = np.array([0.1, 0.8, 0.1])  # Mostly stay, little shift\n",
    "weights_shifted = convolutional_shift(weights_gated, shift_weights)\n",
    "\n",
    "# Sharpen\n",
    "gamma = 2.0\n",
    "weights_sharp = sharpening(weights_shifted, gamma)\n",
    "\n",
    "# Visualize addressing pipeline\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "\n",
    "axes[0, 0].bar(range(len(weights_prev)), weights_prev)\n",
    "axes[0, 0].set_title('Previous Weights')\n",
    "axes[0, 0].set_ylim(0, 0.5)\n",
    "\n",
    "axes[0, 1].bar(range(len(weights_content)), weights_content)\n",
    "axes[0, 1].set_title('Content Weights')\n",
    "axes[0, 1].set_ylim(0, 0.5)\n",
    "\n",
    "axes[0, 2].bar(range(len(weights_gated)), weights_gated)\n",
    "axes[0, 2].set_title(f'Gated (g={g})')\n",
    "axes[0, 2].set_ylim(0, 0.5)\n",
    "\n",
    "axes[1, 0].bar(range(len(shift_weights)), shift_weights, color='orange')\n",
    "axes[1, 0].set_title('Shift Distribution')\n",
    "axes[1, 0].set_xticks([0, 1, 2])\n",
    "axes[1, 0].set_xticklabels(['-1', '0', '+1'])\n",
    "\n",
    "axes[1, 1].bar(range(len(weights_shifted)), weights_shifted, color='green')\n",
    "axes[1, 1].set_title('After Shift')\n",
    "axes[1, 1].set_ylim(0, 0.5)\n",
    "\n",
    "axes[1, 2].bar(range(len(weights_sharp)), weights_sharp, color='red')\n",
    "axes[1, 2].set_title(f'Sharpened (γ={gamma})')\n",
    "axes[1, 2].set_ylim(0, 0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAddressing pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete NTM Head (Read/Write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTMHead:\n",
    "    def __init__(self, memory_slots, memory_size, controller_size):\n",
    "        self.memory_slots = memory_slots\n",
    "        self.memory_size = memory_size\n",
    "        \n",
    "        # Parameters produced by controller\n",
    "        # Key for content addressing\n",
    "        self.W_key = np.random.randn(memory_size, controller_size) * 0.1\n",
    "        \n",
    "        # Strength (beta)\n",
    "        self.W_beta = np.random.randn(1, controller_size) * 0.1\n",
    "        \n",
    "        # Gate (g)\n",
    "        self.W_g = np.random.randn(1, controller_size) * 0.1\n",
    "        \n",
    "        # Shift weights\n",
    "        self.W_shift = np.random.randn(3, controller_size) * 0.1\n",
    "        \n",
    "        # Sharpening (gamma)\n",
    "        self.W_gamma = np.random.randn(1, controller_size) * 0.1\n",
    "        \n",
    "        # For write head: erase and add vectors\n",
    "        self.W_erase = np.random.randn(memory_size, controller_size) * 0.1\n",
    "        self.W_add = np.random.randn(memory_size, controller_size) * 0.1\n",
    "        \n",
    "        # Previous weights\n",
    "        self.weights_prev = np.ones(memory_slots) / memory_slots\n",
    "    \n",
    "    def address(self, memory, controller_output):\n",
    "        \"\"\"\n",
    "        Compute addressing weights from controller output\n",
    "        \"\"\"\n",
    "        # Content addressing\n",
    "        key = np.tanh(np.dot(self.W_key, controller_output))\n",
    "        beta = np.exp(np.dot(self.W_beta, controller_output))[0] + 1e-4\n",
    "        weights_content = content_addressing(memory, key, beta)\n",
    "        \n",
    "        # Interpolation\n",
    "        g = 1 / (1 + np.exp(-np.dot(self.W_g, controller_output)))[0]  # sigmoid\n",
    "        weights_gated = interpolation(weights_content, self.weights_prev, g)\n",
    "        \n",
    "        # Shift\n",
    "        shift_logits = np.dot(self.W_shift, controller_output)\n",
    "        shift_weights = softmax(shift_logits)\n",
    "        weights_shifted = convolutional_shift(weights_gated, shift_weights)\n",
    "        \n",
    "        # Sharpen\n",
    "        gamma = np.exp(np.dot(self.W_gamma, controller_output))[0] + 1.0\n",
    "        weights = sharpening(weights_shifted, gamma)\n",
    "        \n",
    "        self.weights_prev = weights\n",
    "        return weights\n",
    "    \n",
    "    def read(self, memory, weights):\n",
    "        \"\"\"Read from memory\"\"\"\n",
    "        return memory.read(weights)\n",
    "    \n",
    "    def write(self, memory, weights, controller_output):\n",
    "        \"\"\"Write to memory\"\"\"\n",
    "        erase = 1 / (1 + np.exp(-np.dot(self.W_erase, controller_output)))  # sigmoid\n",
    "        add = np.tanh(np.dot(self.W_add, controller_output))\n",
    "        memory.write(weights, erase, add)\n",
    "\n",
    "print(\"NTM Head created with full addressing mechanism\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Task: Copy Sequence\n",
    "\n",
    "Classic NTM task: copy a sequence from input to output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple copy task\n",
    "memory = Memory(num_slots=8, slot_size=4)\n",
    "controller_size = 16\n",
    "head = NTMHead(memory.num_slots, memory.slot_size, controller_size)\n",
    "\n",
    "# Input sequence\n",
    "sequence = [\n",
    "    np.array([1, 0, 0, 0]),\n",
    "    np.array([0, 1, 0, 0]),\n",
    "    np.array([0, 0, 1, 0]),\n",
    "    np.array([0, 0, 0, 1]),\n",
    "]\n",
    "\n",
    "# Write phase: store sequence in memory\n",
    "memory_states = [memory.get_memory()]\n",
    "write_weights_history = []\n",
    "\n",
    "for i, item in enumerate(sequence):\n",
    "    # Simulate controller output (random for demo)\n",
    "    controller_out = np.random.randn(controller_size)\n",
    "    \n",
    "    # Get write weights\n",
    "    weights = head.address(memory.memory, controller_out)\n",
    "    write_weights_history.append(weights)\n",
    "    \n",
    "    # Write to memory\n",
    "    head.write(memory, weights, controller_out)\n",
    "    memory_states.append(memory.get_memory())\n",
    "\n",
    "# Visualize write process\n",
    "fig, axes = plt.subplots(1, len(sequence) + 1, figsize=(16, 4))\n",
    "\n",
    "# Initial memory\n",
    "axes[0].imshow(memory_states[0], cmap='RdBu', aspect='auto')\n",
    "axes[0].set_title('Initial Memory')\n",
    "axes[0].set_ylabel('Memory Slot')\n",
    "axes[0].set_xlabel('Dimension')\n",
    "\n",
    "# After each write\n",
    "for i in range(len(sequence)):\n",
    "    axes[i+1].imshow(memory_states[i+1], cmap='RdBu', aspect='auto')\n",
    "    axes[i+1].set_title(f'After Write {i+1}')\n",
    "    axes[i+1].set_xlabel('Dimension')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Memory Evolution During Write', y=1.05)\n",
    "plt.show()\n",
    "\n",
    "# Show write attention patterns\n",
    "write_weights = np.array(write_weights_history).T\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(write_weights, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Write Weight')\n",
    "plt.xlabel('Write Step')\n",
    "plt.ylabel('Memory Slot')\n",
    "plt.title('Write Attention Patterns')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nWrote {len(sequence)} items to memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### NTM Architecture:\n",
    "1. **Controller**: Neural network (LSTM/FF) that produces control signals\n",
    "2. **Memory Matrix**: External memory (N × M)\n",
    "3. **Read Heads**: Attention-based reading\n",
    "4. **Write Heads**: Attention-based writing with erase + add\n",
    "\n",
    "### Addressing Mechanisms:\n",
    "1. **Content-Based**: Similarity to memory contents\n",
    "2. **Location-Based**: Relative shifts (sequential access)\n",
    "3. **Combination**: Interpolate between content and location\n",
    "\n",
    "### Addressing Pipeline:\n",
    "```\n",
    "Content Addressing → Interpolation → Shift → Sharpening\n",
    "```\n",
    "\n",
    "### Write Operations:\n",
    "- **Erase**: M_t = M_{t-1} ⊙ (1 - w ⊗ e)\n",
    "- **Add**: M_t = M_t + (w ⊗ a)\n",
    "- Combines to allow selective modification\n",
    "\n",
    "### Capabilities:\n",
    "- Copy and recall sequences\n",
    "- Learn algorithms (sorting, copying, etc.)\n",
    "- Generalize to longer sequences\n",
    "- Differentiable memory access\n",
    "\n",
    "### Limitations:\n",
    "- Computationally expensive (attention over all memory)\n",
    "- Difficult to train\n",
    "- Memory size fixed\n",
    "\n",
    "### Impact:\n",
    "- Inspired differentiable memory research\n",
    "- Led to: Differentiable Neural Computer (DNC), Memory Networks\n",
    "- Showed neural networks can learn algorithms\n",
    "- Precursor to modern external memory systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
