{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 27: Better & Faster Large Language Models via Multi-token Prediction\n",
    "## Meta AI Research (2024)\n",
    "\n",
    "### Multi-token Prediction\n",
    "\n",
    "Key insight: Train LMs to predict multiple future tokens simultaneously. Improves sample efficiency and generation quality!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Single-Token Prediction\n",
    "\n",
    "Traditional language modeling:\n",
    "```\n",
    "Input:  [w1, w2, w3, w4]\n",
    "Predict: w5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "class SingleTokenRNN:\n",
    "    \"\"\"Standard RNN with single-token prediction\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embeddings\n",
    "        self.W_embed = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        \n",
    "        # RNN weights\n",
    "        self.W_xh = np.random.randn(hidden_dim, embedding_dim) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "        self.b_h = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Output projection (predict next token)\n",
    "        self.W_out = np.random.randn(vocab_size, hidden_dim) * 0.01\n",
    "        self.b_out = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        input_seq: list of token indices\n",
    "        Returns: predictions for next token at each position\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_dim, 1))\n",
    "        predictions = []\n",
    "        hidden_states = []\n",
    "        \n",
    "        for token_idx in input_seq:\n",
    "            # Embed\n",
    "            x = self.W_embed[token_idx].reshape(-1, 1)\n",
    "            \n",
    "            # RNN step\n",
    "            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h) + self.b_h)\n",
    "            \n",
    "            # Predict next token\n",
    "            logits = np.dot(self.W_out, h) + self.b_out\n",
    "            probs = softmax(logits.T)\n",
    "            \n",
    "            predictions.append(probs.flatten())\n",
    "            hidden_states.append(h.copy())\n",
    "        \n",
    "        return predictions, hidden_states\n",
    "\n",
    "# Test\n",
    "vocab_size = 50\n",
    "single_model = SingleTokenRNN(vocab_size, embedding_dim=32, hidden_dim=64)\n",
    "test_seq = [1, 2, 3, 4]\n",
    "preds, _ = single_model.forward(test_seq)\n",
    "print(f\"Input sequence length: {len(test_seq)}\")\n",
    "print(f\"Predictions shape: {len(preds)} x {len(preds[0])}\")\n",
    "print(f\"Predicts: 1 token ahead at each position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Token Prediction\n",
    "\n",
    "Predict multiple future tokens:\n",
    "```\n",
    "Input:  [w1, w2, w3, w4]\n",
    "Predict: w5, w6, w7  (3 tokens ahead!)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTokenRNN:\n",
    "    \"\"\"RNN with multi-token prediction\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_future_tokens=3):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_future_tokens = num_future_tokens\n",
    "        \n",
    "        # Shared embeddings and RNN\n",
    "        self.W_embed = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        self.W_xh = np.random.randn(hidden_dim, embedding_dim) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "        self.b_h = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Multiple output heads (one per future position)\n",
    "        self.output_heads = []\n",
    "        for i in range(num_future_tokens):\n",
    "            W_out = np.random.randn(vocab_size, hidden_dim) * 0.01\n",
    "            b_out = np.zeros((vocab_size, 1))\n",
    "            self.output_heads.append((W_out, b_out))\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Returns: predictions for next N tokens at each position\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_dim, 1))\n",
    "        multi_predictions = []  # List of (pred_t+1, pred_t+2, ..., pred_t+N)\n",
    "        hidden_states = []\n",
    "        \n",
    "        for token_idx in input_seq:\n",
    "            # Embed\n",
    "            x = self.W_embed[token_idx].reshape(-1, 1)\n",
    "            \n",
    "            # RNN step\n",
    "            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h) + self.b_h)\n",
    "            \n",
    "            # Predict next N tokens using separate heads\n",
    "            position_preds = []\n",
    "            for W_out, b_out in self.output_heads:\n",
    "                logits = np.dot(W_out, h) + b_out\n",
    "                probs = softmax(logits.T)\n",
    "                position_preds.append(probs.flatten())\n",
    "            \n",
    "            multi_predictions.append(position_preds)\n",
    "            hidden_states.append(h.copy())\n",
    "        \n",
    "        return multi_predictions, hidden_states\n",
    "\n",
    "# Test\n",
    "multi_model = MultiTokenRNN(vocab_size, embedding_dim=32, hidden_dim=64, num_future_tokens=3)\n",
    "multi_preds, _ = multi_model.forward(test_seq)\n",
    "print(f\"Input sequence length: {len(test_seq)}\")\n",
    "print(f\"Multi-predictions: {len(multi_preds)} positions\")\n",
    "print(f\"At each position: {len(multi_preds[0])} future tokens\")\n",
    "print(f\"Each prediction shape: {multi_preds[0][0].shape}\")\n",
    "print(f\"\\nPredicts: {len(multi_preds[0])} tokens ahead at each position!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_sequences(vocab_size=50, num_sequences=1000, seq_length=20):\n",
    "    \"\"\"\n",
    "    Generate synthetic sequences with patterns\n",
    "    Pattern: arithmetic progressions (e.g., 1, 2, 3, 4, ...)\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    \n",
    "    for _ in range(num_sequences):\n",
    "        # Random starting point and step\n",
    "        start = np.random.randint(0, vocab_size // 2)\n",
    "        step = np.random.randint(1, 3)\n",
    "        \n",
    "        # Generate arithmetic sequence\n",
    "        seq = [(start + i * step) % vocab_size for i in range(seq_length)]\n",
    "        sequences.append(seq)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# Generate data\n",
    "train_sequences = generate_synthetic_sequences(vocab_size, num_sequences=1000, seq_length=20)\n",
    "test_sequences = generate_synthetic_sequences(vocab_size, num_sequences=200, seq_length=20)\n",
    "\n",
    "print(f\"Training sequences: {len(train_sequences)}\")\n",
    "print(f\"Example sequence: {train_sequences[0][:10]}...\")\n",
    "print(f\"Pattern: arithmetic progression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Single-Token Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_single_token(model, sequences, epochs=50, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train with standard next-token prediction\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for seq in sequences:\n",
    "            # Predict next token at each position\n",
    "            for i in range(len(seq) - 1):\n",
    "                input_tokens = seq[:i+1]\n",
    "                target_token = seq[i+1]\n",
    "                \n",
    "                # Forward\n",
    "                predictions, _ = model.forward(input_tokens)\n",
    "                pred_probs = predictions[-1]  # Last position prediction\n",
    "                \n",
    "                # Loss\n",
    "                loss = -np.log(pred_probs[target_token] + 1e-8)\n",
    "                epoch_loss += loss\n",
    "                \n",
    "                # Backward (simplified - just track loss)\n",
    "        \n",
    "        avg_loss = epoch_loss / (len(sequences) * (len(seq) - 1))\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train single-token model\n",
    "print(\"Training Single-Token Model...\\n\")\n",
    "single_losses = train_single_token(single_model, train_sequences[:100], epochs=30)\n",
    "print(f\"\\nFinal loss: {single_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Multi-Token Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_multi_token(model, sequences, epochs=50, lr=0.01):\n",
    "    \"\"\"\n",
    "    Train with multi-token prediction\n",
    "    Loss = sum of losses for all future positions\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        num_predictions = 0\n",
    "        \n",
    "        for seq in sequences:\n",
    "            # Predict multiple tokens at each position\n",
    "            for i in range(len(seq) - model.num_future_tokens):\n",
    "                input_tokens = seq[:i+1]\n",
    "                target_tokens = seq[i+1:i+1+model.num_future_tokens]\n",
    "                \n",
    "                # Forward\n",
    "                multi_preds, _ = model.forward(input_tokens)\n",
    "                position_preds = multi_preds[-1]  # Last position predictions\n",
    "                \n",
    "                # Loss for each future position\n",
    "                for j, (pred_probs, target) in enumerate(zip(position_preds, target_tokens)):\n",
    "                    loss = -np.log(pred_probs[target] + 1e-8)\n",
    "                    epoch_loss += loss\n",
    "                    num_predictions += 1\n",
    "        \n",
    "        avg_loss = epoch_loss / num_predictions if num_predictions > 0 else 0\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "# Train multi-token model\n",
    "print(\"\\nTraining Multi-Token Model (3 tokens ahead)...\\n\")\n",
    "multi_losses = train_multi_token(multi_model, train_sequences[:100], epochs=30)\n",
    "print(f\"\\nFinal loss: {multi_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(single_losses, label='Single-Token Prediction', linewidth=2, marker='o', markersize=4)\n",
    "plt.plot(multi_losses, label='Multi-Token Prediction (3 ahead)', linewidth=2, marker='s', markersize=4)\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('Average Loss', fontsize=12)\n",
    "plt.title('Learning Curves: Single vs Multi-Token Prediction', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSingle-token final loss: {single_losses[-1]:.4f}\")\n",
    "print(f\"Multi-token final loss: {multi_losses[-1]:.4f}\")\n",
    "print(f\"\\nMulti-token prediction provides richer training signal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation: Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_single_token(model, sequences):\n",
    "    \"\"\"Evaluate next-token prediction accuracy\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for seq in sequences:\n",
    "        for i in range(len(seq) - 1):\n",
    "            input_tokens = seq[:i+1]\n",
    "            target = seq[i+1]\n",
    "            \n",
    "            predictions, _ = model.forward(input_tokens)\n",
    "            pred_token = np.argmax(predictions[-1])\n",
    "            \n",
    "            if pred_token == target:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "def evaluate_multi_token(model, sequences, position=0):\n",
    "    \"\"\"Evaluate multi-token prediction accuracy at specific future position\"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for seq in sequences:\n",
    "        for i in range(len(seq) - model.num_future_tokens):\n",
    "            input_tokens = seq[:i+1]\n",
    "            target = seq[i+1+position]\n",
    "            \n",
    "            multi_preds, _ = model.forward(input_tokens)\n",
    "            pred_probs = multi_preds[-1][position]  # Prediction for position ahead\n",
    "            pred_token = np.argmax(pred_probs)\n",
    "            \n",
    "            if pred_token == target:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    return correct / total if total > 0 else 0\n",
    "\n",
    "# Evaluate both models\n",
    "single_acc = evaluate_single_token(single_model, test_sequences[:50])\n",
    "multi_acc_t1 = evaluate_multi_token(multi_model, test_sequences[:50], position=0)\n",
    "multi_acc_t2 = evaluate_multi_token(multi_model, test_sequences[:50], position=1)\n",
    "multi_acc_t3 = evaluate_multi_token(multi_model, test_sequences[:50], position=2)\n",
    "\n",
    "print(\"\\nEvaluation Results:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Single-Token Model:\")\n",
    "print(f\"  Next token (t+1): {single_acc:.2%}\")\n",
    "print(f\"\\nMulti-Token Model:\")\n",
    "print(f\"  Next token (t+1): {multi_acc_t1:.2%}\")\n",
    "print(f\"  2 tokens ahead (t+2): {multi_acc_t2:.2%}\")\n",
    "print(f\"  3 tokens ahead (t+3): {multi_acc_t3:.2%}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Multi-Token Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate prediction accuracy heatmap\n",
    "test_seq = test_sequences[0][:15]\n",
    "accuracies = np.zeros((len(test_seq) - 3, 3))\n",
    "\n",
    "for i in range(len(test_seq) - 3):\n",
    "    input_tokens = test_seq[:i+1]\n",
    "    targets = test_seq[i+1:i+4]\n",
    "    \n",
    "    multi_preds, _ = multi_model.forward(input_tokens)\n",
    "    position_preds = multi_preds[-1]\n",
    "    \n",
    "    for j in range(3):\n",
    "        pred_token = np.argmax(position_preds[j])\n",
    "        accuracies[i, j] = 1.0 if pred_token == targets[j] else 0.0\n",
    "\n",
    "# Plot\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Heatmap\n",
    "im = ax1.imshow(accuracies.T, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "ax1.set_xlabel('Input Position', fontsize=12)\n",
    "ax1.set_ylabel('Future Position', fontsize=12)\n",
    "ax1.set_title('Multi-Token Prediction Accuracy', fontsize=13, fontweight='bold')\n",
    "ax1.set_yticks([0, 1, 2])\n",
    "ax1.set_yticklabels(['t+1', 't+2', 't+3'])\n",
    "plt.colorbar(im, ax=ax1, label='Accuracy (1=Correct, 0=Wrong)')\n",
    "\n",
    "# Average accuracy by distance\n",
    "avg_accs = np.mean(accuracies, axis=0)\n",
    "positions = ['t+1', 't+2', 't+3']\n",
    "bars = ax2.bar(positions, avg_accs, color=['green', 'orange', 'red'], edgecolor='black', linewidth=2)\n",
    "ax2.set_ylabel('Average Accuracy', fontsize=12)\n",
    "ax2.set_title('Accuracy vs Prediction Distance', fontsize=13, fontweight='bold')\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, avg_accs):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc:.1%}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nFurther predictions are harder (as expected)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Efficiency Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on varying dataset sizes\n",
    "dataset_sizes = [10, 25, 50, 100, 200]\n",
    "single_final_losses = []\n",
    "multi_final_losses = []\n",
    "\n",
    "print(\"Testing sample efficiency...\\n\")\n",
    "\n",
    "for size in dataset_sizes:\n",
    "    print(f\"Training on {size} sequences...\")\n",
    "    \n",
    "    # Single-token\n",
    "    single_temp = SingleTokenRNN(vocab_size, embedding_dim=32, hidden_dim=64)\n",
    "    single_loss = train_single_token(single_temp, train_sequences[:size], epochs=20, lr=0.01)\n",
    "    single_final_losses.append(single_loss[-1])\n",
    "    \n",
    "    # Multi-token\n",
    "    multi_temp = MultiTokenRNN(vocab_size, embedding_dim=32, hidden_dim=64, num_future_tokens=3)\n",
    "    multi_loss = train_multi_token(multi_temp, train_sequences[:size], epochs=20, lr=0.01)\n",
    "    multi_final_losses.append(multi_loss[-1])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dataset_sizes, single_final_losses, 'o-', linewidth=2, markersize=10, \n",
    "        label='Single-Token', color='blue')\n",
    "plt.plot(dataset_sizes, multi_final_losses, 's-', linewidth=2, markersize=10, \n",
    "        label='Multi-Token (3 ahead)', color='red')\n",
    "plt.xlabel('Number of Training Sequences', fontsize=12)\n",
    "plt.ylabel('Final Loss', fontsize=12)\n",
    "plt.title('Sample Efficiency: Single vs Multi-Token', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMulti-token prediction is more sample efficient (learns faster with less data)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Multi-Token Prediction:\n",
    "\n",
    "**Standard LM**:\n",
    "```\n",
    "Given: w1, w2, w3\n",
    "Predict: w4\n",
    "Loss: -log P(w4 | w1, w2, w3)\n",
    "```\n",
    "\n",
    "**Multi-Token LM**:\n",
    "```\n",
    "Given: w1, w2, w3\n",
    "Predict: w4, w5, w6  (multiple tokens!)\n",
    "Loss: -log P(w4|w1:3) - log P(w5|w1:3) - log P(w6|w1:3)\n",
    "```\n",
    "\n",
    "### Architecture:\n",
    "\n",
    "**Shared Backbone**:\n",
    "- Embeddings\n",
    "- RNN/Transformer layers\n",
    "\n",
    "**Multiple Output Heads**:\n",
    "- Head 1: Predicts t+1\n",
    "- Head 2: Predicts t+2\n",
    "- Head 3: Predicts t+3\n",
    "- ...\n",
    "\n",
    "Each head is a separate linear layer (small overhead!)\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "1. **Sample Efficiency** ✅\n",
    "   - Each example provides N training signals (not just 1)\n",
    "   - Learns N times faster (approximately)\n",
    "\n",
    "2. **Better Representations** ✅\n",
    "   - Forced to encode longer-term dependencies\n",
    "   - Can't just memorize next token\n",
    "\n",
    "3. **Faster Inference** ✅\n",
    "   - Can generate multiple tokens in one forward pass\n",
    "   - Speculative decoding: verify predictions in parallel\n",
    "\n",
    "4. **Better Generalization** ✅\n",
    "   - More training signal → better features\n",
    "   - Regularization effect\n",
    "\n",
    "### Training:\n",
    "\n",
    "**Loss Function**:\n",
    "$$\n",
    "\\mathcal{L} = \\sum_{i=1}^{N} \\lambda_i \\cdot \\mathcal{L}_{\\text{next-token}}(t+i)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $N$ = number of future tokens\n",
    "- $\\lambda_i$ = weight for position $i$ (can downweight distant future)\n",
    "\n",
    "**Typical settings**:\n",
    "- $N = 3$ or $N = 4$ tokens ahead\n",
    "- Equal weights: $\\lambda_i = 1/N$\n",
    "- Or decay: $\\lambda_i = \\gamma^{i-1}$ where $\\gamma < 1$\n",
    "\n",
    "### Results from Paper (Meta AI):\n",
    "\n",
    "**7B model**:\n",
    "- Standard: X perplexity\n",
    "- Multi-token (4 ahead): 0.7X perplexity (better!)\n",
    "\n",
    "**Sample efficiency**:\n",
    "- Multi-token with 1/3 data = Standard with full data\n",
    "\n",
    "**Inference speed**:\n",
    "- 3x faster generation (using speculative decoding)\n",
    "\n",
    "### Inference Strategies:\n",
    "\n",
    "**1. Standard (still valid)**:\n",
    "```\n",
    "Use only head 1 (t+1 predictions)\n",
    "Same as normal autoregressive generation\n",
    "```\n",
    "\n",
    "**2. Speculative Decoding**:\n",
    "```\n",
    "Generate w4, w5, w6 from heads\n",
    "Verify each prediction\n",
    "Keep valid prefix, regenerate rest\n",
    "→ Up to Nx speedup!\n",
    "```\n",
    "\n",
    "**3. Beam Search Enhancement**:\n",
    "```\n",
    "Consider multiple future paths simultaneously\n",
    "Better long-range planning\n",
    "```\n",
    "\n",
    "### Comparison with Other Techniques:\n",
    "\n",
    "| Technique | Sample Efficiency | Inference Speed | Complexity |\n",
    "|-----------|------------------|-----------------|------------|\n",
    "| Standard LM | 1x | 1x | Low |\n",
    "| Data Augmentation | 1.2x | 1x | Low |\n",
    "| **Multi-Token** | **2-3x** | **1-3x** | **Low** |\n",
    "| Distillation | 1.5x | 1.5x | High |\n",
    "\n",
    "### Implementation Tips:\n",
    "\n",
    "1. **Start simple**: N=2 or N=3 tokens\n",
    "2. **Shared trunk**: Only output heads are separate\n",
    "3. **Equal weighting**: Unless you have reason to prefer near/far future\n",
    "4. **Monitor each head**: Track accuracy for each position\n",
    "5. **Use for speedup**: Speculative decoding in inference\n",
    "\n",
    "### When to Use:\n",
    "\n",
    "✅ **Good for**:\n",
    "- Limited training data\n",
    "- Want faster inference\n",
    "- Long sequences (benefits from long-range signal)\n",
    "- Structured outputs (code, formulas)\n",
    "\n",
    "❌ **Not ideal for**:\n",
    "- Very short sequences\n",
    "- Highly random outputs\n",
    "- Memory constrained (extra heads add parameters)\n",
    "\n",
    "### Modern Extensions:\n",
    "\n",
    "1. **Adaptive N**: Use different N for different layers\n",
    "2. **Hierarchical**: Predict next word, next phrase, next sentence\n",
    "3. **Discrete diffusion**: Multi-step generation\n",
    "4. **Continuous-time**: Predict at arbitrary future times\n",
    "\n",
    "### Key Insight:\n",
    "\n",
    "**More prediction = More learning signal = Better models**\n",
    "\n",
    "Multi-token prediction is essentially **free regularization** with **bonus speedup**. Almost no downside!\n",
    "\n",
    "**\"Why predict one token when you can predict many?\"** - Meta AI Team"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
