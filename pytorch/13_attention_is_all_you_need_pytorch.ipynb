{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 13: Attention Is All You Need\n",
    "## Vaswani et al. (2017)\n",
    "\n",
    "### The Transformer: Pure Attention Architecture\n",
    "\n",
    "Revolutionary architecture that replaced RNNs with self-attention, enabling modern LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Torch-Numpy compatibility helpers\n",
    "if not hasattr(torch.Tensor, 'copy'):\n",
    "    torch.Tensor.copy = torch.Tensor.clone\n",
    "\n",
    "def _astype(self, dtype):\n",
    "    \"\"\"\n",
    "    Compute  astype.\n",
    "    \n",
    "    Args:\n",
    "        dtype: Input parameter.\n",
    "    Returns:\n",
    "        Computed result.\n",
    "    \"\"\"\n",
    "    if dtype is float:\n",
    "        return self.float()\n",
    "    if dtype is int:\n",
    "        return self.int()\n",
    "    return self.to(dtype)\n",
    "\n",
    "if not hasattr(torch.Tensor, 'astype'):\n",
    "    torch.Tensor.astype = _astype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "The fundamental building block:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, dim=-1):\n",
    "    \"\"\"\n",
    "    Compute softmax probabilities.\n",
    "    \n",
    "    Args:\n",
    "        x: Input data.\n",
    "        dim: Dimension size.\n",
    "    Returns:\n",
    "        Softmax probabilities.\n",
    "    \"\"\"\n",
    "    x_max = torch.amax(x, dim=axis, keepdim=True)\n",
    "    exp_x = torch.exp(x - x_max)\n",
    "    return exp_x / torch.sum(exp_x, dim=axis, keepdim=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q: Input parameter.\n",
    "        K: Input parameter.\n",
    "        V: Input parameter.\n",
    "        mask: Mask tensor or array.\n",
    "    Returns:\n",
    "        output: Input parameter.\n",
    "        attention_weights: Input parameter.\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = torch.matmul(Q, K.T) / torch.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided (for causality or padding)\n",
    "    if mask is not None:\n",
    "        scores = scores + (mask * -1e9)\n",
    "    \n",
    "    # Softmax to get attention weights\n",
    "    attention_weights = softmax(scores, dim=-1)\n",
    "    \n",
    "    # Weighted sum of values\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test scaled dot-product attention\n",
    "seq_len = 5\n",
    "d_model = 8\n",
    "\n",
    "Q = torch.randn(seq_len, d_model)\n",
    "K = torch.randn(seq_len, d_model)\n",
    "V = torch.randn(seq_len, d_model)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Attention output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"Attention weights sum (should be 1): {attn_weights.sum(dim=1)}\")\n",
    "\n",
    "# Visualize attention pattern\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attn_weights, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Attention Weights Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "Multiple attention \"heads\" attend to different aspects of the input:\n",
    "$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(head_1, ..., head_h)W^O$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Initialize the instance.\n",
    "        \n",
    "        Args:\n",
    "            d_model: Model instance.\n",
    "            num_heads: Number of heads.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V for all heads (parallelized)\n",
    "        self.W_q = torch.randn(d_model, d_model) * 0.1\n",
    "        self.W_k = torch.randn(d_model, d_model) * 0.1\n",
    "        self.W_v = torch.randn(d_model, d_model) * 0.1\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = torch.randn(d_model, d_model) * 0.1\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Compute split heads.\n",
    "        \n",
    "        Args:\n",
    "            x: Input data.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[0]\n",
    "        x = x.reshape(seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 0, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        Compute combine heads.\n",
    "        \n",
    "        Args:\n",
    "            x: Input data.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        x = x.transpose(1, 0, 2)\n",
    "        return x.reshape(seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Run the forward pass value.\n",
    "        \n",
    "        Args:\n",
    "            Q: Input parameter.\n",
    "            K: Input parameter.\n",
    "            V: Input parameter.\n",
    "            mask: Mask tensor or array.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        # Linear projections\n",
    "        Q = torch.matmul(Q, self.W_q.T)\n",
    "        K = torch.matmul(K, self.W_k.T)\n",
    "        V = torch.matmul(V, self.W_v.T)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = self.split_heads(Q)  # (num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Apply attention to each head\n",
    "        head_outputs = []\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            head_out, head_attn = scaled_dot_product_attention(\n",
    "                Q[i], K[i], V[i], mask\n",
    "            )\n",
    "            head_outputs.append(head_out)\n",
    "            self.attention_weights.append(head_attn)\n",
    "        \n",
    "        # Stack heads\n",
    "        heads = torch.stack(head_outputs, dim=0)  # (num_heads, seq_len, d_k)\n",
    "        \n",
    "        # Combine heads\n",
    "        combined = self.combine_heads(heads)  # (seq_len, d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = torch.matmul(combined, self.W_o.T)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "X = torch.randn(seq_len, d_model)\n",
    "output = mha.forward(X, X, X)  # Self-attention\n",
    "\n",
    "print(f\"\\nMulti-Head Attention:\")\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of heads: {num_heads}\")\n",
    "print(f\"Dimension per head: {mha.d_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Since Transformers have no recurrence, we add position information:\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Compute positional encoding.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Length parameter.\n",
    "        d_model: Model instance.\n",
    "    Returns:\n",
    "        pe: Input parameter.\n",
    "    \"\"\"\n",
    "    pe = torch.zeros((seq_len, d_model))\n",
    "    \n",
    "    position = torch.arange(0, seq_len)[:, torch.newaxis]\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(10000.0) / d_model))\n",
    "    \n",
    "    # Apply sin to even indices\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    \n",
    "    # Apply cos to odd indices\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# Generate positional encodings\n",
    "seq_len = 50\n",
    "d_model = 64\n",
    "pe = positional_encoding(seq_len, d_model)\n",
    "\n",
    "# Visualize positional encodings\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(pe.T, cmap='RdBu', aspect='auto')\n",
    "plt.colorbar(label='Encoding Value')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Dimension')\n",
    "plt.title('Positional Encoding (All Dimensions)')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "# Plot first few dimensions\n",
    "for i in [0, 1, 2, 3, 10, 20]:\n",
    "    plt.plot(pe[:, i], label=f'Dim {i}')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Encoding Value')\n",
    "plt.title('Positional Encoding (Selected Dimensions)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Positional encoding shape: {pe.shape}\")\n",
    "print(f\"Different frequencies encode position at different scales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Network\n",
    "\n",
    "Applied to each position independently:\n",
    "$$FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        \"\"\"\n",
    "        Initialize the instance.\n",
    "        \n",
    "        Args:\n",
    "            d_model: Model instance.\n",
    "            d_ff: Input parameter.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        self.W1 = torch.randn(d_model, d_ff) * 0.1\n",
    "        self.b1 = torch.zeros(d_ff)\n",
    "        self.W2 = torch.randn(d_ff, d_model) * 0.1\n",
    "        self.b2 = torch.zeros(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Run the forward pass value.\n",
    "        \n",
    "        Args:\n",
    "            x: Input data.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        # First layer with ReLU\n",
    "        hidden = torch.maximum(0, torch.matmul(x, self.W1) + self.b1)\n",
    "        \n",
    "        # Second layer\n",
    "        output = torch.matmul(hidden, self.W2) + self.b2\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test feed-forward\n",
    "d_model = 64\n",
    "d_ff = 256  # Usually 4x larger\n",
    "\n",
    "ff = FeedForward(d_model, d_ff)\n",
    "x = torch.randn(10, d_model)\n",
    "output = ff.forward(x)\n",
    "\n",
    "print(f\"\\nFeed-Forward Network:\")\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Hidden: ({x.shape[0]}, {d_ff})\")\n",
    "print(f\"Output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "Normalize across features (not batch like BatchNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the instance.\n",
    "        \n",
    "        Args:\n",
    "            d_model: Model instance.\n",
    "            eps: Input parameter.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        self.gamma = torch.ones(d_model)\n",
    "        self.beta = torch.zeros(d_model)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Run the forward pass value.\n",
    "        \n",
    "        Args:\n",
    "            x: Input data.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        \n",
    "        normalized = (x - mean) / (std + self.eps)\n",
    "        output = self.gamma * normalized + self.beta\n",
    "        \n",
    "        return output\n",
    "\n",
    "ln = LayerNorm(d_model)\n",
    "x = torch.randn(10, d_model) * 3 + 5  # Unnormalized\n",
    "normalized = ln.forward(x)\n",
    "\n",
    "print(f\"\\nLayer Normalization:\")\n",
    "print(f\"Input mean: {x.mean():.4f}, std: {x.std():.4f}\")\n",
    "print(f\"Output mean: {normalized.mean():.4f}, std: {normalized.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock:\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        \"\"\"\n",
    "        Initialize the instance.\n",
    "        \n",
    "        Args:\n",
    "            d_model: Model instance.\n",
    "            num_heads: Number of heads.\n",
    "            d_ff: Input parameter.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Run the forward pass value.\n",
    "        \n",
    "        Args:\n",
    "            x: Input data.\n",
    "            mask: Mask tensor or array.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        # Multi-head attention with residual connection\n",
    "        attn_output = self.attention.forward(x, x, x, mask)\n",
    "        x = self.norm1.forward(x + attn_output)\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.ff.forward(x)\n",
    "        x = self.norm2.forward(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test transformer block\n",
    "block = TransformerBlock(d_model=64, num_heads=8, d_ff=256)\n",
    "x = torch.randn(10, 64)\n",
    "output = block.forward(x)\n",
    "\n",
    "print(f\"\\nTransformer Block:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nBlock contains:\")\n",
    "print(f\"  1. Multi-Head Self-Attention\")\n",
    "print(f\"  2. Layer Normalization\")\n",
    "print(f\"  3. Feed-Forward Network\")\n",
    "print(f\"  4. Residual Connections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Multi-Head Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention with interpretable input\n",
    "seq_len = 8\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "X = torch.randn(seq_len, d_model)\n",
    "output = mha.forward(X, X, X)\n",
    "\n",
    "# Plot attention patterns for each head\n",
    "fig, axes = plt.subplots(1, num_heads, figsize=(16, 4))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    attn = mha.attention_weights[i]\n",
    "    im = ax.imshow(attn, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Head {i+1}')\n",
    "    ax.set_xlabel('Key')\n",
    "    ax.set_ylabel('Query')\n",
    "    \n",
    "plt.colorbar(im, ax=axes, label='Attention Weight', fraction=0.046, pad=0.04)\n",
    "plt.suptitle('Multi-Head Attention Patterns', fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEach head learns to attend to different patterns!\")\n",
    "print(\"Different heads capture different relationships in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal (Masked) Self-Attention for Autoregressive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create causal mask.\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Length parameter.\n",
    "    Returns:\n",
    "        mask: Mask tensor or array.\n",
    "    \"\"\"\n",
    "    mask = torch.triu(torch.ones((seq_len, seq_len)), k=1)\n",
    "    return mask\n",
    "\n",
    "# Test causal attention\n",
    "seq_len = 8\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "Q = torch.randn(seq_len, d_model)\n",
    "K = torch.randn(seq_len, d_model)\n",
    "V = torch.randn(seq_len, d_model)\n",
    "\n",
    "# Without mask (bidirectional)\n",
    "output_bi, attn_bi = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# With causal mask (unidirectional)\n",
    "output_causal, attn_causal = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "# Visualize difference\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Causal mask\n",
    "ax1.imshow(causal_mask, cmap='Reds', aspect='auto')\n",
    "ax1.set_title('Causal Mask\\n(1 = masked/not allowed)')\n",
    "ax1.set_xlabel('Key Position')\n",
    "ax1.set_ylabel('Query Position')\n",
    "\n",
    "# Bidirectional attention\n",
    "im2 = ax2.imshow(attn_bi, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
    "ax2.set_title('Bidirectional Attention\\n(can see future)')\n",
    "ax2.set_xlabel('Key Position')\n",
    "ax2.set_ylabel('Query Position')\n",
    "\n",
    "# Causal attention\n",
    "im3 = ax3.imshow(attn_causal, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
    "ax3.set_title('Causal Attention\\n(cannot see future)')\n",
    "ax3.set_xlabel('Key Position')\n",
    "ax3.set_ylabel('Query Position')\n",
    "\n",
    "plt.colorbar(im3, ax=[ax2, ax3], label='Attention Weight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCausal masking is crucial for:\")\n",
    "print(\"  - Autoregressive generation (GPT, language models)\")\n",
    "print(\"  - Prevents information leakage from future tokens\")\n",
    "print(\"  - Each position can only attend to itself and previous positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Why \"Attention Is All You Need\"?\n",
    "- **No recurrence**: Processes entire sequence in parallel\n",
    "- **No convolution**: Pure attention mechanism\n",
    "- **Scales better**: O(nÂ²d) vs O(n) sequential operations in RNNs\n",
    "- **Long-range dependencies**: Direct connections between any positions\n",
    "\n",
    "### Core Components:\n",
    "1. **Scaled Dot-Product Attention**: Efficient attention computation\n",
    "2. **Multi-Head Attention**: Multiple representation subspaces\n",
    "3. **Positional Encoding**: Inject position information\n",
    "4. **Feed-Forward Networks**: Position-wise transformations\n",
    "5. **Layer Normalization**: Stabilize training\n",
    "6. **Residual Connections**: Enable deep networks\n",
    "\n",
    "### Architecture Variants:\n",
    "- **Encoder-Decoder**: Original Transformer (translation)\n",
    "- **Encoder-only**: BERT (bidirectional understanding)\n",
    "- **Decoder-only**: GPT (autoregressive generation)\n",
    "\n",
    "### Advantages:\n",
    "- Parallelizable training (unlike RNNs)\n",
    "- Better long-range dependencies\n",
    "- Interpretable attention patterns\n",
    "- State-of-the-art on many tasks\n",
    "\n",
    "### Impact:\n",
    "- Foundation of modern NLP: GPT, BERT, T5, etc.\n",
    "- Extended to vision: Vision Transformer (ViT)\n",
    "- Multi-modal models: CLIP, Flamingo\n",
    "- Enabled LLMs with billions of parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}