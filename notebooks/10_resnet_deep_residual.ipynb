{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 10: Deep Residual Learning for Image Recognition\n",
    "## Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (2015)\n",
    "\n",
    "### ResNet: Skip Connections Enable Very Deep Networks\n",
    "\n",
    "ResNet introduced residual connections that allow training networks with 100+ layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: Degradation in Deep Networks\n",
    "\n",
    "Before ResNet, adding more layers actually made networks worse (not due to overfitting, but optimization difficulty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "class PlainLayer:\n",
    "    \"\"\"Standard neural network layer\"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.W = np.random.randn(output_size, input_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b = np.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.z = np.dot(self.W, x) + self.b\n",
    "        self.a = relu(self.z)\n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        da = dout * relu_derivative(self.z)\n",
    "        self.dW = np.dot(da, self.x.T)\n",
    "        self.db = np.sum(da, axis=1, keepdims=True)\n",
    "        dx = np.dot(self.W.T, da)\n",
    "        return dx\n",
    "\n",
    "class ResidualBlock:\n",
    "    \"\"\"Residual block with skip connection: y = F(x) + x\"\"\"\n",
    "    def __init__(self, size):\n",
    "        self.layer1 = PlainLayer(size, size)\n",
    "        self.layer2 = PlainLayer(size, size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        \n",
    "        # Residual path F(x)\n",
    "        out = self.layer1.forward(x)\n",
    "        out = self.layer2.forward(out)\n",
    "        \n",
    "        # Skip connection: F(x) + x\n",
    "        self.out = out + x\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        # Gradient flows through both paths\n",
    "        # Skip connection provides direct path\n",
    "        dx_residual = self.layer2.backward(dout)\n",
    "        dx_residual = self.layer1.backward(dx_residual)\n",
    "        \n",
    "        # Total gradient: residual path + skip connection\n",
    "        dx = dx_residual + dout  # This is the key!\n",
    "        return dx\n",
    "\n",
    "print(\"ResNet components initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Plain Network vs ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainNetwork:\n",
    "    \"\"\"Plain deep network without skip connections\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        self.layers = []\n",
    "        \n",
    "        # First layer\n",
    "        self.layers.append(PlainLayer(input_size, hidden_size))\n",
    "        \n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(PlainLayer(hidden_size, hidden_size))\n",
    "        \n",
    "        # Output layer\n",
    "        self.layers.append(PlainLayer(hidden_size, input_size))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        for layer in reversed(self.layers):\n",
    "            dout = layer.backward(dout)\n",
    "        return dout\n",
    "\n",
    "class ResidualNetwork:\n",
    "    \"\"\"Deep network with residual connections\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_blocks):\n",
    "        # Project to hidden size\n",
    "        self.input_proj = PlainLayer(input_size, hidden_size)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.blocks = [ResidualBlock(hidden_size) for _ in range(num_blocks)]\n",
    "        \n",
    "        # Project back to output\n",
    "        self.output_proj = PlainLayer(hidden_size, input_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_proj.forward(x)\n",
    "        for block in self.blocks:\n",
    "            x = block.forward(x)\n",
    "        x = self.output_proj.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def backward(self, dout):\n",
    "        dout = self.output_proj.backward(dout)\n",
    "        for block in reversed(self.blocks):\n",
    "            dout = block.backward(dout)\n",
    "        dout = self.input_proj.backward(dout)\n",
    "        return dout\n",
    "\n",
    "# Create networks\n",
    "input_size = 16\n",
    "hidden_size = 16\n",
    "depth = 10\n",
    "\n",
    "plain_net = PlainNetwork(input_size, hidden_size, depth)\n",
    "resnet = ResidualNetwork(input_size, hidden_size, depth)\n",
    "\n",
    "print(f\"Created Plain Network with {depth} layers\")\n",
    "print(f\"Created ResNet with {depth} residual blocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrate Gradient Flow\n",
    "\n",
    "The key advantage: gradients flow more easily through skip connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_gradient_flow(network, name):\n",
    "    \"\"\"Measure gradient magnitude at different depths\"\"\"\n",
    "    # Random input\n",
    "    x = np.random.randn(input_size, 1)\n",
    "    \n",
    "    # Forward pass\n",
    "    output = network.forward(x)\n",
    "    \n",
    "    # Create gradient signal\n",
    "    dout = np.ones_like(output)\n",
    "    \n",
    "    # Backward pass\n",
    "    network.backward(dout)\n",
    "    \n",
    "    # Collect gradient magnitudes\n",
    "    grad_norms = []\n",
    "    \n",
    "    if isinstance(network, PlainNetwork):\n",
    "        for layer in network.layers:\n",
    "            grad_norm = np.linalg.norm(layer.dW)\n",
    "            grad_norms.append(grad_norm)\n",
    "    else:  # ResNet\n",
    "        grad_norms.append(np.linalg.norm(network.input_proj.dW))\n",
    "        for block in network.blocks:\n",
    "            grad_norm1 = np.linalg.norm(block.layer1.dW)\n",
    "            grad_norm2 = np.linalg.norm(block.layer2.dW)\n",
    "            grad_norms.append(np.mean([grad_norm1, grad_norm2]))\n",
    "        grad_norms.append(np.linalg.norm(network.output_proj.dW))\n",
    "    \n",
    "    return grad_norms\n",
    "\n",
    "# Measure gradient flow in both networks\n",
    "plain_grads = measure_gradient_flow(plain_net, \"Plain Network\")\n",
    "resnet_grads = measure_gradient_flow(resnet, \"ResNet\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(range(len(plain_grads)), plain_grads, 'o-', label='Plain Network', linewidth=2)\n",
    "plt.plot(range(len(resnet_grads)), resnet_grads, 's-', label='ResNet', linewidth=2)\n",
    "plt.xlabel('Layer Depth (deeper →)')\n",
    "plt.ylabel('Gradient Magnitude')\n",
    "plt.title('Gradient Flow: ResNet vs Plain Network')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPlain Network - First layer gradient: {plain_grads[0]:.6f}\")\n",
    "print(f\"Plain Network - Last layer gradient: {plain_grads[-1]:.6f}\")\n",
    "print(f\"Gradient ratio (first/last): {plain_grads[0]/plain_grads[-1]:.2f}x\\n\")\n",
    "\n",
    "print(f\"ResNet - First layer gradient: {resnet_grads[0]:.6f}\")\n",
    "print(f\"ResNet - Last layer gradient: {resnet_grads[-1]:.6f}\")\n",
    "print(f\"Gradient ratio (first/last): {resnet_grads[0]/resnet_grads[-1]:.2f}x\")\n",
    "\n",
    "print(f\"\\nResNet maintains gradient flow {(plain_grads[0]/plain_grads[-1]) / (resnet_grads[0]/resnet_grads[-1]):.1f}x better!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Learned Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic image-like data\n",
    "def generate_patterns(num_samples=100, size=8):\n",
    "    \"\"\"Generate simple 2D patterns\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        pattern = np.zeros((size, size))\n",
    "        \n",
    "        if i % 3 == 0:\n",
    "            # Horizontal lines\n",
    "            pattern[2:3, :] = 1\n",
    "            label = 0\n",
    "        elif i % 3 == 1:\n",
    "            # Vertical lines\n",
    "            pattern[:, 3:4] = 1\n",
    "            label = 1\n",
    "        else:\n",
    "            # Diagonal\n",
    "            np.fill_diagonal(pattern, 1)\n",
    "            label = 2\n",
    "        \n",
    "        # Add noise\n",
    "        pattern += np.random.randn(size, size) * 0.1\n",
    "        \n",
    "        X.append(pattern.flatten())\n",
    "        y.append(label)\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = generate_patterns(num_samples=30, size=4)\n",
    "\n",
    "# Visualize sample patterns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for i, ax in enumerate(axes):\n",
    "    sample = X[i].reshape(4, 4)\n",
    "    ax.imshow(sample, cmap='gray')\n",
    "    ax.set_title(f'Pattern Type {y[i]}')\n",
    "    ax.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Generated {len(X)} pattern samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Mapping: The Core Insight\n",
    "\n",
    "**Key Insight**: If identity mapping is optimal, residual should learn F(x) = 0, which is easier than learning H(x) = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate identity mapping\n",
    "x = np.random.randn(hidden_size, 1)\n",
    "\n",
    "# Initialize residual block\n",
    "block = ResidualBlock(hidden_size)\n",
    "\n",
    "# If weights are near zero, F(x) ≈ 0\n",
    "block.layer1.W *= 0.001\n",
    "block.layer2.W *= 0.001\n",
    "\n",
    "# Forward pass\n",
    "output = block.forward(x)\n",
    "\n",
    "# Check if output ≈ input (identity)\n",
    "identity_error = np.linalg.norm(output - x)\n",
    "\n",
    "print(\"Identity Mapping Demonstration:\")\n",
    "print(f\"Input norm: {np.linalg.norm(x):.4f}\")\n",
    "print(f\"Output norm: {np.linalg.norm(output):.4f}\")\n",
    "print(f\"Identity error ||F(x) + x - x||: {identity_error:.6f}\")\n",
    "print(f\"\\nWith near-zero weights, residual block ≈ identity function!\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x.flatten(), 'o-', label='Input x', alpha=0.7)\n",
    "plt.plot(output.flatten(), 's-', label='Output (x + F(x))', alpha=0.7)\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Identity Mapping: Output ≈ Input')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "residual = output - x\n",
    "plt.bar(range(len(residual)), residual.flatten())\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Residual F(x)')\n",
    "plt.title('Learned Residual ≈ 0')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Network Depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_depth_scaling():\n",
    "    \"\"\"Test how gradient flow scales with depth\"\"\"\n",
    "    depths = [5, 10, 20, 30, 40]\n",
    "    plain_ratios = []\n",
    "    resnet_ratios = []\n",
    "    \n",
    "    for depth in depths:\n",
    "        # Create networks\n",
    "        plain = PlainNetwork(input_size, hidden_size, depth)\n",
    "        res = ResidualNetwork(input_size, hidden_size, depth)\n",
    "        \n",
    "        # Measure gradients\n",
    "        plain_grads = measure_gradient_flow(plain, \"Plain\")\n",
    "        res_grads = measure_gradient_flow(res, \"ResNet\")\n",
    "        \n",
    "        # Calculate ratio (first/last layer gradient)\n",
    "        plain_ratio = plain_grads[0] / (plain_grads[-1] + 1e-10)\n",
    "        res_ratio = res_grads[0] / (res_grads[-1] + 1e-10)\n",
    "        \n",
    "        plain_ratios.append(plain_ratio)\n",
    "        resnet_ratios.append(res_ratio)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(depths, plain_ratios, 'o-', label='Plain Network', linewidth=2, markersize=8)\n",
    "    plt.plot(depths, resnet_ratios, 's-', label='ResNet', linewidth=2, markersize=8)\n",
    "    plt.xlabel('Network Depth')\n",
    "    plt.ylabel('Gradient Ratio (first/last layer)')\n",
    "    plt.title('Gradient Flow Degradation with Depth')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nGradient Ratio (first/last) - Higher = Worse gradient flow:\")\n",
    "    for i, d in enumerate(depths):\n",
    "        print(f\"Depth {d:2d}: Plain={plain_ratios[i]:8.2f}, ResNet={resnet_ratios[i]:6.2f} \"\n",
    "              f\"(ResNet is {plain_ratios[i]/resnet_ratios[i]:.1f}x better)\")\n",
    "\n",
    "test_depth_scaling()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### The Degradation Problem:\n",
    "- Adding more layers to plain networks hurts performance\n",
    "- **Not** due to overfitting (training error also increases)\n",
    "- Due to optimization difficulty: vanishing/exploding gradients\n",
    "\n",
    "### ResNet Solution: Skip Connections\n",
    "```\n",
    "y = F(x, {Wi}) + x\n",
    "```\n",
    "\n",
    "**Instead of learning**: H(x) = desired mapping  \n",
    "**Learn residual**: F(x) = H(x) - x  \n",
    "**Then**: H(x) = F(x) + x\n",
    "\n",
    "### Why It Works:\n",
    "1. **Identity mapping is easier**: If optimal mapping is identity, F(x) = 0 is easier to learn than H(x) = x\n",
    "2. **Gradient highways**: Skip connections provide direct gradient paths\n",
    "3. **Additive gradient flow**: Gradients flow through both residual and skip paths\n",
    "4. **No extra parameters**: Skip connection is parameter-free\n",
    "\n",
    "### Impact:\n",
    "- Enabled 152-layer networks (vs 20-layer limit before)\n",
    "- Won ImageNet 2015 (3.57% top-5 error)\n",
    "- Became standard architecture pattern\n",
    "- Inspired variants: DenseNet, ResNeXt, etc.\n",
    "\n",
    "### Mathematical Insight:\n",
    "Gradient of loss L w.r.t. earlier layer:\n",
    "```\n",
    "∂L/∂x = ∂L/∂y * (∂F/∂x + ∂x/∂x) = ∂L/∂y * (∂F/∂x + I)\n",
    "```\n",
    "The `+ I` term ensures gradients always flow!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
