{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 3: Understanding LSTM Networks\n",
    "## Christopher Olah\n",
    "\n",
    "### Implementation of LSTM with Gate Visualization\n",
    "\n",
    "LSTM (Long Short-Term Memory) networks solve the vanishing gradient problem through gated memory cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "import random\n",
    "\n",
    "def randint(low, high=None, size=None):\n",
    "    \"\"\"\n",
    "    Compute randint.\n",
    "    \n",
    "    Args:\n",
    "        low: Input parameter.\n",
    "        high: Input parameter.\n",
    "        size: Size parameter.\n",
    "    Returns:\n",
    "        Computed result.\n",
    "    \"\"\"\n",
    "    if high is None:\n",
    "        high = low\n",
    "        low = 0\n",
    "    if size is None:\n",
    "        return int(torch.randint(low, high, (1,)).item())\n",
    "    return torch.randint(low, high, size)\n",
    "\n",
    "def uniform(low=0.0, high=1.0, size=None):\n",
    "    \"\"\"\n",
    "    Compute uniform.\n",
    "    \n",
    "    Args:\n",
    "        low: Input parameter.\n",
    "        high: Input parameter.\n",
    "        size: Size parameter.\n",
    "    Returns:\n",
    "        Computed result.\n",
    "    \"\"\"\n",
    "    if size is None:\n",
    "        return float(torch.empty(1).uniform_(low, high).item())\n",
    "    return torch.empty(size).uniform_(low, high)\n",
    "\n",
    "def choice(a, size=None, replace=True, p=None):\n",
    "    \"\"\"\n",
    "    Compute choice.\n",
    "    \n",
    "    Args:\n",
    "        a: Input parameter.\n",
    "        size: Size parameter.\n",
    "        replace: Input parameter.\n",
    "        p: Probability or probability vector.\n",
    "    Returns:\n",
    "        Computed result.\n",
    "    \"\"\"\n",
    "    if isinstance(a, int):\n",
    "        a = list(range(a))\n",
    "    if p is not None:\n",
    "        probs = torch.tensor(p, dtype=torch.float32)\n",
    "        if size is None:\n",
    "            idx = int(torch.multinomial(probs, 1, replacement=replace).item())\n",
    "            return a[idx]\n",
    "        idx = torch.multinomial(probs, size, replacement=replace).tolist()\n",
    "        return [a[i] for i in idx]\n",
    "    if size is None:\n",
    "        return random.choice(a)\n",
    "    return random.choices(a, k=size)\n",
    "\n",
    "\n",
    "# Torch-Numpy compatibility helpers\n",
    "if not hasattr(torch.Tensor, 'copy'):\n",
    "    torch.Tensor.copy = torch.Tensor.clone\n",
    "\n",
    "def _astype(self, dtype):\n",
    "    \"\"\"\n",
    "    Compute  astype.\n",
    "    \n",
    "    Args:\n",
    "        dtype: Input parameter.\n",
    "    Returns:\n",
    "        Computed result.\n",
    "    \"\"\"\n",
    "    if dtype is float:\n",
    "        return self.float()\n",
    "    if dtype is int:\n",
    "        return self.int()\n",
    "    return self.to(dtype)\n",
    "\n",
    "if not hasattr(torch.Tensor, 'astype'):\n",
    "    torch.Tensor.astype = _astype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Cell Implementation\n",
    "\n",
    "LSTM has three gates:\n",
    "1. **Forget Gate**: What to forget from cell state\n",
    "2. **Input Gate**: What new information to add\n",
    "3. **Output Gate**: What to output based on cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid.\n",
    "    \n",
    "    Args:\n",
    "        x: Input data.\n",
    "    Returns:\n",
    "        Computed result.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + torch.exp(-x))\n",
    "\n",
    "class LSTMCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Initialize the instance.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Size of input.\n",
    "            hidden_size: Size of hidden.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Concatenated weights for efficiency: [input; hidden] -> gates\n",
    "        concat_size = input_size + hidden_size\n",
    "        \n",
    "        # Forget gate\n",
    "        self.Wf = torch.randn(hidden_size, concat_size) * 0.01\n",
    "        self.bf = torch.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Input gate\n",
    "        self.Wi = torch.randn(hidden_size, concat_size) * 0.01\n",
    "        self.bi = torch.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Candidate cell state\n",
    "        self.Wc = torch.randn(hidden_size, concat_size) * 0.01\n",
    "        self.bc = torch.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Output gate\n",
    "        self.Wo = torch.randn(hidden_size, concat_size) * 0.01\n",
    "        self.bo = torch.zeros((hidden_size, 1))\n",
    "    \n",
    "    def forward(self, x, h_prev, c_prev):\n",
    "        \"\"\"\n",
    "        Run the forward pass value.\n",
    "        \n",
    "        Args:\n",
    "            x: Input data.\n",
    "            h_prev: Input parameter.\n",
    "            c_prev: Input parameter.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        # Concatenate input and previous hidden state\n",
    "        concat = torch.vstack([x, h_prev])\n",
    "        \n",
    "        # Forget gate: decides what to forget from cell state\n",
    "        f = sigmoid(torch.matmul(self.Wf, concat) + self.bf)\n",
    "        \n",
    "        # Input gate: decides what new information to store\n",
    "        i = sigmoid(torch.matmul(self.Wi, concat) + self.bi)\n",
    "        \n",
    "        # Candidate cell state: new information to potentially add\n",
    "        c_tilde = torch.tanh(torch.matmul(self.Wc, concat) + self.bc)\n",
    "        \n",
    "        # Update cell state: forget + input new information\n",
    "        c_next = f * c_prev + i * c_tilde\n",
    "        \n",
    "        # Output gate: decides what to output\n",
    "        o = sigmoid(torch.matmul(self.Wo, concat) + self.bo)\n",
    "        \n",
    "        # Hidden state: filtered cell state\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        \n",
    "        # Cache for backward pass\n",
    "        cache = (x, h_prev, c_prev, concat, f, i, c_tilde, c_next, o, h_next)\n",
    "        \n",
    "        return h_next, c_next, cache\n",
    "\n",
    "# Test LSTM cell\n",
    "input_size = 10\n",
    "hidden_size = 20\n",
    "lstm_cell = LSTMCell(input_size, hidden_size)\n",
    "\n",
    "x = torch.randn(input_size, 1)\n",
    "h = torch.zeros((hidden_size, 1))\n",
    "c = torch.zeros((hidden_size, 1))\n",
    "\n",
    "h_next, c_next, cache = lstm_cell.forward(x, h, c)\n",
    "print(f\"LSTM Cell initialized: input_size={input_size}, hidden_size={hidden_size}\")\n",
    "print(f\"Hidden state shape: {h_next.shape}\")\n",
    "print(f\"Cell state shape: {c_next.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full LSTM Network for Sequence Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize the instance.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Size of input.\n",
    "            hidden_size: Size of hidden.\n",
    "            output_size: Size of output.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        self.hidden_size = hidden_size\n",
    "        self.cell = LSTMCell(input_size, hidden_size)\n",
    "        \n",
    "        # Output layer\n",
    "        self.Why = torch.randn(output_size, hidden_size) * 0.01\n",
    "        self.by = torch.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Run the forward pass value.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input data.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        h = torch.zeros((self.hidden_size, 1))\n",
    "        c = torch.zeros((self.hidden_size, 1))\n",
    "        \n",
    "        # Store states for visualization\n",
    "        h_states = []\n",
    "        c_states = []\n",
    "        gate_values = {'f': [], 'i': [], 'o': []}\n",
    "        \n",
    "        for x in inputs:\n",
    "            h, c, cache = self.cell.forward(x, h, c)\n",
    "            h_states.append(h.copy())\n",
    "            c_states.append(c.copy())\n",
    "            \n",
    "            # Extract gate values from cache\n",
    "            _, _, _, _, f, i, _, _, o, _ = cache\n",
    "            gate_values['f'].append(f.copy())\n",
    "            gate_values['i'].append(i.copy())\n",
    "            gate_values['o'].append(o.copy())\n",
    "        \n",
    "        # Final output\n",
    "        y = torch.matmul(self.Why, h) + self.by\n",
    "        \n",
    "        return y, h_states, c_states, gate_values\n",
    "\n",
    "# Create LSTM model\n",
    "input_size = 5\n",
    "hidden_size = 16\n",
    "output_size = 5\n",
    "lstm = LSTM(input_size, hidden_size, output_size)\n",
    "print(f\"\\nLSTM model created: {input_size} -> {hidden_size} -> {output_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Synthetic Sequence Task: Long-Term Dependency\n",
    "\n",
    "Task: Remember a value from beginning of sequence and output it at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_long_term_dependency_data(seq_length=20, num_samples=100):\n",
    "    \"\"\"\n",
    "    Generate long term dependency data.\n",
    "    \n",
    "    Args:\n",
    "        seq_length: Length parameter.\n",
    "        num_samples: Number of samples.\n",
    "    Returns:\n",
    "        X: Input data.\n",
    "        y: Target labels or values.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Create sequence\n",
    "        sequence = []\n",
    "        \n",
    "        # First element is the important one (one-hot)\n",
    "        first_elem = randint(0, input_size)\n",
    "        first_vec = torch.zeros((input_size, 1))\n",
    "        first_vec[first_elem] = 1\n",
    "        sequence.append(first_vec)\n",
    "        \n",
    "        # Rest are random noise\n",
    "        for _ in range(seq_length - 1):\n",
    "            noise = torch.randn(input_size, 1) * 0.1\n",
    "            sequence.append(noise)\n",
    "        \n",
    "        X.append(sequence)\n",
    "        \n",
    "        # Target: remember first element\n",
    "        target = torch.zeros((output_size, 1))\n",
    "        target[first_elem] = 1\n",
    "        y.append(target)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate test data\n",
    "X_test, y_test = generate_long_term_dependency_data(seq_length=15, num_samples=10)\n",
    "\n",
    "# Test forward pass\n",
    "output, h_states, c_states, gate_values = lstm.forward(X_test[0])\n",
    "\n",
    "print(f\"\\nTest sequence length: {len(X_test[0])}\")\n",
    "print(f\"First element (to remember): {torch.argmax(X_test[0][0])}\")\n",
    "print(f\"Expected output: {torch.argmax(y_test[0])}\")\n",
    "print(f\"Model output (untrained): {output.flatten()[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize LSTM Gates\n",
    "\n",
    "The key to understanding LSTMs is seeing how gates operate over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process a sequence and visualize gates\n",
    "test_seq = X_test[0]\n",
    "output, h_states, c_states, gate_values = lstm.forward(test_seq)\n",
    "\n",
    "# Convert to arrays for plotting\n",
    "forget_gates = torch.hstack(gate_values['f'])\n",
    "input_gates = torch.hstack(gate_values['i'])\n",
    "output_gates = torch.hstack(gate_values['o'])\n",
    "cell_states = torch.hstack(c_states)\n",
    "hidden_states = torch.hstack(h_states)\n",
    "\n",
    "fig, axes = plt.subplots(5, 1, figsize=(14, 12))\n",
    "\n",
    "# Forget gate\n",
    "axes[0].imshow(forget_gates, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "axes[0].set_title('Forget Gate (1=keep, 0=forget)')\n",
    "axes[0].set_ylabel('Hidden Unit')\n",
    "axes[0].set_xlabel('Time Step')\n",
    "\n",
    "# Input gate\n",
    "axes[1].imshow(input_gates, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "axes[1].set_title('Input Gate (1=accept new, 0=ignore new)')\n",
    "axes[1].set_ylabel('Hidden Unit')\n",
    "axes[1].set_xlabel('Time Step')\n",
    "\n",
    "# Output gate\n",
    "axes[2].imshow(output_gates, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "axes[2].set_title('Output Gate (1=expose, 0=hide)')\n",
    "axes[2].set_ylabel('Hidden Unit')\n",
    "axes[2].set_xlabel('Time Step')\n",
    "\n",
    "# Cell state\n",
    "im3 = axes[3].imshow(cell_states, cmap='RdBu', aspect='auto')\n",
    "axes[3].set_title('Cell State (Long-term Memory)')\n",
    "axes[3].set_ylabel('Hidden Unit')\n",
    "axes[3].set_xlabel('Time Step')\n",
    "plt.colorbar(im3, ax=axes[3])\n",
    "\n",
    "# Hidden state\n",
    "im4 = axes[4].imshow(hidden_states, cmap='RdBu', aspect='auto')\n",
    "axes[4].set_title('Hidden State (Output to Next Layer)')\n",
    "axes[4].set_ylabel('Hidden Unit')\n",
    "axes[4].set_xlabel('Time Step')\n",
    "plt.colorbar(im4, ax=axes[4])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGate Interpretation:\")\n",
    "print(\"- Forget gate controls what information to discard from cell state\")\n",
    "print(\"- Input gate controls what new information to add to cell state\")\n",
    "print(\"- Output gate controls what to output from cell state\")\n",
    "print(\"- Cell state is the long-term memory highway\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare LSTM vs Vanilla RNN on Long Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaRNNCell:\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        \"\"\"\n",
    "        Initialize the instance.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Size of input.\n",
    "            hidden_size: Size of hidden.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        concat_size = input_size + hidden_size\n",
    "        self.Wh = torch.randn(hidden_size, concat_size) * 0.01\n",
    "        self.bh = torch.zeros((hidden_size, 1))\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, x, h_prev):\n",
    "        \"\"\"\n",
    "        Run the forward pass value.\n",
    "        \n",
    "        Args:\n",
    "            x: Input data.\n",
    "            h_prev: Input parameter.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        concat = torch.vstack([x, h_prev])\n",
    "        h_next = torch.tanh(torch.matmul(self.Wh, concat) + self.bh)\n",
    "        return h_next\n",
    "\n",
    "# Create vanilla RNN for comparison\n",
    "rnn_cell = VanillaRNNCell(input_size, hidden_size)\n",
    "\n",
    "def process_with_vanilla_rnn(inputs):\n",
    "    \"\"\"\n",
    "    Compute process with vanilla recurrent neural network.\n",
    "    \n",
    "    Args:\n",
    "        inputs: Input data.\n",
    "    Returns:\n",
    "        h_states: Input parameter.\n",
    "    \"\"\"\n",
    "    h = torch.zeros((hidden_size, 1))\n",
    "    h_states = []\n",
    "    \n",
    "    for x in inputs:\n",
    "        h = rnn_cell.forward(x, h)\n",
    "        h_states.append(h.copy())\n",
    "    \n",
    "    return h_states\n",
    "\n",
    "# Process same sequence with both\n",
    "rnn_h_states = process_with_vanilla_rnn(test_seq)\n",
    "rnn_hidden = torch.hstack(rnn_h_states)\n",
    "\n",
    "# Compare hidden state evolution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "im1 = ax1.imshow(rnn_hidden, cmap='RdBu', aspect='auto')\n",
    "ax1.set_title('Vanilla RNN Hidden States')\n",
    "ax1.set_ylabel('Hidden Unit')\n",
    "ax1.set_xlabel('Time Step')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "im2 = ax2.imshow(hidden_states, cmap='RdBu', aspect='auto')\n",
    "ax2.set_title('LSTM Hidden States')\n",
    "ax2.set_ylabel('Hidden Unit')\n",
    "ax2.set_xlabel('Time Step')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Difference:\")\n",
    "print(\"- LSTM maintains cell state separate from hidden state\")\n",
    "print(\"- Gates allow selective information flow\")\n",
    "print(\"- Better gradient flow through time (solves vanishing gradient)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Flow Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate gradient magnitudes\n",
    "def simulate_gradient_flow(seq_length=30):\n",
    "    \"\"\"\n",
    "    Simulate gradient flow.\n",
    "    \n",
    "    Args:\n",
    "        seq_length: Length parameter.\n",
    "    Returns:\n",
    "        item: Computed value.\n",
    "        item: Computed value.\n",
    "    \"\"\"\n",
    "    # Vanilla RNN: gradients decay exponentially\n",
    "    rnn_grads = []\n",
    "    grad = 1.0\n",
    "    decay_factor = 0.85  # Typical decay in vanilla RNN\n",
    "    \n",
    "    for t in range(seq_length):\n",
    "        rnn_grads.append(grad)\n",
    "        grad *= decay_factor\n",
    "    \n",
    "    # LSTM: gradients maintained through cell state highway\n",
    "    lstm_grads = []\n",
    "    grad = 1.0\n",
    "    forget_gate_avg = 0.95  # High forget gate = preserve gradients\n",
    "    \n",
    "    for t in range(seq_length):\n",
    "        lstm_grads.append(grad)\n",
    "        grad *= forget_gate_avg  # Forget gate controls gradient flow\n",
    "    \n",
    "    return torch.tensor(rnn_grads), torch.tensor(lstm_grads)\n",
    "\n",
    "rnn_grads, lstm_grads = simulate_gradient_flow()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(rnn_grads[::-1], label='Vanilla RNN', linewidth=2)\n",
    "plt.plot(lstm_grads[::-1], label='LSTM', linewidth=2)\n",
    "plt.xlabel('Timesteps in the Past')\n",
    "plt.ylabel('Gradient Magnitude')\n",
    "plt.title('Gradient Flow: LSTM vs Vanilla RNN')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nGradient after 30 steps:\")\n",
    "print(f\"Vanilla RNN: {rnn_grads[-1]:.6f} (vanished)\")\n",
    "print(f\"LSTM: {lstm_grads[-1]:.6f} (preserved)\")\n",
    "print(f\"\\nThis is why LSTM can learn long-term dependencies!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### LSTM Architecture:\n",
    "1. **Cell State**: Highway for information flow across time\n",
    "2. **Forget Gate**: Controls what to remove from memory\n",
    "3. **Input Gate**: Controls what new information to add\n",
    "4. **Output Gate**: Controls what to output from memory\n",
    "\n",
    "### Why LSTM Works:\n",
    "- **Constant Error Carousel**: Cell state provides uninterrupted gradient flow\n",
    "- **Multiplicative Gates**: Allow network to learn when to remember/forget\n",
    "- **Additive Updates**: Cell state updated by addition (f*c + i*c_tilde)\n",
    "- **Gradient Preservation**: Forget gate near 1 preserves gradients\n",
    "\n",
    "### Advantages over Vanilla RNN:\n",
    "- Solves vanishing gradient problem\n",
    "- Learns long-term dependencies (100+ timesteps)\n",
    "- More stable training\n",
    "- Better performance on real-world sequence tasks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}