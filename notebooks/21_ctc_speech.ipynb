{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 21: Deep Speech 2 - End-to-End Speech Recognition\n",
    "## Dario Amodei et al., Baidu Research (2015)\n",
    "\n",
    "### CTC Loss: Connectionist Temporal Classification\n",
    "\n",
    "CTC enables training sequence models without frame-level alignments. Critical for speech recognition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Alignment Problem\n",
    "\n",
    "Speech: \"hello\" → Audio frames: [h][h][e][e][l][l][l][o][o]\n",
    "\n",
    "Problem: We don't know which frames correspond to which letters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTC introduces blank symbol (ε) to handle alignment\n",
    "# Vocabulary: [a, b, c, ..., z, space, blank]\n",
    "\n",
    "vocab = list('abcdefghijklmnopqrstuvwxyz ') + ['ε']  # ε is blank\n",
    "char_to_idx = {ch: i for i, ch in enumerate(vocab)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(vocab)}\n",
    "\n",
    "blank_idx = len(vocab) - 1\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "print(f\"Blank index: {blank_idx}\")\n",
    "print(f\"Sample chars: {vocab[:10]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTC Alignment Rules\n",
    "\n",
    "**Collapse rule**: Remove blanks and repeated characters\n",
    "- `[h][ε][e][l][l][o]` → \"hello\"\n",
    "- `[h][h][e][ε][l][o]` → \"helo\" \n",
    "- `[h][ε][h][e][l][o]` → \"hhelo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collapse_ctc(sequence, blank_idx):\n",
    "    \"\"\"\n",
    "    Collapse CTC sequence to target string\n",
    "    1. Remove blanks\n",
    "    2. Merge repeated characters\n",
    "    \"\"\"\n",
    "    # Remove blanks\n",
    "    no_blanks = [s for s in sequence if s != blank_idx]\n",
    "    \n",
    "    # Merge repeats\n",
    "    if len(no_blanks) == 0:\n",
    "        return []\n",
    "    \n",
    "    collapsed = [no_blanks[0]]\n",
    "    for s in no_blanks[1:]:\n",
    "        if s != collapsed[-1]:\n",
    "            collapsed.append(s)\n",
    "    \n",
    "    return collapsed\n",
    "\n",
    "# Test collapse\n",
    "examples = [\n",
    "    [char_to_idx['h'], blank_idx, char_to_idx['e'], char_to_idx['l'], char_to_idx['l'], char_to_idx['o']],\n",
    "    [char_to_idx['h'], char_to_idx['h'], char_to_idx['e'], blank_idx, char_to_idx['l'], char_to_idx['o']],\n",
    "    [blank_idx, char_to_idx['h'], blank_idx, char_to_idx['i'], blank_idx],\n",
    "]\n",
    "\n",
    "for ex in examples:\n",
    "    original = ''.join([idx_to_char[i] for i in ex])\n",
    "    collapsed = collapse_ctc(ex, blank_idx)\n",
    "    result = ''.join([idx_to_char[i] for i in collapsed])\n",
    "    print(f\"{original:20s} → {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Audio Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_audio_features(text, frames_per_char=3, feature_dim=20):\n",
    "    \"\"\"\n",
    "    Simulate audio features (e.g., MFCCs)\n",
    "    In reality: extract from raw audio\n",
    "    \"\"\"\n",
    "    # Convert text to indices\n",
    "    char_indices = [char_to_idx[c] for c in text]\n",
    "    \n",
    "    # Generate features for each character (repeated frames)\n",
    "    features = []\n",
    "    for char_idx in char_indices:\n",
    "        # Create feature vector for this character\n",
    "        char_feature = np.random.randn(feature_dim) + char_idx * 0.1\n",
    "        \n",
    "        # Repeat for multiple frames (simulate speaking duration)\n",
    "        num_frames = np.random.randint(frames_per_char - 1, frames_per_char + 2)\n",
    "        for _ in range(num_frames):\n",
    "            # Add noise\n",
    "            features.append(char_feature + np.random.randn(feature_dim) * 0.3)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "# Generate sample\n",
    "text = \"hello\"\n",
    "features = generate_audio_features(text)\n",
    "\n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"Text length: {len(text)} characters\")\n",
    "print(f\"Audio features: {features.shape} (frames × features)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.imshow(features.T, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Feature Value')\n",
    "plt.xlabel('Time Frame')\n",
    "plt.ylabel('Feature Dimension')\n",
    "plt.title(f'Synthetic Audio Features for \"{text}\"')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple RNN Acoustic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcousticModel:\n",
    "    \"\"\"RNN that outputs character probabilities per frame\"\"\"\n",
    "    def __init__(self, feature_dim, hidden_size, vocab_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # RNN weights\n",
    "        self.W_xh = np.random.randn(hidden_size, feature_dim) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.b_h = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Output layer\n",
    "        self.W_out = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "        self.b_out = np.zeros((vocab_size, 1))\n",
    "    \n",
    "    def forward(self, features):\n",
    "        \"\"\"\n",
    "        features: (num_frames, feature_dim)\n",
    "        Returns: (num_frames, vocab_size) - log probabilities\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(len(features)):\n",
    "            x = features[t:t+1].T  # (feature_dim, 1)\n",
    "            \n",
    "            # RNN update\n",
    "            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h) + self.b_h)\n",
    "            \n",
    "            # Output (logits)\n",
    "            logits = np.dot(self.W_out, h) + self.b_out\n",
    "            \n",
    "            # Log softmax\n",
    "            log_probs = logits - np.log(np.sum(np.exp(logits)))\n",
    "            outputs.append(log_probs.flatten())\n",
    "        \n",
    "        return np.array(outputs)  # (num_frames, vocab_size)\n",
    "\n",
    "# Create model\n",
    "feature_dim = 20\n",
    "hidden_size = 32\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "model = AcousticModel(feature_dim, hidden_size, vocab_size)\n",
    "\n",
    "# Test forward pass\n",
    "log_probs = model.forward(features)\n",
    "print(f\"\\nAcoustic model output: {log_probs.shape}\")\n",
    "print(f\"Each frame has probability distribution over {vocab_size} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTC Forward Algorithm (Simplified)\n",
    "\n",
    "Computes probability of target sequence given frame-level predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_loss_naive(log_probs, target, blank_idx):\n",
    "    \"\"\"\n",
    "    Simplified CTC loss computation\n",
    "    \n",
    "    log_probs: (T, vocab_size) - log probabilities per frame\n",
    "    target: list of character indices (without blanks)\n",
    "    blank_idx: index of blank symbol\n",
    "    \n",
    "    This is a simplified version - full CTC uses dynamic programming\n",
    "    \"\"\"\n",
    "    T = len(log_probs)\n",
    "    U = len(target)\n",
    "    \n",
    "    # Insert blanks between characters: a → ε a ε b → ε a ε b ε\n",
    "    extended_target = [blank_idx]\n",
    "    for t in target:\n",
    "        extended_target.extend([t, blank_idx])\n",
    "    S = len(extended_target)\n",
    "    \n",
    "    # Forward algorithm with dynamic programming\n",
    "    # alpha[t, s] = prob of being at position s at time t\n",
    "    log_alpha = np.ones((T, S)) * -np.inf\n",
    "    \n",
    "    # Initialize\n",
    "    log_alpha[0, 0] = log_probs[0, extended_target[0]]\n",
    "    if S > 1:\n",
    "        log_alpha[0, 1] = log_probs[0, extended_target[1]]\n",
    "    \n",
    "    # Forward pass\n",
    "    for t in range(1, T):\n",
    "        for s in range(S):\n",
    "            label = extended_target[s]\n",
    "            \n",
    "            # Option 1: stay at same label (or blank)\n",
    "            candidates = [log_alpha[t-1, s]]\n",
    "            \n",
    "            # Option 2: transition from previous label\n",
    "            if s > 0:\n",
    "                candidates.append(log_alpha[t-1, s-1])\n",
    "            \n",
    "            # Option 3: skip blank (if current is not blank and different from prev)\n",
    "            if s > 1 and label != blank_idx and extended_target[s-2] != label:\n",
    "                candidates.append(log_alpha[t-1, s-2])\n",
    "            \n",
    "            # Log-sum-exp for numerical stability\n",
    "            log_alpha[t, s] = np.logaddexp.reduce(candidates) + log_probs[t, label]\n",
    "    \n",
    "    # Final probability: sum over last two positions (with/without final blank)\n",
    "    log_prob = np.logaddexp(log_alpha[T-1, S-1], log_alpha[T-1, S-2] if S > 1 else -np.inf)\n",
    "    \n",
    "    # CTC loss is negative log probability\n",
    "    return -log_prob, log_alpha\n",
    "\n",
    "# Test CTC loss\n",
    "target = [char_to_idx[c] for c in \"hi\"]\n",
    "loss, alpha = ctc_loss_naive(log_probs, target, blank_idx)\n",
    "\n",
    "print(f\"\\nTarget: 'hi'\")\n",
    "print(f\"CTC Loss: {loss:.4f}\")\n",
    "print(f\"Log probability: {-loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize CTC Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize forward probabilities (alpha)\n",
    "target_str = \"hi\"\n",
    "target_indices = [char_to_idx[c] for c in target_str]\n",
    "\n",
    "# Recompute with smaller example\n",
    "small_features = generate_audio_features(target_str, frames_per_char=2)\n",
    "small_log_probs = model.forward(small_features)\n",
    "loss, alpha = ctc_loss_naive(small_log_probs, target_indices, blank_idx)\n",
    "\n",
    "# Create extended target for visualization\n",
    "extended = [blank_idx]\n",
    "for t in target_indices:\n",
    "    extended.extend([t, blank_idx])\n",
    "extended_labels = [idx_to_char[i] for i in extended]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(alpha.T, cmap='hot', aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label='Log Probability')\n",
    "plt.xlabel('Time Frame')\n",
    "plt.ylabel('CTC State')\n",
    "plt.title(f'CTC Forward Algorithm for \"{target_str}\"')\n",
    "plt.yticks(range(len(extended_labels)), extended_labels)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBrighter cells = higher probability paths\")\n",
    "print(\"CTC explores all valid alignments!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy CTC Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(log_probs, blank_idx):\n",
    "    \"\"\"\n",
    "    Greedy decoding: pick most likely character at each frame\n",
    "    Then collapse using CTC rules\n",
    "    \"\"\"\n",
    "    # Get most likely character per frame\n",
    "    predictions = np.argmax(log_probs, axis=1)\n",
    "    \n",
    "    # Collapse\n",
    "    decoded = collapse_ctc(predictions.tolist(), blank_idx)\n",
    "    \n",
    "    return decoded, predictions\n",
    "\n",
    "# Test decoding\n",
    "test_text = \"hello\"\n",
    "test_features = generate_audio_features(test_text)\n",
    "test_log_probs = model.forward(test_features)\n",
    "\n",
    "decoded, raw_predictions = greedy_decode(test_log_probs, blank_idx)\n",
    "\n",
    "print(f\"True text: '{test_text}'\")\n",
    "print(f\"\\nFrame-by-frame predictions:\")\n",
    "print(''.join([idx_to_char[i] for i in raw_predictions]))\n",
    "print(f\"\\nAfter CTC collapse:\")\n",
    "print(''.join([idx_to_char[i] for i in decoded]))\n",
    "print(f\"\\n(Model is untrained, so prediction is random)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Predictions vs Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probability distribution over time\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Plot log probabilities\n",
    "ax1.imshow(test_log_probs.T, cmap='viridis', aspect='auto')\n",
    "ax1.set_ylabel('Character')\n",
    "ax1.set_xlabel('Time Frame')\n",
    "ax1.set_title('Log Probabilities per Frame (darker = higher prob)')\n",
    "ax1.set_yticks(range(0, vocab_size, 5))\n",
    "ax1.set_yticklabels([vocab[i] for i in range(0, vocab_size, 5)])\n",
    "\n",
    "# Plot predictions\n",
    "ax2.plot(raw_predictions, 'o-', markersize=6)\n",
    "ax2.set_xlabel('Time Frame')\n",
    "ax2.set_ylabel('Predicted Character Index')\n",
    "ax2.set_title('Greedy Predictions')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### The CTC Problem:\n",
    "- **Unknown alignment**: Don't know which audio frames → which characters\n",
    "- **Variable length**: Audio has more frames than output characters\n",
    "- **No segmentation**: Don't know where words/characters start/end\n",
    "\n",
    "### CTC Solution:\n",
    "1. **Blank symbol (ε)**: Allows repetition and silence\n",
    "2. **All alignments**: Sum over all valid paths\n",
    "3. **End-to-end**: Train without frame-level labels\n",
    "\n",
    "### CTC Rules:\n",
    "```\n",
    "1. Insert blanks: \"cat\" → \"ε c ε a ε t ε\"\n",
    "2. Any path that collapses to target is valid\n",
    "3. Sum probabilities of all valid paths\n",
    "```\n",
    "\n",
    "### Forward Algorithm:\n",
    "- Dynamic programming over time and label positions\n",
    "- α[t, s] = probability of being at position s at time t\n",
    "- Three transitions: stay, move forward, skip blank\n",
    "\n",
    "### Loss:\n",
    "$$\\mathcal{L}_{CTC} = -\\log P(y|x) = -\\log \\sum_{\\pi \\in \\mathcal{B}^{-1}(y)} P(\\pi|x)$$\n",
    "\n",
    "Where $\\mathcal{B}^{-1}(y)$ is all alignments that collapse to y\n",
    "\n",
    "### Decoding:\n",
    "1. **Greedy**: Pick best character per frame, collapse\n",
    "2. **Beam search**: Keep top-k hypotheses\n",
    "3. **Prefix beam search**: Better for CTC (used in production)\n",
    "\n",
    "### Deep Speech 2 Architecture:\n",
    "```\n",
    "Audio → Features (MFCCs/spectrograms)\n",
    "  ↓\n",
    "Convolution layers (capture local patterns)\n",
    "  ↓\n",
    "RNN layers (bidirectional GRU/LSTM)\n",
    "  ↓\n",
    "Fully connected layer\n",
    "  ↓\n",
    "Softmax (character probabilities)\n",
    "  ↓\n",
    "CTC Loss\n",
    "```\n",
    "\n",
    "### Advantages:\n",
    "- ✅ No alignment needed\n",
    "- ✅ End-to-end trainable\n",
    "- ✅ Handles variable lengths\n",
    "- ✅ Works for any sequence task\n",
    "\n",
    "### Limitations:\n",
    "- ❌ Independence assumption (each frame independent)\n",
    "- ❌ Can't model output dependencies well\n",
    "- ❌ Monotonic alignment only\n",
    "\n",
    "### Modern Alternatives:\n",
    "- **Attention-based**: Seq2seq with attention (Listen, Attend, Spell)\n",
    "- **Transducers**: RNN-T combines CTC + attention\n",
    "- **Transformers**: Wav2Vec 2.0, Whisper\n",
    "\n",
    "### Applications:\n",
    "- Speech recognition\n",
    "- Handwriting recognition  \n",
    "- OCR\n",
    "- Keyword spotting\n",
    "- Any task with unknown alignment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
