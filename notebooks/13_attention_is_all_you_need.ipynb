{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 13: Attention Is All You Need\n",
    "## Vaswani et al. (2017)\n",
    "\n",
    "### The Transformer: Pure Attention Architecture\n",
    "\n",
    "Revolutionary architecture that replaced RNNs with self-attention, enabling modern LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention\n",
    "\n",
    "The fundamental building block:\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    \"\"\"Numerically stable softmax\"\"\"\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    exp_x = np.exp(x - x_max)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Scaled Dot-Product Attention\n",
    "    \n",
    "    Q: Queries (seq_len_q, d_k)\n",
    "    K: Keys (seq_len_k, d_k)\n",
    "    V: Values (seq_len_v, d_v)\n",
    "    mask: Optional mask (seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # Compute attention scores\n",
    "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
    "    \n",
    "    # Apply mask if provided (for causality or padding)\n",
    "    if mask is not None:\n",
    "        scores = scores + (mask * -1e9)\n",
    "    \n",
    "    # Softmax to get attention weights\n",
    "    attention_weights = softmax(scores, axis=-1)\n",
    "    \n",
    "    # Weighted sum of values\n",
    "    output = np.dot(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "# Test scaled dot-product attention\n",
    "seq_len = 5\n",
    "d_model = 8\n",
    "\n",
    "Q = np.random.randn(seq_len, d_model)\n",
    "K = np.random.randn(seq_len, d_model)\n",
    "V = np.random.randn(seq_len, d_model)\n",
    "\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(f\"Attention output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"Attention weights sum (should be 1): {attn_weights.sum(axis=1)}\")\n",
    "\n",
    "# Visualize attention pattern\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(attn_weights, cmap='viridis', aspect='auto')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('Key Position')\n",
    "plt.ylabel('Query Position')\n",
    "plt.title('Attention Weights Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "Multiple attention \"heads\" attend to different aspects of the input:\n",
    "$$\\text{MultiHead}(Q,K,V) = \\text{Concat}(head_1, ..., head_h)W^O$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections for Q, K, V for all heads (parallelized)\n",
    "        self.W_q = np.random.randn(d_model, d_model) * 0.1\n",
    "        self.W_k = np.random.randn(d_model, d_model) * 0.1\n",
    "        self.W_v = np.random.randn(d_model, d_model) * 0.1\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = np.random.randn(d_model, d_model) * 0.1\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split into multiple heads: (seq_len, d_model) -> (num_heads, seq_len, d_k)\"\"\"\n",
    "        seq_len = x.shape[0]\n",
    "        x = x.reshape(seq_len, self.num_heads, self.d_k)\n",
    "        return x.transpose(1, 0, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Combine heads: (num_heads, seq_len, d_k) -> (seq_len, d_model)\"\"\"\n",
    "        seq_len = x.shape[1]\n",
    "        x = x.transpose(1, 0, 2)\n",
    "        return x.reshape(seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Multi-head attention forward pass\n",
    "        \n",
    "        Q, K, V: (seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Linear projections\n",
    "        Q = np.dot(Q, self.W_q.T)\n",
    "        K = np.dot(K, self.W_k.T)\n",
    "        V = np.dot(V, self.W_v.T)\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q = self.split_heads(Q)  # (num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Apply attention to each head\n",
    "        head_outputs = []\n",
    "        self.attention_weights = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            head_out, head_attn = scaled_dot_product_attention(\n",
    "                Q[i], K[i], V[i], mask\n",
    "            )\n",
    "            head_outputs.append(head_out)\n",
    "            self.attention_weights.append(head_attn)\n",
    "        \n",
    "        # Stack heads\n",
    "        heads = np.stack(head_outputs, axis=0)  # (num_heads, seq_len, d_k)\n",
    "        \n",
    "        # Combine heads\n",
    "        combined = self.combine_heads(heads)  # (seq_len, d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = np.dot(combined, self.W_o.T)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test multi-head attention\n",
    "d_model = 64\n",
    "num_heads = 8\n",
    "seq_len = 10\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "output = mha.forward(X, X, X)  # Self-attention\n",
    "\n",
    "print(f\"\\nMulti-Head Attention:\")\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of heads: {num_heads}\")\n",
    "print(f\"Dimension per head: {mha.d_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encoding\n",
    "\n",
    "Since Transformers have no recurrence, we add position information:\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Create sinusoidal positional encoding\n",
    "    \"\"\"\n",
    "    pe = np.zeros((seq_len, d_model))\n",
    "    \n",
    "    position = np.arange(0, seq_len)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    \n",
    "    # Apply sin to even indices\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    \n",
    "    # Apply cos to odd indices\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    \n",
    "    return pe\n",
    "\n",
    "# Generate positional encodings\n",
    "seq_len = 50\n",
    "d_model = 64\n",
    "pe = positional_encoding(seq_len, d_model)\n",
    "\n",
    "# Visualize positional encodings\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(pe.T, cmap='RdBu', aspect='auto')\n",
    "plt.colorbar(label='Encoding Value')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Dimension')\n",
    "plt.title('Positional Encoding (All Dimensions)')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "# Plot first few dimensions\n",
    "for i in [0, 1, 2, 3, 10, 20]:\n",
    "    plt.plot(pe[:, i], label=f'Dim {i}')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Encoding Value')\n",
    "plt.title('Positional Encoding (Selected Dimensions)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Positional encoding shape: {pe.shape}\")\n",
    "print(f\"Different frequencies encode position at different scales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward Network\n",
    "\n",
    "Applied to each position independently:\n",
    "$$FFN(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward:\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        self.W1 = np.random.randn(d_model, d_ff) * 0.1\n",
    "        self.b1 = np.zeros(d_ff)\n",
    "        self.W2 = np.random.randn(d_ff, d_model) * 0.1\n",
    "        self.b2 = np.zeros(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # First layer with ReLU\n",
    "        hidden = np.maximum(0, np.dot(x, self.W1) + self.b1)\n",
    "        \n",
    "        # Second layer\n",
    "        output = np.dot(hidden, self.W2) + self.b2\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Test feed-forward\n",
    "d_model = 64\n",
    "d_ff = 256  # Usually 4x larger\n",
    "\n",
    "ff = FeedForward(d_model, d_ff)\n",
    "x = np.random.randn(10, d_model)\n",
    "output = ff.forward(x)\n",
    "\n",
    "print(f\"\\nFeed-Forward Network:\")\n",
    "print(f\"Input: {x.shape}\")\n",
    "print(f\"Hidden: ({x.shape[0]}, {d_ff})\")\n",
    "print(f\"Output: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Normalization\n",
    "\n",
    "Normalize across features (not batch like BatchNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm:\n",
    "    def __init__(self, d_model, eps=1e-6):\n",
    "        self.gamma = np.ones(d_model)\n",
    "        self.beta = np.zeros(d_model)\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(axis=-1, keepdims=True)\n",
    "        std = x.std(axis=-1, keepdims=True)\n",
    "        \n",
    "        normalized = (x - mean) / (std + self.eps)\n",
    "        output = self.gamma * normalized + self.beta\n",
    "        \n",
    "        return output\n",
    "\n",
    "ln = LayerNorm(d_model)\n",
    "x = np.random.randn(10, d_model) * 3 + 5  # Unnormalized\n",
    "normalized = ln.forward(x)\n",
    "\n",
    "print(f\"\\nLayer Normalization:\")\n",
    "print(f\"Input mean: {x.mean():.4f}, std: {x.std():.4f}\")\n",
    "print(f\"Output mean: {normalized.mean():.4f}, std: {normalized.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock:\n",
    "    def __init__(self, d_model, num_heads, d_ff):\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model, d_ff)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Multi-head attention with residual connection\n",
    "        attn_output = self.attention.forward(x, x, x, mask)\n",
    "        x = self.norm1.forward(x + attn_output)\n",
    "        \n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.ff.forward(x)\n",
    "        x = self.norm2.forward(x + ff_output)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Test transformer block\n",
    "block = TransformerBlock(d_model=64, num_heads=8, d_ff=256)\n",
    "x = np.random.randn(10, 64)\n",
    "output = block.forward(x)\n",
    "\n",
    "print(f\"\\nTransformer Block:\")\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"\\nBlock contains:\")\n",
    "print(f\"  1. Multi-Head Self-Attention\")\n",
    "print(f\"  2. Layer Normalization\")\n",
    "print(f\"  3. Feed-Forward Network\")\n",
    "print(f\"  4. Residual Connections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Multi-Head Attention Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention with interpretable input\n",
    "seq_len = 8\n",
    "d_model = 64\n",
    "num_heads = 4\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "output = mha.forward(X, X, X)\n",
    "\n",
    "# Plot attention patterns for each head\n",
    "fig, axes = plt.subplots(1, num_heads, figsize=(16, 4))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    attn = mha.attention_weights[i]\n",
    "    im = ax.imshow(attn, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
    "    ax.set_title(f'Head {i+1}')\n",
    "    ax.set_xlabel('Key')\n",
    "    ax.set_ylabel('Query')\n",
    "    \n",
    "plt.colorbar(im, ax=axes, label='Attention Weight', fraction=0.046, pad=0.04)\n",
    "plt.suptitle('Multi-Head Attention Patterns', fontsize=14, y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nEach head learns to attend to different patterns!\")\n",
    "print(\"Different heads capture different relationships in the data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal (Masked) Self-Attention for Autoregressive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"Create mask to prevent attending to future positions\"\"\"\n",
    "    mask = np.triu(np.ones((seq_len, seq_len)), k=1)\n",
    "    return mask\n",
    "\n",
    "# Test causal attention\n",
    "seq_len = 8\n",
    "causal_mask = create_causal_mask(seq_len)\n",
    "\n",
    "Q = np.random.randn(seq_len, d_model)\n",
    "K = np.random.randn(seq_len, d_model)\n",
    "V = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# Without mask (bidirectional)\n",
    "output_bi, attn_bi = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "# With causal mask (unidirectional)\n",
    "output_causal, attn_causal = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "# Visualize difference\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Causal mask\n",
    "ax1.imshow(causal_mask, cmap='Reds', aspect='auto')\n",
    "ax1.set_title('Causal Mask\\n(1 = masked/not allowed)')\n",
    "ax1.set_xlabel('Key Position')\n",
    "ax1.set_ylabel('Query Position')\n",
    "\n",
    "# Bidirectional attention\n",
    "im2 = ax2.imshow(attn_bi, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
    "ax2.set_title('Bidirectional Attention\\n(can see future)')\n",
    "ax2.set_xlabel('Key Position')\n",
    "ax2.set_ylabel('Query Position')\n",
    "\n",
    "# Causal attention\n",
    "im3 = ax3.imshow(attn_causal, cmap='viridis', aspect='auto', vmin=0, vmax=1)\n",
    "ax3.set_title('Causal Attention\\n(cannot see future)')\n",
    "ax3.set_xlabel('Key Position')\n",
    "ax3.set_ylabel('Query Position')\n",
    "\n",
    "plt.colorbar(im3, ax=[ax2, ax3], label='Attention Weight')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCausal masking is crucial for:\")\n",
    "print(\"  - Autoregressive generation (GPT, language models)\")\n",
    "print(\"  - Prevents information leakage from future tokens\")\n",
    "print(\"  - Each position can only attend to itself and previous positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Why \"Attention Is All You Need\"?\n",
    "- **No recurrence**: Processes entire sequence in parallel\n",
    "- **No convolution**: Pure attention mechanism\n",
    "- **Scales better**: O(nÂ²d) vs O(n) sequential operations in RNNs\n",
    "- **Long-range dependencies**: Direct connections between any positions\n",
    "\n",
    "### Core Components:\n",
    "1. **Scaled Dot-Product Attention**: Efficient attention computation\n",
    "2. **Multi-Head Attention**: Multiple representation subspaces\n",
    "3. **Positional Encoding**: Inject position information\n",
    "4. **Feed-Forward Networks**: Position-wise transformations\n",
    "5. **Layer Normalization**: Stabilize training\n",
    "6. **Residual Connections**: Enable deep networks\n",
    "\n",
    "### Architecture Variants:\n",
    "- **Encoder-Decoder**: Original Transformer (translation)\n",
    "- **Encoder-only**: BERT (bidirectional understanding)\n",
    "- **Decoder-only**: GPT (autoregressive generation)\n",
    "\n",
    "### Advantages:\n",
    "- Parallelizable training (unlike RNNs)\n",
    "- Better long-range dependencies\n",
    "- Interpretable attention patterns\n",
    "- State-of-the-art on many tasks\n",
    "\n",
    "### Impact:\n",
    "- Foundation of modern NLP: GPT, BERT, T5, etc.\n",
    "- Extended to vision: Vision Transformer (ViT)\n",
    "- Multi-modal models: CLIP, Flamingo\n",
    "- Enabled LLMs with billions of parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
