{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 19: The Coffee Automaton - A Deep Dive into Irreversibility\n",
    "\n",
    "**Paper**: Scott Aaronson (2016) - \"The Coffee Automaton\"\n",
    "\n",
    "**Central Question**: Why can't you unmix coffee and milk? What does irreversibility tell us about computation, information, and the nature of time itself?\n",
    "\n",
    "---\n",
    "\n",
    "## The Puzzle of Irreversibility\n",
    "\n",
    "Drop milk into coffee. Watch it spread, swirl, and mix until it's uniform. Now try to reverse it. **You can't.**\n",
    "\n",
    "But here's the puzzle:\n",
    "- Newton's laws are **time-reversible**: Running them backward is perfectly valid\n",
    "- Every collision between molecules is reversible\n",
    "- The microscopic laws don't prefer any direction of time\n",
    "\n",
    "So **where does irreversibility come from?**\n",
    "\n",
    "This isn't just physicsâ€”it's deeply connected to:\n",
    "- **Computation**: Can we reverse calculations?\n",
    "- **Information**: What does \"forgetting\" really mean?\n",
    "- **Machine Learning**: Why do neural networks compress information?\n",
    "- **The arrow of time**: Why does time have a direction?\n",
    "\n",
    "---\n",
    "\n",
    "## This Implementation\n",
    "\n",
    "We'll explore irreversibility from multiple angles:\n",
    "\n",
    "1. **Coffee Mixing**: Diffusion and entropy growth\n",
    "2. **Phase Space**: Where reversibility lives\n",
    "3. **Coarse-Graining**: How we lose information\n",
    "4. **PoincarÃ© Recurrence**: The universe *will* unmix (eventually)\n",
    "5. **Maxwell's Demon**: Can intelligence reverse entropy?\n",
    "6. **Landauer's Principle**: Information erasure costs energy\n",
    "7. **Computational Irreversibility**: One-way functions and hashing\n",
    "8. **Machine Learning**: Why neural nets compress and forget\n",
    "9. **The Second Law**: Statistical mechanics perspective\n",
    "10. **Modern Insights**: Thermodynamics of computation\n",
    "\n",
    "This goes far beyond Paper 1's introduction to complexity. We'll dive deep into **why** irreversibility emerges and what it means for computation and learning.\n",
    "\n",
    "Let's begin!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from scipy.ndimage import convolve\n",
    "from scipy.stats import entropy as scipy_entropy\n",
    "from typing import Tuple, List, Callable\n",
    "import hashlib\n",
    "from dataclasses import dataclass\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"\\nReady to explore irreversibility...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Coffee Mixing - The Diffusion Process\n",
    "\n",
    "Let's start with the classic example: milk diffusing into coffee.\n",
    "\n",
    "## The Diffusion Equation\n",
    "\n",
    "The concentration $c(x, y, t)$ of milk evolves according to:\n",
    "\n",
    "$$\\frac{\\partial c}{\\partial t} = D \\nabla^2 c = D \\left(\\frac{\\partial^2 c}{\\partial x^2} + \\frac{\\partial^2 c}{\\partial y^2}\\right)$$\n",
    "\n",
    "where $D$ is the **diffusion coefficient**.\n",
    "\n",
    "**Key insight**: This equation has an **arrow of time**. It looks different running backward:\n",
    "- Forward: Concentration spreads out (milk mixes)\n",
    "- Backward: Concentration would gather together (milk unmixes)\n",
    "\n",
    "But the second scenario violates the second law of thermodynamics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_coffee_cup(size: int = 64) -> np.ndarray:\n",
    "    \"\"\"Initialize a 'coffee cup' with a drop of milk in the center.\n",
    "    \n",
    "    Returns:\n",
    "        concentration: 2D array where 1 = milk, 0 = coffee\n",
    "    \"\"\"\n",
    "    cup = np.zeros((size, size))\n",
    "    \n",
    "    # Add a drop of milk in the center\n",
    "    center = size // 2\n",
    "    radius = size // 8\n",
    "    \n",
    "    y, x = np.ogrid[:size, :size]\n",
    "    mask = (x - center)**2 + (y - center)**2 <= radius**2\n",
    "    cup[mask] = 1.0\n",
    "    \n",
    "    return cup\n",
    "\n",
    "\n",
    "def diffusion_step(concentration: np.ndarray, D: float = 0.1) -> np.ndarray:\n",
    "    \"\"\"One timestep of diffusion using finite differences.\n",
    "    \n",
    "    Args:\n",
    "        concentration: Current concentration field\n",
    "        D: Diffusion coefficient\n",
    "    \n",
    "    Returns:\n",
    "        New concentration after one timestep\n",
    "    \"\"\"\n",
    "    # Laplacian kernel (discrete âˆ‡Â²)\n",
    "    kernel = np.array([[0, 1, 0],\n",
    "                       [1, -4, 1],\n",
    "                       [0, 1, 0]])\n",
    "    \n",
    "    # Apply Laplacian\n",
    "    laplacian = convolve(concentration, kernel, mode='constant', cval=0.0)\n",
    "    \n",
    "    # Update: c(t+Î”t) = c(t) + DÂ·Î”tÂ·âˆ‡Â²c\n",
    "    dt = 0.1\n",
    "    new_concentration = concentration + D * dt * laplacian\n",
    "    \n",
    "    # Keep concentrations in valid range\n",
    "    new_concentration = np.clip(new_concentration, 0, 1)\n",
    "    \n",
    "    return new_concentration\n",
    "\n",
    "\n",
    "def simulate_coffee_mixing(steps: int = 200, D: float = 0.1) -> List[np.ndarray]:\n",
    "    \"\"\"Simulate coffee mixing over time.\n",
    "    \n",
    "    Returns:\n",
    "        List of concentration fields at each timestep\n",
    "    \"\"\"\n",
    "    cup = initialize_coffee_cup()\n",
    "    history = [cup.copy()]\n",
    "    \n",
    "    for _ in range(steps):\n",
    "        cup = diffusion_step(cup, D)\n",
    "        history.append(cup.copy())\n",
    "    \n",
    "    return history\n",
    "\n",
    "\n",
    "# Simulate mixing\n",
    "print(\"Simulating coffee mixing...\\n\")\n",
    "mixing_history = simulate_coffee_mixing(steps=200)\n",
    "\n",
    "# Show key frames\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "timesteps = [0, 25, 50, 100, 200]\n",
    "\n",
    "for ax, t in zip(axes, timesteps):\n",
    "    im = ax.imshow(mixing_history[t], cmap='RdYlBu_r', vmin=0, vmax=1)\n",
    "    ax.set_title(f't = {t}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.colorbar(im, ax=axes, label='Milk Concentration', fraction=0.046)\n",
    "plt.suptitle('Coffee Mixing: Irreversible Diffusion', fontsize=14, fontweight='bold', y=1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observation: The milk spreads out and can never spontaneously unmix!\")\n",
    "print(\"\\nâœ“ Diffusion simulation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Entropy Growth - Quantifying Irreversibility\n",
    "\n",
    "**Entropy** measures disorder or \"spreadoutness\". As milk mixes, entropy increases.\n",
    "\n",
    "## Shannon Entropy\n",
    "\n",
    "Divide the cup into bins. The Shannon entropy is:\n",
    "\n",
    "$$H = -\\sum_i p_i \\log_2 p_i$$\n",
    "\n",
    "where $p_i$ is the probability of finding milk in bin $i$.\n",
    "\n",
    "## Thermodynamic Entropy\n",
    "\n",
    "Related to the number of microstates $\\Omega$ consistent with a macrostate:\n",
    "\n",
    "$$S = k_B \\ln \\Omega$$\n",
    "\n",
    "**Second Law of Thermodynamics**: In an isolated system, entropy never decreases:\n",
    "\n",
    "$$\\frac{dS}{dt} \\geq 0$$\n",
    "\n",
    "This is the arrow of time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_shannon_entropy(concentration: np.ndarray, num_bins: int = 10) -> float:\n",
    "    \"\"\"Compute Shannon entropy of concentration distribution.\n",
    "    \n",
    "    Args:\n",
    "        concentration: 2D concentration field\n",
    "        num_bins: Number of bins for histogram\n",
    "    \n",
    "    Returns:\n",
    "        Shannon entropy in bits\n",
    "    \"\"\"\n",
    "    # Flatten and create histogram\n",
    "    flat = concentration.flatten()\n",
    "    hist, _ = np.histogram(flat, bins=num_bins, range=(0, 1), density=True)\n",
    "    \n",
    "    # Normalize to probabilities\n",
    "    hist = hist / hist.sum()\n",
    "    \n",
    "    # Compute Shannon entropy\n",
    "    return scipy_entropy(hist, base=2)\n",
    "\n",
    "\n",
    "def compute_spatial_entropy(concentration: np.ndarray) -> float:\n",
    "    \"\"\"Compute spatial entropy (variance of concentration).\n",
    "    \n",
    "    As mixing proceeds, variance decreases (becomes more uniform).\n",
    "    So we use negative variance as a measure of mixing.\n",
    "    \"\"\"\n",
    "    return -np.var(concentration)\n",
    "\n",
    "\n",
    "def compute_mixing_quality(concentration: np.ndarray) -> float:\n",
    "    \"\"\"Compute how well-mixed the cup is.\n",
    "    \n",
    "    Returns:\n",
    "        Value in [0, 1] where 1 = perfectly mixed\n",
    "    \"\"\"\n",
    "    # Perfect mixing = all pixels have same concentration\n",
    "    mean_concentration = concentration.mean()\n",
    "    variance = np.var(concentration)\n",
    "    \n",
    "    # Maximum variance (for this mean) is when half is 0, half is 2*mean\n",
    "    max_variance = mean_concentration * (1 - mean_concentration)\n",
    "    \n",
    "    if max_variance == 0:\n",
    "        return 1.0\n",
    "    \n",
    "    return 1 - (variance / max_variance)\n",
    "\n",
    "\n",
    "# Compute entropy over time\n",
    "print(\"Computing entropy over time...\\n\")\n",
    "\n",
    "shannon_entropies = [compute_shannon_entropy(cup) for cup in mixing_history]\n",
    "spatial_entropies = [compute_spatial_entropy(cup) for cup in mixing_history]\n",
    "mixing_qualities = [compute_mixing_quality(cup) for cup in mixing_history]\n",
    "\n",
    "# Plot entropy growth\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Shannon entropy\n",
    "axes[0].plot(shannon_entropies, linewidth=2, color='darkblue')\n",
    "axes[0].set_xlabel('Time Step', fontsize=11)\n",
    "axes[0].set_ylabel('Shannon Entropy (bits)', fontsize=11)\n",
    "axes[0].set_title('Information Entropy Growth', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].axhline(y=shannon_entropies[0], color='red', linestyle='--', alpha=0.5, label='Initial')\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Spatial entropy (negative variance)\n",
    "axes[1].plot(spatial_entropies, linewidth=2, color='darkgreen')\n",
    "axes[1].set_xlabel('Time Step', fontsize=11)\n",
    "axes[1].set_ylabel('Spatial Entropy (-Variance)', fontsize=11)\n",
    "axes[1].set_title('Spatial Disorder Growth', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Mixing quality\n",
    "axes[2].plot(mixing_qualities, linewidth=2, color='darkorange')\n",
    "axes[2].set_xlabel('Time Step', fontsize=11)\n",
    "axes[2].set_ylabel('Mixing Quality', fontsize=11)\n",
    "axes[2].set_title('Approach to Equilibrium', fontsize=12, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].axhline(y=1.0, color='red', linestyle='--', alpha=0.5, label='Perfect mixing')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Initial Shannon entropy: {shannon_entropies[0]:.3f} bits\")\n",
    "print(f\"Final Shannon entropy:   {shannon_entropies[-1]:.3f} bits\")\n",
    "print(f\"Entropy increase:        {shannon_entropies[-1] - shannon_entropies[0]:.3f} bits\")\n",
    "print(f\"\\nFinal mixing quality:    {mixing_qualities[-1]:.1%}\")\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insight: Entropy monotonically increases!\")\n",
    "print(\"   This is the second law of thermodynamics in action.\")\n",
    "print(\"\\nâœ“ Entropy analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Phase Space and Liouville's Theorem\n",
    "\n",
    "Here's where it gets deep: **Why does entropy increase if the microscopic laws are reversible?**\n",
    "\n",
    "## Phase Space\n",
    "\n",
    "Consider a system with $N$ particles. The **phase space** is the 6N-dimensional space of all positions and momenta:\n",
    "\n",
    "$$\\Gamma = (x_1, y_1, z_1, p_{x1}, p_{y1}, p_{z1}, \\ldots, x_N, y_N, z_N, p_{xN}, p_{yN}, p_{zN})$$\n",
    "\n",
    "A point in phase space is a **microstate** - complete specification of the system.\n",
    "\n",
    "## Liouville's Theorem\n",
    "\n",
    "**Liouville's theorem** says phase space volume is conserved:\n",
    "\n",
    "$$\\frac{d}{dt} \\int_{V} d\\Gamma = 0$$\n",
    "\n",
    "This means:\n",
    "- **Microscopically, dynamics are reversible**\n",
    "- Phase space volume doesn't change\n",
    "- Information is conserved\n",
    "\n",
    "## The Resolution: Coarse-Graining\n",
    "\n",
    "The key is **coarse-graining**: We can't track individual molecules, so we group nearby microstates into **macrostates**.\n",
    "\n",
    "- **Microstate**: Exact positions/momenta of all molecules (reversible)\n",
    "- **Macrostate**: Coarse description like \"temperature\" or \"concentration\" (irreversible)\n",
    "\n",
    "**Entropy increases** because:\n",
    "1. Many microstates map to the same macrostate\n",
    "2. Evolution tends toward macrostates with more microstates\n",
    "3. We lose information when coarse-graining\n",
    "\n",
    "Let's simulate this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Particle:\n",
    "    \"\"\"A particle in 2D phase space.\"\"\"\n",
    "    x: float\n",
    "    y: float\n",
    "    vx: float\n",
    "    vy: float\n",
    "\n",
    "\n",
    "def initialize_particles(num_particles: int = 100, region: str = 'left') -> List[Particle]:\n",
    "    \"\"\"Initialize particles in a specific region.\n",
    "    \n",
    "    Args:\n",
    "        num_particles: Number of particles\n",
    "        region: 'left' or 'right' half of box\n",
    "    \"\"\"\n",
    "    particles = []\n",
    "    \n",
    "    for _ in range(num_particles):\n",
    "        if region == 'left':\n",
    "            x = np.random.uniform(0.1, 0.4)\n",
    "        else:\n",
    "            x = np.random.uniform(0.6, 0.9)\n",
    "        \n",
    "        y = np.random.uniform(0.1, 0.9)\n",
    "        \n",
    "        # Random velocities\n",
    "        speed = 0.02\n",
    "        angle = np.random.uniform(0, 2*np.pi)\n",
    "        vx = speed * np.cos(angle)\n",
    "        vy = speed * np.sin(angle)\n",
    "        \n",
    "        particles.append(Particle(x, y, vx, vy))\n",
    "    \n",
    "    return particles\n",
    "\n",
    "\n",
    "def update_particles(particles: List[Particle], dt: float = 1.0) -> List[Particle]:\n",
    "    \"\"\"Update particle positions (free motion with reflecting boundaries).\"\"\"\n",
    "    new_particles = []\n",
    "    \n",
    "    for p in particles:\n",
    "        # Update position\n",
    "        new_x = p.x + p.vx * dt\n",
    "        new_y = p.y + p.vy * dt\n",
    "        new_vx, new_vy = p.vx, p.vy\n",
    "        \n",
    "        # Reflecting boundaries\n",
    "        if new_x < 0 or new_x > 1:\n",
    "            new_vx = -new_vx\n",
    "            new_x = np.clip(new_x, 0, 1)\n",
    "        \n",
    "        if new_y < 0 or new_y > 1:\n",
    "            new_vy = -new_vy\n",
    "            new_y = np.clip(new_y, 0, 1)\n",
    "        \n",
    "        new_particles.append(Particle(new_x, new_y, new_vx, new_vy))\n",
    "    \n",
    "    return new_particles\n",
    "\n",
    "\n",
    "def compute_macrostate(particles: List[Particle], num_bins: int = 4) -> np.ndarray:\n",
    "    \"\"\"Coarse-grain particles into spatial bins.\n",
    "    \n",
    "    Returns:\n",
    "        2D histogram of particle counts\n",
    "    \"\"\"\n",
    "    positions = np.array([[p.x, p.y] for p in particles])\n",
    "    \n",
    "    hist, _, _ = np.histogram2d(\n",
    "        positions[:, 0], positions[:, 1],\n",
    "        bins=num_bins,\n",
    "        range=[[0, 1], [0, 1]]\n",
    "    )\n",
    "    \n",
    "    return hist\n",
    "\n",
    "\n",
    "def compute_macrostate_entropy(macrostate: np.ndarray) -> float:\n",
    "    \"\"\"Compute entropy of a macrostate.\"\"\"\n",
    "    # Flatten and normalize\n",
    "    counts = macrostate.flatten()\n",
    "    if counts.sum() == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    probs = counts / counts.sum()\n",
    "    probs = probs[probs > 0]  # Remove zeros\n",
    "    \n",
    "    return -np.sum(probs * np.log2(probs))\n",
    "\n",
    "\n",
    "# Simulate particle mixing\n",
    "print(\"Simulating particle mixing with coarse-graining...\\n\")\n",
    "\n",
    "# Initialize particles on left side\n",
    "particles = initialize_particles(num_particles=200, region='left')\n",
    "\n",
    "# Simulate\n",
    "num_steps = 500\n",
    "particle_history = [particles]\n",
    "macrostate_history = []\n",
    "macrostate_entropies = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    particles = update_particles(particles)\n",
    "    particle_history.append([Particle(p.x, p.y, p.vx, p.vy) for p in particles])\n",
    "    \n",
    "    # Compute macrostate every 10 steps\n",
    "    if step % 10 == 0:\n",
    "        macrostate = compute_macrostate(particles)\n",
    "        macrostate_history.append(macrostate)\n",
    "        macrostate_entropies.append(compute_macrostate_entropy(macrostate))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "# Top row: Microstate (particle positions)\n",
    "timesteps_vis = [0, 100, 500]\n",
    "for idx, t in enumerate(timesteps_vis):\n",
    "    ax = axes[0, idx]\n",
    "    ps = particle_history[t]\n",
    "    positions = np.array([[p.x, p.y] for p in ps])\n",
    "    \n",
    "    ax.scatter(positions[:, 0], positions[:, 1], s=10, alpha=0.6, color='blue')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_title(f'Microstate at t={t}', fontweight='bold')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "# Bottom row: Macrostate (coarse-grained)\n",
    "macro_timesteps = [0, 10, 50]\n",
    "for idx, mt in enumerate(macro_timesteps):\n",
    "    ax = axes[1, idx]\n",
    "    im = ax.imshow(macrostate_history[mt].T, cmap='Blues', origin='lower', \n",
    "                   extent=[0, 1, 0, 1], vmin=0, vmax=macrostate_history[0].max())\n",
    "    ax.set_title(f'Macrostate at t={mt*10}\\nS={macrostate_entropies[mt]:.2f} bits', \n",
    "                fontweight='bold')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    plt.colorbar(im, ax=ax, label='Particle count')\n",
    "\n",
    "plt.suptitle('Microscopic Reversibility vs Macroscopic Irreversibility', \n",
    "            fontsize=14, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot entropy growth\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(np.arange(len(macrostate_entropies)) * 10, macrostate_entropies, \n",
    "         linewidth=2, color='darkred')\n",
    "plt.xlabel('Time Step', fontsize=12)\n",
    "plt.ylabel('Macroscopic Entropy (bits)', fontsize=12)\n",
    "plt.title('Entropy Growth Despite Microscopic Reversibility', fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ðŸ”‘ Key insight: Liouville's theorem guarantees microscopic reversibility.\")\n",
    "print(\"   But macroscopic entropy STILL increases due to coarse-graining!\")\n",
    "print(\"\\n   â€¢ Microstate: Exact particle positions (reversible)\")\n",
    "print(\"   â€¢ Macrostate: Binned particle counts (irreversible)\")\n",
    "print(\"   â€¢ Many microstates â†’ same macrostate\")\n",
    "print(\"   â€¢ Evolution explores more microstates over time\")\n",
    "print(\"\\nâœ“ Phase space analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: PoincarÃ© Recurrence - The Universe Will Unmix!\n",
    "\n",
    "Here's a startling fact: **PoincarÃ© recurrence theorem** says that any finite system will eventually return arbitrarily close to its initial state.\n",
    "\n",
    "## The Theorem\n",
    "\n",
    "For a **finite** phase space volume, almost every initial condition will recur infinitely often:\n",
    "\n",
    "$$\\lim_{t \\to \\infty} \\inf \\, d(\\Gamma(t), \\Gamma(0)) = 0$$\n",
    "\n",
    "This means:\n",
    "- Given enough time, the coffee **will** unmix!\n",
    "- The particles **will** return to their starting configuration!\n",
    "\n",
    "## The Catch: Recurrence Time\n",
    "\n",
    "How long do you need to wait? For $N$ particles:\n",
    "\n",
    "$$t_{\\text{recurrence}} \\sim e^{N}$$\n",
    "\n",
    "For a coffee cup with $N \\sim 10^{23}$ molecules:\n",
    "\n",
    "$$t_{\\text{recurrence}} \\sim 10^{10^{23}} \\text{ seconds}$$\n",
    "\n",
    "This is **vastly** longer than the age of the universe ($\\sim 10^{17}$ seconds).\n",
    "\n",
    "**Practical irreversibility**: The system is theoretically reversible but practically irreversible.\n",
    "\n",
    "Let's demonstrate this with a small system!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_phase_space_system(num_states: int = 8, num_steps: int = 10000) -> Tuple[List[int], List[int]]:\n",
    "    \"\"\"Simulate a simple deterministic system on a finite phase space.\n",
    "    \n",
    "    Args:\n",
    "        num_states: Size of phase space (small for demonstration)\n",
    "        num_steps: Number of steps to simulate\n",
    "    \n",
    "    Returns:\n",
    "        states: List of states visited\n",
    "        recurrence_times: Time to return to initial state\n",
    "    \"\"\"\n",
    "    # Define a deterministic evolution rule\n",
    "    # Simple example: state_next = (a * state + b) mod num_states\n",
    "    a, b = 3, 1  # Parameters chosen to give interesting dynamics\n",
    "    \n",
    "    initial_state = 0\n",
    "    state = initial_state\n",
    "    states = [state]\n",
    "    recurrence_times = []\n",
    "    \n",
    "    for step in range(1, num_steps):\n",
    "        # Deterministic evolution\n",
    "        state = (a * state + b) % num_states\n",
    "        states.append(state)\n",
    "        \n",
    "        # Check for recurrence\n",
    "        if state == initial_state:\n",
    "            recurrence_times.append(step)\n",
    "    \n",
    "    return states, recurrence_times\n",
    "\n",
    "\n",
    "def estimate_recurrence_time_scaling() -> Tuple[List[int], List[float]]:\n",
    "    \"\"\"Estimate how recurrence time scales with system size.\"\"\"\n",
    "    sizes = [4, 8, 16, 32, 64, 128]\n",
    "    avg_recurrence_times = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        _, rec_times = simple_phase_space_system(num_states=size, num_steps=size*20)\n",
    "        if len(rec_times) > 0:\n",
    "            avg_time = np.mean(np.diff([0] + rec_times))\n",
    "        else:\n",
    "            avg_time = size * 10  # Estimate\n",
    "        avg_recurrence_times.append(avg_time)\n",
    "    \n",
    "    return sizes, avg_recurrence_times\n",
    "\n",
    "\n",
    "# Demonstrate PoincarÃ© recurrence\n",
    "print(\"Demonstrating PoincarÃ© recurrence...\\n\")\n",
    "\n",
    "num_states = 16\n",
    "states, recurrence_times = simple_phase_space_system(num_states=num_states, num_steps=200)\n",
    "\n",
    "print(f\"System with {num_states} states\")\n",
    "print(f\"Initial state: {states[0]}\")\n",
    "print(f\"\\nRecurrences found at timesteps: {recurrence_times[:5]}...\")\n",
    "\n",
    "if len(recurrence_times) > 1:\n",
    "    period = recurrence_times[1] - recurrence_times[0]\n",
    "    print(f\"Recurrence period: {period} steps\")\n",
    "\n",
    "# Visualize state evolution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: State trajectory\n",
    "ax = axes[0, 0]\n",
    "ax.plot(states[:200], linewidth=1.5, color='darkblue')\n",
    "for rt in recurrence_times:\n",
    "    if rt < 200:\n",
    "        ax.axvline(rt, color='red', alpha=0.3, linestyle='--')\n",
    "ax.set_xlabel('Time Step', fontsize=11)\n",
    "ax.set_ylabel('State', fontsize=11)\n",
    "ax.set_title(f'State Evolution (Red lines = recurrence)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: State histogram\n",
    "ax = axes[0, 1]\n",
    "ax.hist(states, bins=num_states, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax.set_xlabel('State', fontsize=11)\n",
    "ax.set_ylabel('Frequency', fontsize=11)\n",
    "ax.set_title('State Distribution (Should be uniform)', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Recurrence time scaling\n",
    "ax = axes[1, 0]\n",
    "sizes, rec_times_avg = estimate_recurrence_time_scaling()\n",
    "ax.semilogy(sizes, rec_times_avg, 'o-', linewidth=2, markersize=8, color='darkgreen', label='Observed')\n",
    "# Fit exponential\n",
    "log_rec_times = np.log(rec_times_avg)\n",
    "coeffs = np.polyfit(sizes, log_rec_times, 1)\n",
    "fit_line = np.exp(coeffs[1]) * np.exp(coeffs[0] * np.array(sizes))\n",
    "ax.semilogy(sizes, fit_line, '--', linewidth=2, color='red', label=f'Exponential fit')\n",
    "ax.set_xlabel('System Size (# states)', fontsize=11)\n",
    "ax.set_ylabel('Average Recurrence Time', fontsize=11)\n",
    "ax.set_title('Exponential Growth of Recurrence Time', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Extrapolation to realistic systems\n",
    "ax = axes[1, 1]\n",
    "N_values = np.array([10, 100, 1000, 10000, 1e23])  # Up to coffee cup\n",
    "t_rec_estimates = np.exp(N_values * 0.5)  # Very rough estimate\n",
    "universe_age = 4.3e17  # seconds\n",
    "\n",
    "ax.loglog(N_values[:-1], t_rec_estimates[:-1], 'o-', linewidth=2, markersize=8, color='purple')\n",
    "ax.axhline(universe_age, color='red', linestyle='--', linewidth=2, label='Age of universe')\n",
    "ax.set_xlabel('Number of Particles', fontsize=11)\n",
    "ax.set_ylabel('Recurrence Time (seconds)', fontsize=11)\n",
    "ax.set_title('Why Coffee Never Unmixes', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, which='both')\n",
    "ax.text(100, 1e20, f'Coffee cup:\\n$N \\\\sim 10^{{23}}$\\n$t_{{rec}} \\\\sim 10^{{10^{{23}}}}$ s', \n",
    "        fontsize=10, bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights:\")\n",
    "print(\"   1. PoincarÃ© recurrence IS real - finite systems eventually recur\")\n",
    "print(\"   2. But recurrence time grows EXPONENTIALLY with system size\")\n",
    "print(f\"   3. For coffee (~10Â²Â³ particles): t_rec ~ 10^(10^23) seconds\")\n",
    "print(f\"   4. Universe age: ~10^17 seconds\")\n",
    "print(\"   5. So coffee will unmix... after waiting 10^(10^23) times the age of the universe!\")\n",
    "print(\"\\n   This is why irreversibility is PRACTICAL even though dynamics are reversible.\")\n",
    "print(\"\\nâœ“ PoincarÃ© recurrence analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Maxwell's Demon - Can Intelligence Reverse Entropy?\n",
    "\n",
    "**Maxwell's Demon** is a thought experiment that challenges the second law.\n",
    "\n",
    "## The Setup\n",
    "\n",
    "1. Box divided into two chambers by a partition with a door\n",
    "2. Gas molecules move randomly\n",
    "3. A \"demon\" operates the door:\n",
    "   - Opens for fast molecules going right\n",
    "   - Opens for slow molecules going left\n",
    "4. Result: Hot gas on right, cold on left (entropy decreases!)\n",
    "\n",
    "## The Paradox\n",
    "\n",
    "The demon appears to **violate the second law** by decreasing entropy without doing work.\n",
    "\n",
    "## The Resolution: Landauer's Principle\n",
    "\n",
    "The demon must **measure** molecule speeds and **store** this information. Eventually, it must **erase** its memory to continue operating.\n",
    "\n",
    "**Landauer's principle** (1961): Erasing one bit of information requires dissipating at least:\n",
    "\n",
    "$$E_{\\text{min}} = k_B T \\ln 2$$\n",
    "\n",
    "of energy as heat.\n",
    "\n",
    "This heat dissipation **increases entropy** by exactly the amount the demon decreased it!\n",
    "\n",
    "**Profound implication**: **Information is physical**. Computation has thermodynamic costs.\n",
    "\n",
    "Let's simulate Maxwell's demon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GasParticle:\n",
    "    \"\"\"A gas particle with position and velocity.\"\"\"\n",
    "    x: float\n",
    "    y: float\n",
    "    vx: float\n",
    "    vy: float\n",
    "    \n",
    "    def speed(self) -> float:\n",
    "        return np.sqrt(self.vx**2 + self.vy**2)\n",
    "\n",
    "\n",
    "class MaxwellsDemon:\n",
    "    \"\"\"Simulates Maxwell's demon thought experiment.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_particles: int = 100):\n",
    "        self.num_particles = num_particles\n",
    "        self.particles = self._initialize_particles()\n",
    "        self.door_position = 0.5  # Middle of box\n",
    "        self.memory = []  # Demon's memory\n",
    "        self.entropy_cost = 0.0  # Cost of erasure\n",
    "    \n",
    "    def _initialize_particles(self) -> List[GasParticle]:\n",
    "        \"\"\"Initialize particles with Maxwell-Boltzmann distribution.\"\"\"\n",
    "        particles = []\n",
    "        for _ in range(self.num_particles):\n",
    "            x = np.random.uniform(0.1, 0.9)\n",
    "            y = np.random.uniform(0.1, 0.9)\n",
    "            \n",
    "            # Maxwell-Boltzmann velocity distribution\n",
    "            speed = np.random.rayleigh(0.02)\n",
    "            angle = np.random.uniform(0, 2*np.pi)\n",
    "            vx = speed * np.cos(angle)\n",
    "            vy = speed * np.sin(angle)\n",
    "            \n",
    "            particles.append(GasParticle(x, y, vx, vy))\n",
    "        return particles\n",
    "    \n",
    "    def update_without_demon(self, dt: float = 1.0):\n",
    "        \"\"\"Update particles without demon (natural evolution).\"\"\"\n",
    "        new_particles = []\n",
    "        for p in self.particles:\n",
    "            new_x = p.x + p.vx * dt\n",
    "            new_y = p.y + p.vy * dt\n",
    "            new_vx, new_vy = p.vx, p.vy\n",
    "            \n",
    "            # Reflecting boundaries\n",
    "            if new_x < 0 or new_x > 1:\n",
    "                new_vx = -new_vx\n",
    "                new_x = np.clip(new_x, 0, 1)\n",
    "            if new_y < 0 or new_y > 1:\n",
    "                new_vy = -new_vy\n",
    "                new_y = np.clip(new_y, 0, 1)\n",
    "            \n",
    "            new_particles.append(GasParticle(new_x, new_y, new_vx, new_vy))\n",
    "        \n",
    "        self.particles = new_particles\n",
    "    \n",
    "    def update_with_demon(self, dt: float = 1.0, threshold_speed: float = 0.025):\n",
    "        \"\"\"Update particles with demon operating the door.\"\"\"\n",
    "        new_particles = []\n",
    "        \n",
    "        for p in self.particles:\n",
    "            new_x = p.x + p.vx * dt\n",
    "            new_y = p.y + p.vy * dt\n",
    "            new_vx, new_vy = p.vx, p.vy\n",
    "            \n",
    "            # Check if particle crosses middle partition\n",
    "            crosses_door = (p.x < self.door_position <= new_x) or (p.x > self.door_position >= new_x)\n",
    "            \n",
    "            if crosses_door and 0.4 < new_y < 0.6:  # Door is in middle vertically\n",
    "                # Demon measures speed and decides\n",
    "                speed = p.speed()\n",
    "                self.memory.append(speed)  # Store measurement (costs memory)\n",
    "                \n",
    "                # Demon's rule:\n",
    "                # - Fast particles go right\n",
    "                # - Slow particles go left\n",
    "                going_right = new_x > p.x\n",
    "                is_fast = speed > threshold_speed\n",
    "                \n",
    "                if (going_right and not is_fast) or (not going_right and is_fast):\n",
    "                    # Reflect the particle (close door)\n",
    "                    new_vx = -new_vx\n",
    "                    new_x = p.x\n",
    "            \n",
    "            # Regular boundaries\n",
    "            if new_x < 0 or new_x > 1:\n",
    "                new_vx = -new_vx\n",
    "                new_x = np.clip(new_x, 0, 1)\n",
    "            if new_y < 0 or new_y > 1:\n",
    "                new_vy = -new_vy\n",
    "                new_y = np.clip(new_y, 0, 1)\n",
    "            \n",
    "            new_particles.append(GasParticle(new_x, new_y, new_vx, new_vy))\n",
    "        \n",
    "        self.particles = new_particles\n",
    "        \n",
    "        # Landauer erasure cost\n",
    "        if len(self.memory) > 50:  # Erase old memories\n",
    "            bits_erased = len(self.memory) - 50\n",
    "            self.entropy_cost += bits_erased * np.log(2)  # k_B T ln(2) per bit\n",
    "            self.memory = self.memory[-50:]\n",
    "    \n",
    "    def compute_temperature_difference(self) -> float:\n",
    "        \"\"\"Compute temperature difference between left and right chambers.\"\"\"\n",
    "        left_particles = [p for p in self.particles if p.x < self.door_position]\n",
    "        right_particles = [p for p in self.particles if p.x >= self.door_position]\n",
    "        \n",
    "        if len(left_particles) == 0 or len(right_particles) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        # Temperature âˆ average kinetic energy âˆ average speedÂ²\n",
    "        left_temp = np.mean([p.speed()**2 for p in left_particles])\n",
    "        right_temp = np.mean([p.speed()**2 for p in right_particles])\n",
    "        \n",
    "        return right_temp - left_temp\n",
    "\n",
    "\n",
    "# Simulate with and without demon\n",
    "print(\"Simulating Maxwell's demon...\\n\")\n",
    "\n",
    "# Without demon\n",
    "system_no_demon = MaxwellsDemon(num_particles=150)\n",
    "temp_diffs_no_demon = []\n",
    "\n",
    "for _ in range(300):\n",
    "    system_no_demon.update_without_demon()\n",
    "    temp_diffs_no_demon.append(system_no_demon.compute_temperature_difference())\n",
    "\n",
    "# With demon\n",
    "system_with_demon = MaxwellsDemon(num_particles=150)\n",
    "temp_diffs_with_demon = []\n",
    "entropy_costs = []\n",
    "\n",
    "for _ in range(300):\n",
    "    system_with_demon.update_with_demon()\n",
    "    temp_diffs_with_demon.append(system_with_demon.compute_temperature_difference())\n",
    "    entropy_costs.append(system_with_demon.entropy_cost)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Temperature difference without demon\n",
    "axes[0, 0].plot(temp_diffs_no_demon, linewidth=2, color='blue', alpha=0.7)\n",
    "axes[0, 0].axhline(0, color='black', linestyle='--', alpha=0.3)\n",
    "axes[0, 0].set_xlabel('Time Step', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Temp Diff (Right - Left)', fontsize=11)\n",
    "axes[0, 0].set_title('Without Demon: No Temperature Gradient', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Temperature difference with demon\n",
    "axes[0, 1].plot(temp_diffs_with_demon, linewidth=2, color='red', alpha=0.7)\n",
    "axes[0, 1].axhline(0, color='black', linestyle='--', alpha=0.3)\n",
    "axes[0, 1].set_xlabel('Time Step', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Temp Diff (Right - Left)', fontsize=11)\n",
    "axes[0, 1].set_title('With Demon: Temperature Gradient Created!', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Particle distribution (final state without demon)\n",
    "ax = axes[1, 0]\n",
    "positions_no_demon = np.array([[p.x, p.y] for p in system_no_demon.particles])\n",
    "speeds_no_demon = np.array([p.speed() for p in system_no_demon.particles])\n",
    "scatter = ax.scatter(positions_no_demon[:, 0], positions_no_demon[:, 1], \n",
    "                    c=speeds_no_demon, s=20, cmap='coolwarm', alpha=0.6)\n",
    "ax.axvline(0.5, color='black', linewidth=2, linestyle='--', label='Partition')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Final State Without Demon', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "plt.colorbar(scatter, ax=ax, label='Speed')\n",
    "\n",
    "# Plot 4: Particle distribution (final state with demon)\n",
    "ax = axes[1, 1]\n",
    "positions_with_demon = np.array([[p.x, p.y] for p in system_with_demon.particles])\n",
    "speeds_with_demon = np.array([p.speed() for p in system_with_demon.particles])\n",
    "scatter = ax.scatter(positions_with_demon[:, 0], positions_with_demon[:, 1],\n",
    "                    c=speeds_with_demon, s=20, cmap='coolwarm', alpha=0.6)\n",
    "ax.axvline(0.5, color='black', linewidth=2, linestyle='--', label='Partition')\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_xlabel('x', fontsize=11)\n",
    "ax.set_ylabel('y', fontsize=11)\n",
    "ax.set_title('Final State With Demon: Fastâ†’Right, Slowâ†’Left', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "plt.colorbar(scatter, ax=ax, label='Speed')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Landauer's principle\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(entropy_costs, linewidth=2, color='darkgreen')\n",
    "plt.xlabel('Time Step', fontsize=12)\n",
    "plt.ylabel('Cumulative Entropy Cost (k_B ln 2 per bit)', fontsize=12)\n",
    "plt.title(\"Landauer's Principle: Information Erasure Has Entropic Cost\", \n",
    "         fontsize=13, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights:\")\n",
    "print(\"   1. Demon CAN create temperature gradient (decrease entropy locally)\")\n",
    "print(\"   2. But demon must MEASURE and REMEMBER particle speeds\")\n",
    "print(\"   3. Memory is finite â†’ must ERASE old measurements\")\n",
    "print(f\"   4. Landauer: Erasing 1 bit releases â‰¥ k_B T ln(2) heat\")\n",
    "print(f\"   5. Total entropy cost from erasure: {entropy_costs[-1]:.2f} k_B ln(2)\")\n",
    "print(\"   6. This EXACTLY compensates for entropy decrease!\")\n",
    "print(\"\\n   â†’ The second law is saved! Information is physical.\")\n",
    "print(\"\\nâœ“ Maxwell's demon simulation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Computational Irreversibility - One-Way Functions\n",
    "\n",
    "Irreversibility isn't just physicsâ€”it's central to **computation**!\n",
    "\n",
    "## One-Way Functions\n",
    "\n",
    "A function $f$ is **one-way** if:\n",
    "1. Easy to compute: $y = f(x)$ is fast\n",
    "2. Hard to invert: Given $y$, finding $x$ such that $f(x) = y$ is hard\n",
    "\n",
    "**Examples**:\n",
    "- **Multiplication**: $f(p, q) = p \\times q$ (easy)\n",
    "- **Factoring**: Given $n = p \\times q$, find $p, q$ (hard for large $n$)\n",
    "- **Cryptographic hashing**: SHA-256, etc.\n",
    "\n",
    "## Connection to Thermodynamics\n",
    "\n",
    "One-way functions are **computationally irreversible**:\n",
    "- Information is **destroyed** when computing $f(x) \\to y$\n",
    "- Many inputs $x$ map to same output $y$\n",
    "- This is like **coarse-graining** in thermodynamics!\n",
    "\n",
    "## Landauer's Principle for Computation\n",
    "\n",
    "**Irreversible computation** (where information is destroyed) has a minimum energy cost:\n",
    "\n",
    "$$E_{\\text{min}} = k_B T \\ln(2) \\times (\\text{bits destroyed})$$\n",
    "\n",
    "**Reversible computation** (where no information is destroyed) can in principle be done with **zero energy cost**!\n",
    "\n",
    "This is why quantum computers are designed to be reversible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_function(x: int, num_bits_out: int = 8) -> int:\n",
    "    \"\"\"Simple hash function (one-way).\n",
    "    \n",
    "    Args:\n",
    "        x: Input integer\n",
    "        num_bits_out: Number of bits in output\n",
    "    \n",
    "    Returns:\n",
    "        Hash value in [0, 2^num_bits_out - 1]\n",
    "    \"\"\"\n",
    "    # Use Python's built-in hash with modulo\n",
    "    return hash(x) % (2 ** num_bits_out)\n",
    "\n",
    "\n",
    "def cryptographic_hash(data: str) -> str:\n",
    "    \"\"\"Cryptographic hash using SHA-256.\"\"\"\n",
    "    return hashlib.sha256(data.encode()).hexdigest()\n",
    "\n",
    "\n",
    "def demonstrate_collision(num_inputs: int = 1000, num_bits_out: int = 8) -> dict:\n",
    "    \"\"\"Demonstrate hash collisions (many inputs â†’ same output).\"\"\"\n",
    "    hash_map = {}\n",
    "    \n",
    "    for x in range(num_inputs):\n",
    "        h = hash_function(x, num_bits_out)\n",
    "        if h not in hash_map:\n",
    "            hash_map[h] = []\n",
    "        hash_map[h].append(x)\n",
    "    \n",
    "    return hash_map\n",
    "\n",
    "\n",
    "def compute_information_loss(input_bits: int, output_bits: int) -> float:\n",
    "    \"\"\"Compute information loss in bits.\n",
    "    \n",
    "    When hashing n-bit input to m-bit output (m < n),\n",
    "    we lose (n - m) bits of information.\n",
    "    \"\"\"\n",
    "    return max(0, input_bits - output_bits)\n",
    "\n",
    "\n",
    "# Demonstrate one-way functions\n",
    "print(\"Demonstrating computational irreversibility...\\n\")\n",
    "\n",
    "# Example 1: Simple hash collisions\n",
    "print(\"Example 1: Hash Collisions\")\n",
    "print(\"=\"*50)\n",
    "num_inputs = 1000\n",
    "num_bits_out = 6  # Only 64 possible outputs\n",
    "hash_map = demonstrate_collision(num_inputs, num_bits_out)\n",
    "\n",
    "print(f\"Hashed {num_inputs} inputs to {2**num_bits_out} possible outputs\")\n",
    "print(f\"Number of collisions: {sum(1 for v in hash_map.values() if len(v) > 1)}\")\n",
    "print(f\"Average inputs per output: {num_inputs / len(hash_map):.1f}\")\n",
    "\n",
    "# Show some collisions\n",
    "collision_example = [v for v in hash_map.values() if len(v) > 2][0]\n",
    "print(f\"\\nExample collision: Inputs {collision_example[:5]} all hash to {hash_function(collision_example[0], num_bits_out)}\")\n",
    "\n",
    "# Example 2: Cryptographic hash\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Example 2: Cryptographic Hash (SHA-256)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "messages = [\n",
    "    \"Hello, World!\",\n",
    "    \"Hello, World.\",  # One character different\n",
    "    \"The coffee automaton demonstrates irreversibility\"\n",
    "]\n",
    "\n",
    "for msg in messages:\n",
    "    hash_val = cryptographic_hash(msg)\n",
    "    print(f\"Message: '{msg[:40]}'\")\n",
    "    print(f\"SHA-256: {hash_val[:32]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"Note: Tiny change in input â†’ completely different hash (avalanche effect)\")\n",
    "print(\"This makes inversion computationally infeasible!\")\n",
    "\n",
    "# Visualize hash distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Hash collision distribution\n",
    "ax = axes[0]\n",
    "collision_counts = [len(v) for v in hash_map.values()]\n",
    "ax.hist(collision_counts, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "ax.set_xlabel('Number of Inputs per Hash Value', fontsize=11)\n",
    "ax.set_ylabel('Frequency', fontsize=11)\n",
    "ax.set_title('Hash Collisions: Many-to-One Mapping', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.axvline(np.mean(collision_counts), color='red', linestyle='--', linewidth=2,\n",
    "          label=f'Mean: {np.mean(collision_counts):.1f}')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 2: Information loss\n",
    "ax = axes[1]\n",
    "input_bits_range = np.arange(8, 65, 4)\n",
    "output_bits_fixed = 16\n",
    "info_loss = [compute_information_loss(ib, output_bits_fixed) for ib in input_bits_range]\n",
    "\n",
    "ax.plot(input_bits_range, info_loss, 'o-', linewidth=2, markersize=8, color='darkred')\n",
    "ax.fill_between(input_bits_range, 0, info_loss, alpha=0.3, color='red')\n",
    "ax.set_xlabel('Input Size (bits)', fontsize=11)\n",
    "ax.set_ylabel('Information Lost (bits)', fontsize=11)\n",
    "ax.set_title(f'Information Loss in Hashing (â†’{output_bits_fixed} bits)', \n",
    "            fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Landauer's cost\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Landauer's Principle for Computation\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "bits_destroyed = 48  # Hash 64-bit input to 16-bit output\n",
    "k_B = 1.380649e-23  # Boltzmann constant (J/K)\n",
    "T = 300  # Temperature (K)\n",
    "energy_per_bit = k_B * T * np.log(2)  # Joules\n",
    "total_energy = bits_destroyed * energy_per_bit\n",
    "\n",
    "print(f\"Hashing 64-bit input â†’ 16-bit output:\")\n",
    "print(f\"  Bits destroyed: {bits_destroyed}\")\n",
    "print(f\"  Minimum energy cost: {total_energy:.2e} J\")\n",
    "print(f\"  At T=300K: {energy_per_bit:.2e} J per bit\")\n",
    "print(f\"\\nModern CPUs use ~10^-9 J per operation (well above Landauer limit)\")\n",
    "print(f\"Landauer limit: {energy_per_bit:.2e} J per bit erased\")\n",
    "print(f\"Gap: ~10^6Ã— above minimum! Room for improvement.\")\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights:\")\n",
    "print(\"   1. One-way functions destroy information (irreversible)\")\n",
    "print(\"   2. Many inputs map to same output (collisions)\")\n",
    "print(\"   3. This is computational coarse-graining!\")\n",
    "print(\"   4. Landauer: Destroying 1 bit costs â‰¥ k_B T ln(2) energy\")\n",
    "print(\"   5. Reversible computation (no info loss) could be free!\")\n",
    "print(\"\\nâœ“ Computational irreversibility demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Machine Learning and the Information Bottleneck\n",
    "\n",
    "**Irreversibility is essential to machine learning!**\n",
    "\n",
    "## Why Neural Networks Must Forget\n",
    "\n",
    "A neural network is a **lossy compression** function:\n",
    "\n",
    "$$f: \\mathbb{R}^{d_{\\text{in}}} \\to \\mathbb{R}^{d_{\\text{out}}}$$\n",
    "\n",
    "where typically $d_{\\text{out}} \\ll d_{\\text{in}}$.\n",
    "\n",
    "**Information bottleneck**: Hidden layers compress input, discarding irrelevant details:\n",
    "- Input: High-dimensional data (e.g., image pixels)\n",
    "- Hidden layers: Progressive compression\n",
    "- Output: Low-dimensional representation (e.g., class label)\n",
    "\n",
    "## Why Compression Helps\n",
    "\n",
    "1. **Generalization**: Forget noise, remember signal\n",
    "2. **Efficiency**: Store only relevant features\n",
    "3. **Robustness**: Similar inputs â†’ similar outputs\n",
    "\n",
    "**Connection to thermodynamics**:\n",
    "- **Irreversible compression** = losing information\n",
    "- **Entropy increase** = spreading uncertainty\n",
    "- **Coarse-graining** = grouping similar inputs\n",
    "\n",
    "## The Information Bottleneck Principle\n",
    "\n",
    "Find representation $T$ that:\n",
    "$$\\min_{T} [I(X; T) - \\beta \\cdot I(T; Y)]$$\n",
    "\n",
    "- $I(X; T)$: Information $T$ remembers about input $X$ (compression)\n",
    "- $I(T; Y)$: Information $T$ retains about output $Y$ (prediction)\n",
    "- $\\beta$: Trade-off parameter\n",
    "\n",
    "**Goal**: Compress maximally while retaining predictive power.\n",
    "\n",
    "Let's demonstrate this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_autoencoder_layers(input_dim: int, hidden_dims: List[int]) -> List[Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"Create a simple autoencoder (compression then decompression).\n",
    "    \n",
    "    Returns:\n",
    "        List of (W, b) for each layer\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    dims = [input_dim] + hidden_dims + [input_dim]\n",
    "    \n",
    "    for i in range(len(dims) - 1):\n",
    "        W = np.random.randn(dims[i], dims[i+1]) * np.sqrt(2.0 / dims[i])\n",
    "        b = np.zeros(dims[i+1])\n",
    "        layers.append((W, b))\n",
    "    \n",
    "    return layers\n",
    "\n",
    "\n",
    "def forward_autoencoder(x: np.ndarray, layers: List[Tuple]) -> Tuple[np.ndarray, List[np.ndarray]]:\n",
    "    \"\"\"Forward pass through autoencoder.\n",
    "    \n",
    "    Returns:\n",
    "        reconstruction, list of activations at each layer\n",
    "    \"\"\"\n",
    "    activations = [x]\n",
    "    current = x\n",
    "    \n",
    "    for i, (W, b) in enumerate(layers):\n",
    "        current = current @ W + b\n",
    "        # ReLU for hidden layers, linear for output\n",
    "        if i < len(layers) - 1:\n",
    "            current = np.maximum(0, current)\n",
    "        activations.append(current)\n",
    "    \n",
    "    return current, activations\n",
    "\n",
    "\n",
    "def measure_information_content(activations: np.ndarray, num_bins: int = 20) -> float:\n",
    "    \"\"\"Estimate information content via entropy of activation distribution.\"\"\"\n",
    "    # Flatten activations\n",
    "    flat = activations.flatten()\n",
    "    \n",
    "    # Create histogram\n",
    "    hist, _ = np.histogram(flat, bins=num_bins, density=True)\n",
    "    hist = hist / hist.sum()\n",
    "    hist = hist[hist > 0]\n",
    "    \n",
    "    # Shannon entropy\n",
    "    return -np.sum(hist * np.log2(hist))\n",
    "\n",
    "\n",
    "# Generate synthetic data\n",
    "print(\"Demonstrating information bottleneck in neural networks...\\n\")\n",
    "\n",
    "# Create high-dimensional input with low-dimensional structure\n",
    "num_samples = 500\n",
    "input_dim = 50\n",
    "latent_dim = 3  # True underlying dimensionality\n",
    "\n",
    "# Generate data: low-dim latent â†’ high-dim observation\n",
    "latent = np.random.randn(num_samples, latent_dim)\n",
    "projection = np.random.randn(latent_dim, input_dim)\n",
    "X = latent @ projection\n",
    "X += np.random.randn(num_samples, input_dim) * 0.5  # Add noise\n",
    "\n",
    "print(f\"Generated data:\")\n",
    "print(f\"  Samples: {num_samples}\")\n",
    "print(f\"  Input dimension: {input_dim}\")\n",
    "print(f\"  True latent dimension: {latent_dim}\")\n",
    "print(f\"  Information must be compressed {input_dim/latent_dim:.1f}Ã—!\\n\")\n",
    "\n",
    "# Create autoencoder with bottleneck\n",
    "hidden_dims = [25, 10, 5]  # Progressively compress\n",
    "layers = create_autoencoder_layers(input_dim, hidden_dims)\n",
    "\n",
    "print(f\"Autoencoder architecture: {input_dim} â†’ {' â†’ '.join(map(str, hidden_dims))} â†’ {input_dim}\")\n",
    "print(f\"Bottleneck: {hidden_dims[-1]} dimensions (compression: {input_dim/hidden_dims[-1]:.1f}Ã—)\\n\")\n",
    "\n",
    "# Analyze information flow\n",
    "reconstructions = []\n",
    "all_activations = []\n",
    "\n",
    "for x in X[:100]:  # Use subset for speed\n",
    "    recon, acts = forward_autoencoder(x, layers)\n",
    "    reconstructions.append(recon)\n",
    "    all_activations.append(acts)\n",
    "\n",
    "# Measure information at each layer\n",
    "layer_entropies = []\n",
    "layer_sizes = [input_dim] + hidden_dims + [input_dim]\n",
    "\n",
    "for layer_idx in range(len(layers) + 1):\n",
    "    layer_activations = np.array([acts[layer_idx] for acts in all_activations])\n",
    "    entropy = measure_information_content(layer_activations)\n",
    "    layer_entropies.append(entropy)\n",
    "\n",
    "# Visualize information bottleneck\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Network architecture\n",
    "ax = axes[0, 0]\n",
    "x_pos = np.arange(len(layer_sizes))\n",
    "ax.bar(x_pos, layer_sizes, color=['blue' if s > hidden_dims[-1] else 'red' for s in layer_sizes],\n",
    "       alpha=0.7, edgecolor='black')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(['Input'] + [f'H{i+1}' for i in range(len(hidden_dims))] + ['Output'])\n",
    "ax.set_ylabel('Layer Dimension', fontsize=11)\n",
    "ax.set_title('Network Architecture: Information Bottleneck', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Information content per layer\n",
    "ax = axes[0, 1]\n",
    "ax.plot(x_pos, layer_entropies, 'o-', linewidth=2, markersize=10, color='darkgreen')\n",
    "ax.fill_between(x_pos, 0, layer_entropies, alpha=0.3, color='green')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(['Input'] + [f'H{i+1}' for i in range(len(hidden_dims))] + ['Output'])\n",
    "ax.set_ylabel('Information Content (bits)', fontsize=11)\n",
    "ax.set_title('Information Loss Through Network', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axvline(len(hidden_dims), color='red', linestyle='--', linewidth=2, label='Bottleneck')\n",
    "ax.legend()\n",
    "\n",
    "# Plot 3: Compression ratio\n",
    "ax = axes[1, 0]\n",
    "compression_ratios = [layer_sizes[0] / s for s in layer_sizes]\n",
    "ax.bar(x_pos, compression_ratios, color='orange', alpha=0.7, edgecolor='black')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(['Input'] + [f'H{i+1}' for i in range(len(hidden_dims))] + ['Output'])\n",
    "ax.set_ylabel('Compression Ratio', fontsize=11)\n",
    "ax.set_title('Lossy Compression Through Network', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.axhline(1, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Plot 4: Dimension vs information\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(layer_sizes, layer_entropies, s=200, c=range(len(layer_sizes)), \n",
    "          cmap='viridis', edgecolor='black', linewidth=2)\n",
    "for i, (d, e) in enumerate(zip(layer_sizes, layer_entropies)):\n",
    "    label = 'Input' if i == 0 else f'H{i}' if i <= len(hidden_dims) else 'Output'\n",
    "    ax.annotate(label, (d, e), xytext=(5, 5), textcoords='offset points', fontsize=10)\n",
    "ax.set_xlabel('Layer Dimension', fontsize=11)\n",
    "ax.set_ylabel('Information Content (bits)', fontsize=11)\n",
    "ax.set_title('Dimension vs Information Content', fontsize=12, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights:\")\n",
    "print(f\"   1. Input dimension: {input_dim}, Information: {layer_entropies[0]:.2f} bits\")\n",
    "print(f\"   2. Bottleneck dimension: {hidden_dims[-1]}, Information: {layer_entropies[len(hidden_dims)]:.2f} bits\")\n",
    "print(f\"   3. Information lost: {layer_entropies[0] - layer_entropies[len(hidden_dims)]:.2f} bits ({(1 - layer_entropies[len(hidden_dims)]/layer_entropies[0])*100:.1f}%)\")\n",
    "print(f\"   4. This is INTENTIONAL! Network forgets noise, remembers structure.\")\n",
    "print(\"\\n   â†’ Irreversibility (information loss) is essential for learning!\")\n",
    "print(\"   â†’ Networks that compress well generalize well.\")\n",
    "print(\"   â†’ Thermodynamic analogy: Compress = coarse-grain = increase entropy\")\n",
    "print(\"\\nâœ“ Information bottleneck demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 8: The Arrow of Time - Fundamental vs Emergent\n",
    "\n",
    "We've seen irreversibility from many angles. Now let's ask the deep question:\n",
    "\n",
    "**Is the arrow of time fundamental or emergent?**\n",
    "\n",
    "## Arguments for \"Fundamental\"\n",
    "\n",
    "1. **Weak interactions violate CP symmetry** â†’ slight time-reversal violation\n",
    "2. **Cosmological arrow**: Universe expanding (not contracting)\n",
    "3. **Quantum measurement**: Wavefunction collapse is irreversible\n",
    "\n",
    "## Arguments for \"Emergent\"\n",
    "\n",
    "1. **Statistical mechanics**: Second law emerges from statistics\n",
    "2. **Microscopic reversibility**: Fundamental laws are time-symmetric\n",
    "3. **Boundary conditions**: Low-entropy Big Bang sets arrow of time\n",
    "\n",
    "## The Consensus: Mostly Emergent\n",
    "\n",
    "The arrow of time is **not** in the laws of physicsâ€”it's in the **initial conditions**:\n",
    "\n",
    "$$S_{\\text{universe}}(t) > S_{\\text{universe}}(0)$$\n",
    "\n",
    "The Big Bang started with **extremely low entropy**. Since then, entropy has been increasing.\n",
    "\n",
    "**Why was the Big Bang low-entropy?** We don't know! This is one of the deepest mysteries in physics.\n",
    "\n",
    "## Implications\n",
    "\n",
    "- **Thermodynamic arrow**: Entropy increases\n",
    "- **Psychological arrow**: We remember past, not future (memory requires low entropy)\n",
    "- **Cosmological arrow**: Universe expands\n",
    "\n",
    "All three are **consequences** of the low-entropy initial condition.\n",
    "\n",
    "Let's visualize this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the arrow of time\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Entropy vs time for universe\n",
    "ax = axes[0, 0]\n",
    "time_universe = np.linspace(0, 13.8, 1000)  # Billion years\n",
    "# Simplified model: entropy grows logarithmically with time\n",
    "entropy_universe = np.log(1 + time_universe * 10) + np.random.randn(1000) * 0.1\n",
    "\n",
    "ax.plot(time_universe, entropy_universe, linewidth=2, color='darkblue')\n",
    "ax.scatter([0], [entropy_universe[0]], s=200, c='red', marker='*', \n",
    "          label='Big Bang (low entropy!)', zorder=5, edgecolor='black', linewidth=2)\n",
    "ax.arrow(3, 1, 3, 1, head_width=0.2, head_length=0.5, fc='red', ec='red', linewidth=2)\n",
    "ax.text(4, 2.5, 'Arrow of Time', fontsize=12, color='red', fontweight='bold')\n",
    "ax.set_xlabel('Time (billion years since Big Bang)', fontsize=11)\n",
    "ax.set_ylabel('Universe Entropy (arbitrary units)', fontsize=11)\n",
    "ax.set_title('Cosmological Arrow of Time', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Forward vs backward in time\n",
    "ax = axes[0, 1]\n",
    "t = np.linspace(0, 10, 100)\n",
    "entropy_forward = 1 - np.exp(-t/3)  # Entropy increases\n",
    "entropy_backward = np.flip(entropy_forward)  # Time-reversed (entropy decreases)\n",
    "\n",
    "ax.plot(t, entropy_forward, linewidth=3, color='green', label='Forward in time (2nd law holds)')\n",
    "ax.plot(t, entropy_backward, linewidth=3, color='red', linestyle='--', \n",
    "       label='Backward in time (2nd law violated!)')\n",
    "ax.set_xlabel('Time', fontsize=11)\n",
    "ax.set_ylabel('Entropy', fontsize=11)\n",
    "ax.set_title('Time Asymmetry: Why Backward Looks Wrong', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Phase space volume (Liouville) vs coarse-grained entropy\n",
    "ax = axes[1, 0]\n",
    "t = np.linspace(0, 10, 100)\n",
    "phase_space_volume = np.ones_like(t)  # Constant (Liouville)\n",
    "macro_entropy = 1 - np.exp(-t/2)  # Increases\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "ax.plot(t, phase_space_volume, linewidth=3, color='blue', label='Microscopic (Liouville)')\n",
    "ax2.plot(t, macro_entropy, linewidth=3, color='red', label='Macroscopic (2nd law)')\n",
    "ax.set_xlabel('Time', fontsize=11)\n",
    "ax.set_ylabel('Phase Space Volume (conserved)', fontsize=11, color='blue')\n",
    "ax2.set_ylabel('Macroscopic Entropy (increases)', fontsize=11, color='red')\n",
    "ax.set_title('Microscopic Reversibility vs Macroscopic Irreversibility', \n",
    "            fontsize=12, fontweight='bold')\n",
    "ax.tick_params(axis='y', labelcolor='blue')\n",
    "ax2.tick_params(axis='y', labelcolor='red')\n",
    "lines1, labels1 = ax.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax.legend(lines1 + lines2, labels1 + labels2, loc='center right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Three arrows of time\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "# Draw three arrows\n",
    "arrow_props = dict(arrowstyle='->', lw=4, color='darkblue')\n",
    "ax.annotate('', xy=(0.8, 0.8), xytext=(0.2, 0.8), arrowprops=arrow_props)\n",
    "ax.text(0.5, 0.85, 'Thermodynamic Arrow', ha='center', fontsize=12, fontweight='bold')\n",
    "ax.text(0.5, 0.75, 'Entropy increases', ha='center', fontsize=10, style='italic')\n",
    "\n",
    "arrow_props['color'] = 'darkgreen'\n",
    "ax.annotate('', xy=(0.8, 0.5), xytext=(0.2, 0.5), arrowprops=arrow_props)\n",
    "ax.text(0.5, 0.55, 'Psychological Arrow', ha='center', fontsize=12, fontweight='bold')\n",
    "ax.text(0.5, 0.45, 'Remember past, not future', ha='center', fontsize=10, style='italic')\n",
    "\n",
    "arrow_props['color'] = 'darkred'\n",
    "ax.annotate('', xy=(0.8, 0.2), xytext=(0.2, 0.2), arrowprops=arrow_props)\n",
    "ax.text(0.5, 0.25, 'Cosmological Arrow', ha='center', fontsize=12, fontweight='bold')\n",
    "ax.text(0.5, 0.15, 'Universe expands', ha='center', fontsize=10, style='italic')\n",
    "\n",
    "ax.text(0.5, 0.05, 'All three aligned â†’ All consequences of low-entropy Big Bang', \n",
    "       ha='center', fontsize=10, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('The Three Arrows of Time', fontsize=13, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”‘ Deep insights about time:\")\n",
    "print(\"\\n1. MICROSCOPIC LAWS: Time-reversible (Newtonian, quantum mechanics)\")\n",
    "print(\"   â†’ Running laws backward is mathematically valid\")\n",
    "print(\"\\n2. MACROSCOPIC BEHAVIOR: Time-irreversible (thermodynamics)\")\n",
    "print(\"   â†’ Entropy increases, mixing happens, cannot unmix\")\n",
    "print(\"\\n3. THE RESOLUTION: Initial conditions + statistics\")\n",
    "print(\"   â†’ Big Bang had EXTREMELY low entropy\")\n",
    "print(\"   â†’ Statistical evolution explores high-entropy states\")\n",
    "print(\"   â†’ Coarse-graining makes evolution appear irreversible\")\n",
    "print(\"\\n4. THE DEEP MYSTERY: Why was the Big Bang low-entropy?\")\n",
    "print(\"   â†’ We don't know! This is the origin of the arrow of time.\")\n",
    "print(\"   â†’ Some theories: anthropic principle, bouncing cosmology, etc.\")\n",
    "print(\"\\n5. THREE ARROWS, ONE CAUSE:\")\n",
    "print(\"   â†’ Thermodynamic: Entropy â†‘ (consequence of initial conditions)\")\n",
    "print(\"   â†’ Psychological: Memory works backward (requires low entropy)\")\n",
    "print(\"   â†’ Cosmological: Universe expands (related to initial conditions)\")\n",
    "print(\"\\nâœ“ Arrow of time analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 9: Biological Irreversibility - Life and the Second Law\n",
    "\n",
    "**Does life violate the second law?**\n",
    "\n",
    "No! Life is an **open system** that:\n",
    "1. Decreases entropy locally (creates order)\n",
    "2. Increases entropy globally (exports disorder)\n",
    "\n",
    "## SchrÃ¶dinger's Insight\n",
    "\n",
    "In \"What is Life?\" (1944), SchrÃ¶dinger said:\n",
    "> \"Life feeds on negative entropy\"\n",
    "\n",
    "Living organisms:\n",
    "- Import **low-entropy** energy (food, sunlight)\n",
    "- Use it to maintain order (metabolism, growth, reproduction)\n",
    "- Export **high-entropy** waste (heat, COâ‚‚)\n",
    "\n",
    "**Net result**: Universe entropy increases, second law obeyed.\n",
    "\n",
    "## The Second Law for Open Systems\n",
    "\n",
    "For an open system exchanging entropy $S_{\\text{env}}$ with environment:\n",
    "\n",
    "$$\\frac{dS_{\\text{system}}}{dt} + \\frac{dS_{\\text{env}}}{dt} \\geq 0$$\n",
    "\n",
    "$S_{\\text{system}}$ can decrease if $S_{\\text{env}}$ increases more!\n",
    "\n",
    "Let's model this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model a simple \"living\" system that maintains order\n",
    "\n",
    "def simulate_open_system(num_steps: int = 200) -> dict:\n",
    "    \"\"\"Simulate an open system that maintains low entropy.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with entropy histories\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    S_system = 10.0  # System starts with moderate entropy\n",
    "    S_environment = 0.0  # Track cumulative entropy exported\n",
    "    \n",
    "    S_system_history = [S_system]\n",
    "    S_environment_history = [S_environment]\n",
    "    S_total_history = [S_system + S_environment]\n",
    "    \n",
    "    for t in range(num_steps):\n",
    "        # System processes: would naturally increase entropy\n",
    "        natural_increase = 0.5\n",
    "        \n",
    "        # Active maintenance: system exports entropy to environment\n",
    "        # This \"costs\" more entropy in environment than saved in system\n",
    "        entropy_exported = 0.8  # More than natural increase (2nd law!)\n",
    "        entropy_reduced = 0.3  # System entropy reduction\n",
    "        \n",
    "        # Update\n",
    "        S_system = S_system + natural_increase - entropy_reduced\n",
    "        S_environment = S_environment + entropy_exported\n",
    "        \n",
    "        # Record\n",
    "        S_system_history.append(S_system)\n",
    "        S_environment_history.append(S_environment)\n",
    "        S_total_history.append(S_system + S_environment)\n",
    "    \n",
    "    return {\n",
    "        'system': S_system_history,\n",
    "        'environment': S_environment_history,\n",
    "        'total': S_total_history\n",
    "    }\n",
    "\n",
    "\n",
    "def simulate_closed_system(num_steps: int = 200) -> dict:\n",
    "    \"\"\"Simulate a closed system (for comparison).\"\"\"\n",
    "    S_total = 10.0\n",
    "    S_total_history = [S_total]\n",
    "    \n",
    "    for t in range(num_steps):\n",
    "        # Entropy only increases in closed system\n",
    "        S_total = S_total + 0.5\n",
    "        S_total_history.append(S_total)\n",
    "    \n",
    "    return {'total': S_total_history}\n",
    "\n",
    "\n",
    "print(\"Simulating open system (life-like) vs closed system...\\n\")\n",
    "\n",
    "open_sys = simulate_open_system()\n",
    "closed_sys = simulate_closed_system()\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Open system (life)\n",
    "ax = axes[0]\n",
    "t = np.arange(len(open_sys['total']))\n",
    "ax.plot(t, open_sys['system'], linewidth=2, label='System (organism)', color='green')\n",
    "ax.plot(t, open_sys['environment'], linewidth=2, label='Environment', color='brown')\n",
    "ax.plot(t, open_sys['total'], linewidth=3, label='Total (system + env)', \n",
    "       color='red', linestyle='--')\n",
    "ax.set_xlabel('Time', fontsize=11)\n",
    "ax.set_ylabel('Entropy', fontsize=11)\n",
    "ax.set_title('Open System: Life Maintains Order by Exporting Entropy', \n",
    "            fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.text(100, 50, 'System entropy\\ncan decrease!', fontsize=10, color='green',\n",
    "       bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "ax.text(100, 120, 'But total entropy\\nalways increases!', fontsize=10, color='red',\n",
    "       bbox=dict(boxstyle='round', facecolor='pink', alpha=0.5))\n",
    "\n",
    "# Plot 2: Comparison with closed system\n",
    "ax = axes[1]\n",
    "ax.plot(t, closed_sys['total'], linewidth=3, label='Closed system', color='blue')\n",
    "ax.plot(t, open_sys['total'], linewidth=3, label='Open system (total)', \n",
    "       color='red', linestyle='--')\n",
    "ax.set_xlabel('Time', fontsize=11)\n",
    "ax.set_ylabel('Total Entropy', fontsize=11)\n",
    "ax.set_title('Both Systems Obey 2nd Law: Total Entropy â†‘', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.text(100, 50, 'Open system increases\\nentropy FASTER\\n(due to metabolism)', \n",
    "       fontsize=10, bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights about life and entropy:\")\n",
    "print(\"\\n1. LOCAL vs GLOBAL:\")\n",
    "print(f\"   â†’ System entropy: {open_sys['system'][0]:.1f} â†’ {open_sys['system'][-1]:.1f} (maintained low!)\")\n",
    "print(f\"   â†’ Total entropy:  {open_sys['total'][0]:.1f} â†’ {open_sys['total'][-1]:.1f} (increased!)\")\n",
    "print(\"\\n2. LIFE'S TRICK:\")\n",
    "print(\"   â†’ Import low-entropy energy (food, sunlight)\")\n",
    "print(\"   â†’ Use it to build order (proteins, cells, organisms)\")\n",
    "print(\"   â†’ Export high-entropy waste (heat, COâ‚‚, etc.)\")\n",
    "print(\"\\n3. NET RESULT:\")\n",
    "print(\"   â†’ Organism entropy â†“ (more order)\")\n",
    "print(\"   â†’ Environment entropy â†‘â†‘ (much more disorder)\")\n",
    "print(\"   â†’ Total entropy â†‘ (2nd law satisfied!)\")\n",
    "print(\"\\n4. WHY IT WORKS:\")\n",
    "print(\"   â†’ Exporting entropy is IRREVERSIBLE\")\n",
    "print(\"   â†’ Heat cannot spontaneously reconcentrate\")\n",
    "print(\"   â†’ This is why death is inevitable (eventually can't export enough)\")\n",
    "print(\"\\n5. SCHRÃ–DINGER WAS RIGHT:\")\n",
    "print(\"   â†’ Life feeds on negative entropy (order)\")\n",
    "print(\"   â†’ Exports positive entropy (disorder)\")\n",
    "print(\"   â†’ This is how life is compatible with 2nd law!\")\n",
    "print(\"\\nâœ“ Biological irreversibility demonstration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 10: Synthesis - Irreversibility Across Scales\n",
    "\n",
    "We've explored irreversibility from coffee cups to the cosmos. Let's synthesize.\n",
    "\n",
    "## The Hierarchy of Irreversibility\n",
    "\n",
    "| Scale | System | Reversible? | Why Irreversible? |\n",
    "|-------|--------|-------------|-------------------|\n",
    "| **Microscopic** | Individual particles | âœ… Yes | Newtonian/quantum laws are time-symmetric |\n",
    "| **Mesoscopic** | Small groups (~100s) | âš ï¸ Practically no | PoincarÃ© recurrence time >> observation time |\n",
    "| **Macroscopic** | Everyday objects | âŒ No | Coarse-graining + statistics + high N |\n",
    "| **Cosmological** | Universe | âŒ No | Low-entropy initial condition |\n",
    "\n",
    "## Common Threads\n",
    "\n",
    "All forms of irreversibility share:\n",
    "\n",
    "1. **Coarse-graining**: Losing fine-grained information\n",
    "   - Physics: Macrostates vs microstates\n",
    "   - Computation: Hash functions, lossy compression\n",
    "   - ML: Neural network bottlenecks\n",
    "\n",
    "2. **Many-to-one mapping**: Multiple inputs â†’ same output\n",
    "   - Coffee: Many molecular configs â†’ same macroscopic appearance\n",
    "   - Hashing: Many strings â†’ same hash value\n",
    "   - Neural nets: Many images â†’ same class label\n",
    "\n",
    "3. **Information loss**: Cannot recover original state\n",
    "   - Thermodynamics: Entropy increase\n",
    "   - Landauer: Bit erasure costs energy\n",
    "   - ML: Compression discards details\n",
    "\n",
    "4. **Statistical bias**: Evolution toward high-probability states\n",
    "   - Physics: Equilibrium is most likely\n",
    "   - Computation: Random walk in state space\n",
    "   - ML: Gradient descent toward minima\n",
    "\n",
    "## Profound Implications\n",
    "\n",
    "### For Physics\n",
    "- Time is emergent, not fundamental\n",
    "- Second law is statistical, not absolute\n",
    "- Low-entropy initial condition is key mystery\n",
    "\n",
    "### For Computation\n",
    "- All computation has thermodynamic cost\n",
    "- Reversible computing could be \"free\"\n",
    "- Information is physical (not abstract)\n",
    "\n",
    "### For Machine Learning\n",
    "- Compression is essential for generalization\n",
    "- Information bottleneck = good inductive bias\n",
    "- Forgetting (irreversibility) helps learning\n",
    "\n",
    "### For Life\n",
    "- Open systems can maintain order\n",
    "- But must export entropy\n",
    "- Death is thermodynamic inevitability\n",
    "\n",
    "## The Ultimate Question\n",
    "\n",
    "**Why was the universe born with such low entropy?**\n",
    "\n",
    "This single fact explains:\n",
    "- The arrow of time\n",
    "- Why mixing is irreversible\n",
    "- Why we remember the past, not the future\n",
    "- Why life can exist\n",
    "- Why computation is possible\n",
    "\n",
    "We don't know the answer. But we know it's one of the deepest questions in science.\n",
    "\n",
    "---\n",
    "\n",
    "## Connections to Machine Learning (Revisited)\n",
    "\n",
    "**Why does this matter for AI?**\n",
    "\n",
    "1. **Information bottleneck**: Neural networks must compress â†’ irreversible\n",
    "2. **One-way functions**: Security depends on computational irreversibility\n",
    "3. **Landauer's limit**: Future AI energy efficiency bounded by thermodynamics\n",
    "4. **Memory**: Brains/computers must erase old memories â†’ entropic cost\n",
    "5. **Learning = compression**: Good models compress data irreversibly\n",
    "\n",
    "**Irreversibility isn't a bugâ€”it's a feature!**\n",
    "\n",
    "Without it:\n",
    "- No generalization (would memorize everything)\n",
    "- No computation (no one-way functions)\n",
    "- No security (could invert any function)\n",
    "- No learning (no abstraction)\n",
    "\n",
    "---\n",
    "\n",
    "**The coffee automaton teaches us**: The universe has an arrow. And that arrow points toward maximum entropy, forgotten information, and irreversible change.\n",
    "\n",
    "**But paradoxically**, this irreversibility is what makes:\n",
    "- Computation possible\n",
    "- Life possible\n",
    "- Learning possible\n",
    "- Thought possible\n",
    "\n",
    "You can't unmix the coffee. But if you could, you wouldn't be able to think about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final visualization: The complete picture\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# Central concept: The arrow of time\n",
    "ax_center = fig.add_subplot(gs[1, 1])\n",
    "ax_center.axis('off')\n",
    "ax_center.text(0.5, 0.5, 'â˜•\\n\\nThe Coffee Automaton\\n\\nIrreversibility', \n",
    "              ha='center', va='center', fontsize=20, fontweight='bold',\n",
    "              bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', alpha=0.8))\n",
    "ax_center.set_xlim(0, 1)\n",
    "ax_center.set_ylim(0, 1)\n",
    "\n",
    "# Surrounding concepts\n",
    "concepts = [\n",
    "    (\"Physics\\n\\nEntropy â†‘\\n2nd Law\", gs[0, 0]),\n",
    "    (\"Phase Space\\n\\nLiouville\\nCoarse-graining\", gs[0, 1]),\n",
    "    (\"Computation\\n\\nOne-way\\nLandauer\", gs[0, 2]),\n",
    "    (\"Recurrence\\n\\nPoincarÃ©\\ne^N time\", gs[1, 0]),\n",
    "    (\"Maxwell\\n\\nDemon\\nInfo=Physical\", gs[1, 2]),\n",
    "    (\"Biology\\n\\nLife\\nOpen system\", gs[2, 0]),\n",
    "    (\"ML\\n\\nCompression\\nBottleneck\", gs[2, 1]),\n",
    "    (\"Cosmology\\n\\nBig Bang\\nLow entropy\", gs[2, 2]),\n",
    "]\n",
    "\n",
    "colors = ['lightcoral', 'lightgreen', 'lightyellow', 'lightpink', \n",
    "         'lightcyan', 'lavender', 'peachpuff', 'thistle']\n",
    "\n",
    "for (text, pos), color in zip(concepts, colors):\n",
    "    ax = fig.add_subplot(pos)\n",
    "    ax.axis('off')\n",
    "    ax.text(0.5, 0.5, text, ha='center', va='center', fontsize=12,\n",
    "           bbox=dict(boxstyle='round', facecolor=color, alpha=0.7))\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Draw arrow to center\n",
    "    ax.annotate('', xy=(0.5, 0.5), xytext=(0.5, 0.5),\n",
    "               arrowprops=dict(arrowstyle='->', lw=2, color='gray', alpha=0.5))\n",
    "\n",
    "plt.suptitle('Irreversibility: A Unifying Concept Across All Scales', \n",
    "            fontsize=16, fontweight='bold', y=0.98)\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY: THE COFFEE AUTOMATON - KEY TAKEAWAYS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. FUNDAMENTAL PUZZLE:\")\n",
    "print(\"   â€¢ Microscopic laws are reversible (Newton, SchrÃ¶dinger)\")\n",
    "print(\"   â€¢ Macroscopic behavior is irreversible (coffee mixes, never unmixes)\")\n",
    "print(\"   â€¢ Resolution: Coarse-graining + statistics + low-entropy initial condition\")\n",
    "\n",
    "print(\"\\n2. MECHANISMS OF IRREVERSIBILITY:\")\n",
    "print(\"   â€¢ Coarse-graining: Losing information when grouping microstates\")\n",
    "print(\"   â€¢ Statistical mechanics: High-entropy states vastly outnumber low-entropy\")\n",
    "print(\"   â€¢ PoincarÃ© recurrence: Reversible, but on timescale e^N >> universe age\")\n",
    "\n",
    "print(\"\\n3. INFORMATION IS PHYSICAL:\")\n",
    "print(\"   â€¢ Landauer's principle: Erasing 1 bit costs k_B T ln(2) energy\")\n",
    "print(\"   â€¢ Maxwell's demon: Information gathering/erasure has entropic cost\")\n",
    "print(\"   â€¢ Computation: All irreversible operations dissipate heat\")\n",
    "\n",
    "print(\"\\n4. COMPUTATIONAL IRREVERSIBILITY:\")\n",
    "print(\"   â€¢ One-way functions: Easy forward, hard backward\")\n",
    "print(\"   â€¢ Cryptographic hashing: Many inputs â†’ same output\")\n",
    "print(\"   â€¢ Security depends on computational irreversibility\")\n",
    "\n",
    "print(\"\\n5. MACHINE LEARNING:\")\n",
    "print(\"   â€¢ Neural networks compress (information bottleneck)\")\n",
    "print(\"   â€¢ Compression = irreversible = forgetting details\")\n",
    "print(\"   â€¢ Generalization requires irreversibility!\")\n",
    "\n",
    "print(\"\\n6. LIFE AND THERMODYNAMICS:\")\n",
    "print(\"   â€¢ Life is open system: imports order, exports disorder\")\n",
    "print(\"   â€¢ Local entropy â†“, but total entropy â†‘ (2nd law satisfied)\")\n",
    "print(\"   â€¢ Death is thermodynamic inevitability\")\n",
    "\n",
    "print(\"\\n7. THE ARROW OF TIME:\")\n",
    "print(\"   â€¢ Not in the lawsâ€”in the initial conditions!\")\n",
    "print(\"   â€¢ Big Bang had extremely low entropy (why??)\")\n",
    "print(\"   â€¢ All three arrows (thermodynamic, psychological, cosmological) aligned\")\n",
    "\n",
    "print(\"\\n8. DEEP MYSTERY:\")\n",
    "print(\"   â€¢ Why was the Big Bang low-entropy?\")\n",
    "print(\"   â€¢ This single fact explains the arrow of time\")\n",
    "print(\"   â€¢ Still unsolved! One of deepest questions in physics\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nâ˜• The coffee automaton: Simple system, profound implications\")\n",
    "print(\"\\nYou can't unmix the coffee.\")\n",
    "print(\"But this irreversibility is what makes computation, life, and thought possible.\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"\\nâœ“ Complete analysis of irreversibility finished!\")\n",
    "print(\"\\nðŸŽ“ Paper 19 implementation complete: A deep dive into the coffee automaton.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
