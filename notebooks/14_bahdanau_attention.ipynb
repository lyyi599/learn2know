{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 14: Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "## Dzmitry Bahdanau, KyungHyun Cho, Yoshua Bengio (2014)\n",
    "\n",
    "### The Original Attention Mechanism\n",
    "\n",
    "This paper introduced **attention** - one of the most important innovations in deep learning. It preceded Transformers by 3 years!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem: Fixed-Length Context Vector\n",
    "\n",
    "Traditional seq2seq compresses entire input into single vector → information bottleneck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "class EncoderRNN:\n",
    "    \"\"\"Bidirectional RNN encoder\"\"\"\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Forward RNN\n",
    "        self.W_fwd = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.b_fwd = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Backward RNN\n",
    "        self.W_bwd = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.b_bwd = np.zeros((hidden_size, 1))\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: list of (input_size, 1) vectors\n",
    "        Returns: list of bidirectional hidden states (2*hidden_size, 1)\n",
    "        \"\"\"\n",
    "        seq_len = len(inputs)\n",
    "        \n",
    "        # Forward pass\n",
    "        h_fwd = []\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        for x in inputs:\n",
    "            concat = np.vstack([x, h])\n",
    "            h = np.tanh(np.dot(self.W_fwd, concat) + self.b_fwd)\n",
    "            h_fwd.append(h)\n",
    "        \n",
    "        # Backward pass\n",
    "        h_bwd = []\n",
    "        h = np.zeros((self.hidden_size, 1))\n",
    "        for x in reversed(inputs):\n",
    "            concat = np.vstack([x, h])\n",
    "            h = np.tanh(np.dot(self.W_bwd, concat) + self.b_bwd)\n",
    "            h_bwd.append(h)\n",
    "        h_bwd = list(reversed(h_bwd))\n",
    "        \n",
    "        # Concatenate forward and backward\n",
    "        annotations = [np.vstack([h_f, h_b]) for h_f, h_b in zip(h_fwd, h_bwd)]\n",
    "        \n",
    "        return annotations\n",
    "\n",
    "print(\"Bidirectional Encoder created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bahdanau Attention Mechanism\n",
    "\n",
    "The key innovation: align and translate jointly!\n",
    "\n",
    "**Attention score**: $e_{ij} = a(s_{i-1}, h_j)$ where $s$ is decoder state, $h$ is encoder annotation\n",
    "\n",
    "**Attention weights**: $\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_k \\exp(e_{ik})}$\n",
    "\n",
    "**Context vector**: $c_i = \\sum_j \\alpha_{ij} h_j$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention:\n",
    "    \"\"\"Additive attention mechanism\"\"\"\n",
    "    def __init__(self, hidden_size, annotation_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Attention parameters\n",
    "        self.W_a = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.U_a = np.random.randn(hidden_size, annotation_size) * 0.01\n",
    "        self.v_a = np.random.randn(1, hidden_size) * 0.01\n",
    "    \n",
    "    def forward(self, decoder_hidden, encoder_annotations):\n",
    "        \"\"\"\n",
    "        decoder_hidden: (hidden_size, 1) - current decoder state s_{i-1}\n",
    "        encoder_annotations: list of (annotation_size, 1) - all encoder states h_j\n",
    "        \n",
    "        Returns:\n",
    "        context: (annotation_size, 1) - weighted sum of annotations\n",
    "        attention_weights: (seq_len,) - attention distribution\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        # Compute attention scores for each position\n",
    "        for h_j in encoder_annotations:\n",
    "            # e_ij = v_a^T * tanh(W_a * s_{i-1} + U_a * h_j)\n",
    "            score = np.dot(self.v_a, np.tanh(\n",
    "                np.dot(self.W_a, decoder_hidden) + \n",
    "                np.dot(self.U_a, h_j)\n",
    "            ))\n",
    "            scores.append(score[0, 0])\n",
    "        \n",
    "        # Softmax to get attention weights\n",
    "        scores = np.array(scores)\n",
    "        attention_weights = softmax(scores)\n",
    "        \n",
    "        # Compute context vector as weighted sum\n",
    "        context = sum(alpha * h for alpha, h in zip(attention_weights, encoder_annotations))\n",
    "        \n",
    "        return context, attention_weights\n",
    "\n",
    "print(\"Bahdanau Attention mechanism created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionDecoder:\n",
    "    \"\"\"RNN decoder with Bahdanau attention\"\"\"\n",
    "    def __init__(self, output_size, hidden_size, annotation_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = BahdanauAttention(hidden_size, annotation_size)\n",
    "        \n",
    "        # RNN: takes previous output + context\n",
    "        input_size = output_size + annotation_size\n",
    "        self.W_dec = np.random.randn(hidden_size, input_size + hidden_size) * 0.01\n",
    "        self.b_dec = np.zeros((hidden_size, 1))\n",
    "        \n",
    "        # Output layer\n",
    "        self.W_out = np.random.randn(output_size, hidden_size + annotation_size + output_size) * 0.01\n",
    "        self.b_out = np.zeros((output_size, 1))\n",
    "    \n",
    "    def step(self, prev_output, decoder_hidden, encoder_annotations):\n",
    "        \"\"\"\n",
    "        Single decoding step\n",
    "        \n",
    "        prev_output: (output_size, 1) - previous output word\n",
    "        decoder_hidden: (hidden_size, 1) - previous decoder state\n",
    "        encoder_annotations: list of (annotation_size, 1) - encoder states\n",
    "        \n",
    "        Returns:\n",
    "        output: (output_size, 1) - predicted output distribution\n",
    "        new_hidden: (hidden_size, 1) - new decoder state\n",
    "        attention_weights: attention distribution\n",
    "        \"\"\"\n",
    "        # Compute attention and context\n",
    "        context, attention_weights = self.attention.forward(decoder_hidden, encoder_annotations)\n",
    "        \n",
    "        # Decoder RNN: s_i = f(s_{i-1}, y_{i-1}, c_i)\n",
    "        rnn_input = np.vstack([prev_output, context])\n",
    "        concat = np.vstack([rnn_input, decoder_hidden])\n",
    "        new_hidden = np.tanh(np.dot(self.W_dec, concat) + self.b_dec)\n",
    "        \n",
    "        # Output: y_i = g(s_i, y_{i-1}, c_i)\n",
    "        output_input = np.vstack([new_hidden, context, prev_output])\n",
    "        output = np.dot(self.W_out, output_input) + self.b_out\n",
    "        \n",
    "        return output, new_hidden, attention_weights\n",
    "    \n",
    "    def forward(self, encoder_annotations, max_length=20, start_token=None):\n",
    "        \"\"\"\n",
    "        Full decoding\n",
    "        \"\"\"\n",
    "        if start_token is None:\n",
    "            start_token = np.zeros((self.output_size, 1))\n",
    "        \n",
    "        outputs = []\n",
    "        attention_history = []\n",
    "        \n",
    "        # Initialize\n",
    "        decoder_hidden = np.zeros((self.hidden_size, 1))\n",
    "        prev_output = start_token\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            output, decoder_hidden, attention_weights = self.step(\n",
    "                prev_output, decoder_hidden, encoder_annotations\n",
    "            )\n",
    "            \n",
    "            outputs.append(output)\n",
    "            attention_history.append(attention_weights)\n",
    "            \n",
    "            # Next input is current output (greedy decoding)\n",
    "            prev_output = output\n",
    "        \n",
    "        return outputs, attention_history\n",
    "\n",
    "print(\"Attention Decoder created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Seq2Seq with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqWithAttention:\n",
    "    def __init__(self, input_vocab_size, output_vocab_size, hidden_size=32):\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Embedding layers\n",
    "        self.input_embedding = np.random.randn(input_vocab_size, hidden_size) * 0.01\n",
    "        self.output_embedding = np.random.randn(output_vocab_size, hidden_size) * 0.01\n",
    "        \n",
    "        # Encoder (bidirectional, so annotation size is 2*hidden_size)\n",
    "        self.encoder = EncoderRNN(hidden_size, hidden_size)\n",
    "        \n",
    "        # Decoder with attention\n",
    "        annotation_size = 2 * hidden_size\n",
    "        self.decoder = AttentionDecoder(hidden_size, hidden_size, annotation_size)\n",
    "    \n",
    "    def translate(self, input_sequence, max_output_length=15):\n",
    "        \"\"\"\n",
    "        Translate input sequence to output sequence\n",
    "        \n",
    "        input_sequence: list of token indices\n",
    "        \"\"\"\n",
    "        # Embed input\n",
    "        embedded = [self.input_embedding[idx:idx+1].T for idx in input_sequence]\n",
    "        \n",
    "        # Encode\n",
    "        annotations = self.encoder.forward(embedded)\n",
    "        \n",
    "        # Decode\n",
    "        start_token = self.output_embedding[0:1].T  # Use first token as start\n",
    "        outputs, attention_history = self.decoder.forward(\n",
    "            annotations, max_length=max_output_length, start_token=start_token\n",
    "        )\n",
    "        \n",
    "        return outputs, attention_history, annotations\n",
    "\n",
    "# Create model\n",
    "input_vocab_size = 20   # Source language vocab\n",
    "output_vocab_size = 20  # Target language vocab\n",
    "model = Seq2SeqWithAttention(input_vocab_size, output_vocab_size, hidden_size=16)\n",
    "\n",
    "print(f\"Seq2Seq with Attention created\")\n",
    "print(f\"Input vocab: {input_vocab_size}\")\n",
    "print(f\"Output vocab: {output_vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on Synthetic Translation Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple synthetic task: reverse sequence\n",
    "# Input: [1, 2, 3, 4, 5]\n",
    "# Output: [5, 4, 3, 2, 1]\n",
    "\n",
    "input_seq = [1, 2, 3, 4, 5, 6, 7]\n",
    "outputs, attention_history, annotations = model.translate(input_seq, max_output_length=len(input_seq))\n",
    "\n",
    "print(f\"Input sequence: {input_seq}\")\n",
    "print(f\"Number of output steps: {len(outputs)}\")\n",
    "print(f\"Number of attention distributions: {len(attention_history)}\")\n",
    "print(f\"Encoder annotations shape: {len(annotations)} x {annotations[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Attention Weights\n",
    "\n",
    "The key insight: see what the model attends to!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert attention history to matrix\n",
    "attention_matrix = np.array(attention_history)  # (output_len, input_len)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(attention_matrix, cmap='Blues', aspect='auto', interpolation='nearest')\n",
    "plt.colorbar(label='Attention Weight')\n",
    "plt.xlabel('Input Position (Source)')\n",
    "plt.ylabel('Output Position (Target)')\n",
    "plt.title('Bahdanau Attention Alignment Matrix')\n",
    "\n",
    "# Add grid\n",
    "plt.xticks(range(len(input_seq)), [f'x{i+1}' for i in input_seq])\n",
    "plt.yticks(range(len(outputs)), [f'y{i+1}' for i in range(len(outputs))])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAttention patterns show which input positions influence each output.\")\n",
    "print(\"Brighter cells = higher attention weight.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention at Each Decoder Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention distribution at specific decoder steps\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 6))\n",
    "axes = axes.flatten()\n",
    "\n",
    "steps_to_show = min(8, len(attention_history))\n",
    "\n",
    "for i in range(steps_to_show):\n",
    "    axes[i].bar(range(len(input_seq)), attention_history[i])\n",
    "    axes[i].set_title(f'Output Step {i+1}')\n",
    "    axes[i].set_xlabel('Input Position')\n",
    "    axes[i].set_ylabel('Attention Weight')\n",
    "    axes[i].set_ylim(0, 1)\n",
    "    axes[i].set_xticks(range(len(input_seq)))\n",
    "    axes[i].set_xticklabels([f'x{j+1}' for j in input_seq], fontsize=8)\n",
    "    axes[i].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Attention Distribution at Each Decoding Step', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Each decoder step focuses on different input positions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare: With vs Without Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate fixed-context seq2seq (no attention)\n",
    "def fixed_context_attention(seq_len):\n",
    "    \"\"\"Simulates attending only to last encoder state\"\"\"\n",
    "    weights = np.zeros(seq_len)\n",
    "    weights[-1] = 1.0  # Only attend to last position\n",
    "    return weights\n",
    "\n",
    "# Create comparison\n",
    "input_length = len(input_seq)\n",
    "output_length = len(outputs)\n",
    "\n",
    "# Fixed context\n",
    "fixed_attention = np.array([fixed_context_attention(input_length) for _ in range(output_length)])\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Without attention (fixed context)\n",
    "im1 = ax1.imshow(fixed_attention, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
    "ax1.set_xlabel('Input Position')\n",
    "ax1.set_ylabel('Output Position')\n",
    "ax1.set_title('Without Attention (Fixed Context)\\nAll decoder steps see only last encoder state')\n",
    "plt.colorbar(im1, ax=ax1)\n",
    "\n",
    "# With Bahdanau attention\n",
    "im2 = ax2.imshow(attention_matrix, cmap='Blues', aspect='auto', vmin=0, vmax=1)\n",
    "ax2.set_xlabel('Input Position')\n",
    "ax2.set_ylabel('Output Position')\n",
    "ax2.set_title('With Bahdanau Attention\\nEach decoder step attends to different positions')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Difference:\")\n",
    "print(\"  Without attention: Information bottleneck at last encoder state\")\n",
    "print(\"  With attention: Dynamic access to all encoder states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bahdanau_score(s, h, W_a, U_a, v_a):\n",
    "    \"\"\"Additive/Concat attention (Bahdanau)\"\"\"\n",
    "    return np.dot(v_a.T, np.tanh(np.dot(W_a, s) + np.dot(U_a, h)))[0, 0]\n",
    "\n",
    "def dot_product_score(s, h):\n",
    "    \"\"\"Dot product attention (Luong)\"\"\"\n",
    "    return np.dot(s.T, h)[0, 0]\n",
    "\n",
    "def scaled_dot_product_score(s, h):\n",
    "    \"\"\"Scaled dot product (Transformer-style)\"\"\"\n",
    "    d_k = s.shape[0]\n",
    "    return np.dot(s.T, h)[0, 0] / np.sqrt(d_k)\n",
    "\n",
    "# Compare scoring functions\n",
    "s = np.random.randn(16, 1)\n",
    "h = np.random.randn(32, 1)\n",
    "W_a = np.random.randn(16, 16)\n",
    "U_a = np.random.randn(16, 32)\n",
    "v_a = np.random.randn(1, 16)\n",
    "\n",
    "print(\"Attention Score Functions:\")\n",
    "print(f\"  Bahdanau (additive): score = v^T tanh(W*s + U*h)\")\n",
    "print(f\"  Dot product: score = s^T h\")\n",
    "print(f\"  Scaled dot product: score = s^T h / sqrt(d_k)\")\n",
    "print(f\"\\nBahdanau is more expressive but has more parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### The Problem Attention Solves:\n",
    "- **Fixed-length context**: Entire input compressed to single vector\n",
    "- **Information bottleneck**: Long sequences lose information\n",
    "- **No alignment**: Decoder doesn't know which input to focus on\n",
    "\n",
    "### Bahdanau Attention Innovation:\n",
    "1. **Dynamic context**: Different for each decoder step\n",
    "2. **Soft alignment**: Learns to align source and target\n",
    "3. **All encoder states**: Decoder has access to all, not just last\n",
    "\n",
    "### How It Works:\n",
    "```\n",
    "1. Encoder produces annotations h_1, ..., h_T\n",
    "2. For each decoder step i:\n",
    "   a. Compute attention scores: e_ij = score(s_{i-1}, h_j)\n",
    "   b. Normalize to weights: α_ij = softmax(e_ij)\n",
    "   c. Compute context: c_i = Σ α_ij * h_j\n",
    "   d. Generate output: y_i = f(s_i, c_i, y_{i-1})\n",
    "```\n",
    "\n",
    "### Bahdanau vs Luong Attention:\n",
    "| Feature | Bahdanau (2014) | Luong (2015) |\n",
    "|---------|----------------|---------------|\n",
    "| Score | Additive: v·tanh(W·s + U·h) | Multiplicative: s·h |\n",
    "| When | Uses s_{i-1} (previous) | Uses s_i (current) |\n",
    "| Global/Local | Global only | Both options |\n",
    "\n",
    "### Mathematical Formulation:\n",
    "\n",
    "**Attention score (alignment model)**:\n",
    "$$e_{ij} = v_a^T \\tanh(W_a s_{i-1} + U_a h_j)$$\n",
    "\n",
    "**Attention weights**:\n",
    "$$\\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})}$$\n",
    "\n",
    "**Context vector**:\n",
    "$$c_i = \\sum_{j=1}^{T_x} \\alpha_{ij} h_j$$\n",
    "\n",
    "**Decoder**:\n",
    "$$s_i = f(s_{i-1}, y_{i-1}, c_i)$$\n",
    "$$p(y_i | y_{<i}, x) = g(s_i, y_{i-1}, c_i)$$\n",
    "\n",
    "### Impact:\n",
    "- **Revolutionized NMT**: BLEU scores jumped significantly\n",
    "- **Interpretability**: Can visualize alignments\n",
    "- **Foundation for Transformers**: Pure attention (2017)\n",
    "- **Beyond NMT**: Used in vision, speech, etc.\n",
    "\n",
    "### Why It Worked:\n",
    "1. **Solves bottleneck**: Variable-length context\n",
    "2. **Learns alignment**: No need for separate alignment model\n",
    "3. **Differentiable**: End-to-end training\n",
    "4. **Works for long sequences**: Attention doesn't decay\n",
    "\n",
    "### Modern Perspective:\n",
    "- Transformers use **self-attention** (attend to same sequence)\n",
    "- Scaled dot-product is now standard (simpler, faster)\n",
    "- Multi-head attention captures different relationships\n",
    "- But Bahdanau's core idea remains: **attend to what's relevant**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
