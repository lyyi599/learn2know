{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 4: Recurrent Neural Network Regularization\n",
    "## Wojciech Zaremba, Ilya Sutskever, Oriol Vinyals (2014)\n",
    "\n",
    "### Dropout for RNNs\n",
    "\n",
    "Key insight: Apply dropout to **non-recurrent connections only**, not recurrent connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# Torch-Numpy compatibility helpers\n",
    "if not hasattr(torch.Tensor, 'copy'):\n",
    "    torch.Tensor.copy = torch.Tensor.clone\n",
    "\n",
    "def _astype(self, dtype):\n",
    "    \"\"\"\n",
    "    Compute  astype.\n",
    "    \n",
    "    Args:\n",
    "        dtype: Input parameter.\n",
    "    Returns:\n",
    "        Computed result.\n",
    "    \"\"\"\n",
    "    if dtype is float:\n",
    "        return self.float()\n",
    "    if dtype is int:\n",
    "        return self.int()\n",
    "    return self.to(dtype)\n",
    "\n",
    "if not hasattr(torch.Tensor, 'astype'):\n",
    "    torch.Tensor.astype = _astype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout(x, dropout_rate=0.5, training=True):\n",
    "    \"\"\"\n",
    "    Compute dropout.\n",
    "    \n",
    "    Args:\n",
    "        x: Input data.\n",
    "        dropout_rate: Rate parameter.\n",
    "        training: Input parameter.\n",
    "    Returns:\n",
    "        Computed result.\n",
    "    \"\"\"\n",
    "    if not training or dropout_rate == 0:\n",
    "        return x\n",
    "    \n",
    "    # Inverted dropout (scale during training)\n",
    "    mask = (torch.rand(*x.shape) > dropout_rate).astype(float)\n",
    "    return x * mask / (1 - dropout_rate)\n",
    "\n",
    "# Test dropout\n",
    "x = torch.ones((5, 1))\n",
    "print(\"Original:\", x.T)\n",
    "print(\"With dropout (p=0.5):\", dropout(x, 0.5).T)\n",
    "print(\"With dropout (p=0.5):\", dropout(x, 0.5).T)\n",
    "print(\"Test mode:\", dropout(x, 0.5, training=False).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with Proper Dropout\n",
    "\n",
    "**Key**: Dropout on **inputs** and **outputs**, NOT on recurrent connections!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWithDropout:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize the instance.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Size of input.\n",
    "            hidden_size: Size of hidden.\n",
    "            output_size: Size of output.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Weights\n",
    "        self.W_xh = torch.randn(hidden_size, input_size) * 0.01\n",
    "        self.W_hh = torch.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.W_hy = torch.randn(output_size, hidden_size) * 0.01\n",
    "        self.bh = torch.zeros((hidden_size, 1))\n",
    "        self.by = torch.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, inputs, dropout_rate=0.0, training=True):\n",
    "        \"\"\"\n",
    "        Run the forward pass value.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input data.\n",
    "            dropout_rate: Rate parameter.\n",
    "            training: Input parameter.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        h = torch.zeros((self.hidden_size, 1))\n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        \n",
    "        for x in inputs:\n",
    "            # Apply dropout to INPUT\n",
    "            x_dropped = dropout(x, dropout_rate, training)\n",
    "            \n",
    "            # RNN update (NO dropout on recurrent connection)\n",
    "            h = torch.tanh(\n",
    "                torch.matmul(self.W_xh, x_dropped) +  # Dropout HERE\n",
    "                torch.matmul(self.W_hh, h) +           # NO dropout HERE\n",
    "                self.bh\n",
    "            )\n",
    "            \n",
    "            # Apply dropout to HIDDEN state before output\n",
    "            h_dropped = dropout(h, dropout_rate, training)\n",
    "            \n",
    "            # Output\n",
    "            y = torch.matmul(self.W_hy, h_dropped) + self.by  # Dropout HERE\n",
    "            \n",
    "            outputs.append(y)\n",
    "            hidden_states.append(h)\n",
    "        \n",
    "        return outputs, hidden_states\n",
    "\n",
    "# Test\n",
    "rnn = RNNWithDropout(input_size=10, hidden_size=20, output_size=10)\n",
    "test_inputs = [torch.randn(10, 1) for _ in range(5)]\n",
    "\n",
    "outputs_train, _ = rnn.forward(test_inputs, dropout_rate=0.5, training=True)\n",
    "outputs_test, _ = rnn.forward(test_inputs, dropout_rate=0.5, training=False)\n",
    "\n",
    "print(f\"Training output[0] mean: {outputs_train[0].mean():.4f}\")\n",
    "print(f\"Test output[0] mean: {outputs_test[0].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Dropout\n",
    "\n",
    "**Key innovation**: Use **same** dropout mask across all timesteps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNWithVariationalDropout:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        \"\"\"\n",
    "        Initialize the instance.\n",
    "        \n",
    "        Args:\n",
    "            input_size: Size of input.\n",
    "            hidden_size: Size of hidden.\n",
    "            output_size: Size of output.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Weights (same as before)\n",
    "        self.W_xh = torch.randn(hidden_size, input_size) * 0.01\n",
    "        self.W_hh = torch.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.W_hy = torch.randn(output_size, hidden_size) * 0.01\n",
    "        self.bh = torch.zeros((hidden_size, 1))\n",
    "        self.by = torch.zeros((output_size, 1))\n",
    "    \n",
    "    def forward(self, inputs, dropout_rate=0.0, training=True):\n",
    "        \"\"\"\n",
    "        Run the forward pass value.\n",
    "        \n",
    "        Args:\n",
    "            inputs: Input data.\n",
    "            dropout_rate: Rate parameter.\n",
    "            training: Input parameter.\n",
    "        Returns:\n",
    "            Computed result.\n",
    "        \"\"\"\n",
    "        h = torch.zeros((self.hidden_size, 1))\n",
    "        outputs = []\n",
    "        hidden_states = []\n",
    "        \n",
    "        # Generate masks ONCE for entire sequence\n",
    "        if training and dropout_rate > 0:\n",
    "            input_mask = (torch.rand(self.input_size, 1) > dropout_rate).astype(float) / (1 - dropout_rate)\n",
    "            hidden_mask = (torch.rand(self.hidden_size, 1) > dropout_rate).astype(float) / (1 - dropout_rate)\n",
    "        else:\n",
    "            input_mask = torch.ones((self.input_size, 1))\n",
    "            hidden_mask = torch.ones((self.hidden_size, 1))\n",
    "        \n",
    "        for x in inputs:\n",
    "            # Apply SAME mask to each input\n",
    "            x_dropped = x * input_mask\n",
    "            \n",
    "            # RNN update\n",
    "            h = torch.tanh(\n",
    "                torch.matmul(self.W_xh, x_dropped) +\n",
    "                torch.matmul(self.W_hh, h) +\n",
    "                self.bh\n",
    "            )\n",
    "            \n",
    "            # Apply SAME mask to each hidden state\n",
    "            h_dropped = h * hidden_mask\n",
    "            \n",
    "            # Output\n",
    "            y = torch.matmul(self.W_hy, h_dropped) + self.by\n",
    "            \n",
    "            outputs.append(y)\n",
    "            hidden_states.append(h)\n",
    "        \n",
    "        return outputs, hidden_states\n",
    "\n",
    "# Test variational dropout\n",
    "var_rnn = RNNWithVariationalDropout(input_size=10, hidden_size=20, output_size=10)\n",
    "outputs_var, _ = var_rnn.forward(test_inputs, dropout_rate=0.5, training=True)\n",
    "\n",
    "print(\"Variational dropout uses consistent masks across timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Dropout Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic sequence data\n",
    "seq_length = 20\n",
    "test_sequence = [torch.randn(10, 1) for _ in range(seq_length)]\n",
    "\n",
    "# Run with different strategies\n",
    "_, h_no_dropout = rnn.forward(test_sequence, dropout_rate=0.0, training=False)\n",
    "_, h_standard = rnn.forward(test_sequence, dropout_rate=0.5, training=True)\n",
    "_, h_variational = var_rnn.forward(test_sequence, dropout_rate=0.5, training=True)\n",
    "\n",
    "# Convert to arrays\n",
    "h_no_dropout = torch.hstack([h.flatten() for h in h_no_dropout]).T\n",
    "h_standard = torch.hstack([h.flatten() for h in h_standard]).T\n",
    "h_variational = torch.hstack([h.flatten() for h in h_variational]).T\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axes[0].imshow(h_no_dropout, cmap='RdBu', aspect='auto')\n",
    "axes[0].set_title('No Dropout')\n",
    "axes[0].set_xlabel('Hidden Unit')\n",
    "axes[0].set_ylabel('Time Step')\n",
    "\n",
    "axes[1].imshow(h_standard, cmap='RdBu', aspect='auto')\n",
    "axes[1].set_title('Standard Dropout (different masks per timestep)')\n",
    "axes[1].set_xlabel('Hidden Unit')\n",
    "axes[1].set_ylabel('Time Step')\n",
    "\n",
    "axes[2].imshow(h_variational, cmap='RdBu', aspect='auto')\n",
    "axes[2].set_title('Variational Dropout (same mask all timesteps)')\n",
    "axes[2].set_xlabel('Hidden Unit')\n",
    "axes[2].set_ylabel('Time Step')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Variational dropout shows consistent patterns (same units dropped throughout)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Placement Matters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize where dropout is applied\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Create a simple RNN diagram\n",
    "def draw_rnn_cell(ax, title, show_input_dropout, show_hidden_dropout, show_recurrent_dropout):\n",
    "    \"\"\"\n",
    "    Compute draw recurrent neural network cell.\n",
    "    \n",
    "    Args:\n",
    "        ax: Input parameter.\n",
    "        title: Input parameter.\n",
    "        show_input_dropout: Input parameter.\n",
    "        show_hidden_dropout: Input parameter.\n",
    "        show_recurrent_dropout: Input parameter.\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 10)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Draw boxes\n",
    "    # Input\n",
    "    ax.add_patch(plt.Rectangle((1, 2), 1.5, 1, fill=True, color='lightblue', ec='black'))\n",
    "    ax.text(1.75, 2.5, 'x_t', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Hidden (current)\n",
    "    ax.add_patch(plt.Rectangle((4, 4.5), 2, 2, fill=True, color='lightgreen', ec='black'))\n",
    "    ax.text(5, 5.5, 'h_t', ha='center', va='center', fontsize=12)\n",
    "    \n",
    "    # Hidden (previous)\n",
    "    ax.add_patch(plt.Rectangle((7, 4.5), 2, 2, fill=True, color='lightyellow', ec='black'))\n",
    "    ax.text(8, 5.5, 'h_{t-1}', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Output\n",
    "    ax.add_patch(plt.Rectangle((4, 7.5), 2, 1, fill=True, color='lightcoral', ec='black'))\n",
    "    ax.text(5, 8, 'y_t', ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    # Arrows\n",
    "    # Input to hidden\n",
    "    color_input = 'red' if show_input_dropout else 'black'\n",
    "    width_input = 3 if show_input_dropout else 1\n",
    "    ax.arrow(2.5, 2.5, 1.3, 2, head_width=0.3, color=color_input, lw=width_input)\n",
    "    if show_input_dropout:\n",
    "        ax.text(3.2, 3.5, 'DROPOUT', fontsize=8, color='red', fontweight='bold')\n",
    "    \n",
    "    # Recurrent\n",
    "    color_rec = 'red' if show_recurrent_dropout else 'black'\n",
    "    width_rec = 3 if show_recurrent_dropout else 1\n",
    "    ax.arrow(7, 5.5, -0.8, 0, head_width=0.3, color=color_rec, lw=width_rec)\n",
    "    if show_recurrent_dropout:\n",
    "        ax.text(6.5, 6.2, 'DROPOUT', fontsize=8, color='red', fontweight='bold')\n",
    "    \n",
    "    # Hidden to output\n",
    "    color_hidden = 'red' if show_hidden_dropout else 'black'\n",
    "    width_hidden = 3 if show_hidden_dropout else 1\n",
    "    ax.arrow(5, 6.6, 0, 0.7, head_width=0.3, color=color_hidden, lw=width_hidden)\n",
    "    if show_hidden_dropout:\n",
    "        ax.text(5.5, 7, 'DROPOUT', fontsize=8, color='red', fontweight='bold')\n",
    "\n",
    "# Wrong: dropout everywhere\n",
    "draw_rnn_cell(axes[0, 0], 'WRONG: Dropout Everywhere\\n(Disrupts temporal flow)', \n",
    "             show_input_dropout=True, show_hidden_dropout=True, show_recurrent_dropout=True)\n",
    "\n",
    "# Wrong: only recurrent\n",
    "draw_rnn_cell(axes[0, 1], 'WRONG: Only Recurrent\\n(Loses gradient flow)', \n",
    "             show_input_dropout=False, show_hidden_dropout=False, show_recurrent_dropout=True)\n",
    "\n",
    "# Correct: Zaremba et al.\n",
    "draw_rnn_cell(axes[1, 0], 'CORRECT: Zaremba et al.\\n(Input & Output only)', \n",
    "             show_input_dropout=True, show_hidden_dropout=True, show_recurrent_dropout=False)\n",
    "\n",
    "# No dropout\n",
    "draw_rnn_cell(axes[1, 1], 'Baseline: No Dropout\\n(May overfit)', \n",
    "             show_input_dropout=False, show_hidden_dropout=False, show_recurrent_dropout=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### The Problem:\n",
    "- Naive dropout on RNNs doesn't work well\n",
    "- Dropping recurrent connections disrupts temporal information flow\n",
    "- Standard dropout changes mask every timestep (noisy)\n",
    "\n",
    "### Zaremba et al. Solution:\n",
    "\n",
    "**Apply dropout to:**\n",
    "- ✅ Input-to-hidden connections (W_xh)\n",
    "- ✅ Hidden-to-output connections (W_hy)\n",
    "\n",
    "**Do NOT apply to:**\n",
    "- ❌ Recurrent connections (W_hh)\n",
    "\n",
    "### Variational Dropout:\n",
    "- Use **same dropout mask** for all timesteps\n",
    "- More stable than changing mask\n",
    "- Better theoretical justification (Bayesian)\n",
    "\n",
    "### Results:\n",
    "- Significant improvement on language modeling\n",
    "- Penn Treebank: Test perplexity improved from 78.4 to 68.7\n",
    "- Works with LSTMs and GRUs too\n",
    "\n",
    "### Implementation Tips:\n",
    "1. Use higher dropout rates (0.5-0.7) than feedforward nets\n",
    "2. Apply dropout in **both** directions for bidirectional RNNs\n",
    "3. Can stack multiple LSTM layers with dropout between them\n",
    "4. Variational dropout: generate mask once per sequence\n",
    "\n",
    "### Why It Works:\n",
    "- Preserves temporal dependencies (no dropout on recurrence)\n",
    "- Regularizes non-temporal transformations\n",
    "- Forces robustness to missing input features\n",
    "- Consistent masks (variational) reduce variance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}