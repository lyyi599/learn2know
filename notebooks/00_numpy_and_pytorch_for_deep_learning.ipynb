{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87de4353",
   "metadata": {},
   "source": [
    "# NumPy and Pytorch for Deep Learning\n",
    "**Goal:** Cover the *minimum* NumPy required to read and run deep learning code implemented purely with NumPy (e.g., linear layers, softmax, loss computation, batch reductions, broadcasting, etc.) and corresponding pytorch knowledge.\n",
    "\n",
    "> This notebook intentionally avoids advanced/rare NumPy features.  \n",
    "> Focus: **shape**, **indexing**, **broadcasting**, **matmul**, **axis/keepdims**, **numerical stability**, **random init**, **tiny forward+loss demo**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffa0b93",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a92e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Recommended: deterministic RNG for reproducibility\n",
    "rng = np.random.default_rng(seed=42)\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "print(\"NumPy version:\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eec7b4",
   "metadata": {},
   "source": [
    "## 1) Arrays, dtype, shape (the #1 survival skill)\n",
    "\n",
    "Deep learning NumPy code is essentially **array programs**.  \n",
    "Tracking `shape` consistently makes it possible to read such repos.\n",
    "\n",
    "Common conventions:\n",
    "- `X.shape == (B, D)` : batch size `B`, feature dim `D`\n",
    "- `logits.shape == (B, C)` : `C` classes\n",
    "- sequences often: `(B, S, D)` : sequence length `S`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e490fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[1.0, 2.0, 3.0],\n",
    "              [4.0, 5.0, 6.0]], dtype=np.float32)\n",
    "\n",
    "print(\"x:\\n\", x)\n",
    "print(\"dtype:\", x.dtype)\n",
    "print(\"shape:\", x.shape)   # (rows, cols)\n",
    "print(\"ndim:\", x.ndim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b10ba5",
   "metadata": {},
   "source": [
    "### Mini rule\n",
    "> **The gradient of a parameter has the same shape as the parameter.**  \n",
    "Even when backprop is not implemented in this notebook, this rule is useful for debugging later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9ac211",
   "metadata": {},
   "source": [
    "## 2) Indexing & slicing: batch selection, feature selection, and the `np.arange` trick\n",
    "\n",
    "Most code needs:\n",
    "- pick a subset of samples (mini-batch)\n",
    "- pick some feature columns\n",
    "- pick per-sample correct-class log-probability with `np.arange(B)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196d9813",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.arange(24).reshape(4, 6)  # 4 samples, 6 features\n",
    "print(\"X:\\n\", X)\n",
    "print(\"X shape:\", X.shape)\n",
    "\n",
    "# (1) Select one sample (note: dimension drops!)\n",
    "i = 2\n",
    "xi = X[i]              # shape (6,)\n",
    "print(\"\\nX[i] shape:\", xi.shape, \"->\", xi)\n",
    "\n",
    "# (2) Keep 2D shape by slicing\n",
    "xi2d = X[i:i+1]         # shape (1, 6)\n",
    "print(\"X[i:i+1] shape:\", xi2d.shape)\n",
    "\n",
    "# (3) Select first n samples\n",
    "X_small = X[:2]         # shape (2, 6)\n",
    "print(\"\\nX[:2] shape:\", X_small.shape)\n",
    "\n",
    "# (4) Select specific feature columns\n",
    "X_feat = X[:, [0, 2, 4]]   # shape (4, 3)\n",
    "print(\"X[:, [0,2,4]] shape:\", X_feat.shape)\n",
    "print(X_feat)\n",
    "\n",
    "# (5) Last column (often labels are last column in datasets)\n",
    "last_col = X[:, -1]       # shape (4,)\n",
    "print(\"\\nX[:, -1] shape:\", last_col.shape, \"->\", last_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901a7681",
   "metadata": {},
   "source": [
    "### `np.arange(B)` indexing (super common in cross-entropy)\n",
    "When `log_probs` has shape `(B, C)` and labels `y` has shape `(B,)`, the following selects correct-class values per sample:\n",
    "```python\n",
    "log_probs[np.arange(B), y]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f2089",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C = 4, 3\n",
    "log_probs = rng.standard_normal((B, C))\n",
    "y = np.array([0, 2, 1, 2])  # labels\n",
    "\n",
    "picked = log_probs[np.arange(B), y]   # only the label probs are chosen\n",
    "print(\"log_probs:\\n\", log_probs)\n",
    "print(\"y:\", y)\n",
    "print(\"picked shape:\", picked.shape)\n",
    "print(\"picked (correct class per sample):\", picked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c62ed",
   "metadata": {},
   "source": [
    "## 3) reshape & transpose: align shapes for math\n",
    "\n",
    "Two main tools:\n",
    "- `reshape(...)` : change how data is viewed (no element count change)\n",
    "- `transpose(...)` / `.T` : swap axes (very common in attention/sequence code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4df741",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rng.standard_normal((4, 6))\n",
    "\n",
    "# reshape: flatten then restore\n",
    "flat = X.reshape(-1)\n",
    "X_back = flat.reshape(4, 6)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"flat shape:\", flat.shape)\n",
    "print(\"X_back shape:\", X_back.shape)\n",
    "\n",
    "# transpose: matrix transpose\n",
    "Xt = X.T\n",
    "print(\"X.T shape:\", Xt.shape)\n",
    "\n",
    "# general transpose: (B, S, D) -> (S, B, D)\n",
    "A = rng.standard_normal((2, 3, 4))   # B=2, S=3, D=4\n",
    "A_perm = A.transpose(1, 0, 2)\n",
    "print(\"A shape:\", A.shape, \"-> A_perm shape:\", A_perm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f6ede",
   "metadata": {},
   "source": [
    "## 4) Broadcasting: why `X @ W + b` works\n",
    "\n",
    "Broadcasting lets arrays of different shapes interact without manual tiling.\n",
    "\n",
    "Most common patterns:\n",
    "- add bias: `(B, D) + (D,) -> (B, D)`\n",
    "- per-sample scaling: `(B, D) * (B, 1) -> (B, D)`\n",
    "- masks: `(B, D) * (B, D) -> (B, D)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9265cee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, D = 4, 6\n",
    "X = rng.standard_normal((B, D))\n",
    "\n",
    "# Case A: bias add\n",
    "b = rng.standard_normal((D,))\n",
    "Y = X + b\n",
    "print(\"Case A: X shape\", X.shape, \"+ b shape\", b.shape, \"=>\", Y.shape)\n",
    "\n",
    "# Case B: per-sample scaling (IMPORTANT: use (B,1) not (B,))\n",
    "scale = rng.random((B, 1))\n",
    "Y2 = X * scale\n",
    "print(\"Case B: X shape\", X.shape, \"* scale shape\", scale.shape, \"=>\", Y2.shape)\n",
    "\n",
    "# Case C: mask (e.g., ReLU)\n",
    "mask = (X > 0).astype(np.float32)\n",
    "relu_out = X * mask\n",
    "print(\"Case C: relu_out shape:\", relu_out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87950366",
   "metadata": {},
   "source": [
    "### Common beginner bug: `(B,)` vs `(B,1)`\n",
    "Using `scale = rng.random(B)` (shape `(B,)`) causes broadcasting along the **last** dimension and may produce unintended behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a9faa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, D = 4, 6\n",
    "X = rng.standard_normal((B, D))\n",
    "\n",
    "scale_bad = rng.random(B)     # (B,)\n",
    "scale_good = rng.random((B,1))# (B,1)\n",
    "\n",
    "# Semantics differ!\n",
    "# Y_bad = X * scale_bad         # broadcasts (B,) to (B, D) by matching last axis -> often WRONG intention\n",
    "Y_good = X * scale_good       # clearly per-sample scaling\n",
    "\n",
    "print(\"scale_bad shape:\", scale_bad.shape)\n",
    "print(\"scale_good shape:\", scale_good.shape)\n",
    "# print(\"Y_bad shape:\", Y_bad.shape, \"| Y_good shape:\", Y_good.shape)\n",
    "\n",
    "# See the difference for first row\n",
    "print(\"\\nFirst row scaling factors:\")\n",
    "print(\"bad uses scale_bad aligned to last axis (features):\", scale_bad[:6] if B>=6 else scale_bad)\n",
    "print(\"good uses one factor per sample:\", scale_good[:,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b4e72",
   "metadata": {},
   "source": [
    "## 5) Matrix multiplication: `@` as the linear layer\n",
    "\n",
    "A fully-connected layer is:\n",
    "```python\n",
    "Y = X @ W + b\n",
    "```\n",
    "where:\n",
    "- `X` : `(B, Din)`\n",
    "- `W` : `(Din, Dout)`\n",
    "- `b` : `(Dout,)`\n",
    "- `Y` : `(B, Dout)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e03892",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, Din, Dout = 4, 6, 3\n",
    "X = rng.standard_normal((B, Din))\n",
    "W = rng.standard_normal((Din, Dout))\n",
    "b = rng.standard_normal((Dout,))\n",
    "\n",
    "Y = X @ W + b\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"W shape:\", W.shape)\n",
    "print(\"b shape:\", b.shape)\n",
    "print(\"Y shape:\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98cf1e7",
   "metadata": {},
   "source": [
    "### Debug helper for matmul\n",
    "For `A @ B`, the shape rule is:\n",
    "- `A.shape[-1] == B.shape[-2]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb95d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_matmul(A, B):\n",
    "    \"\"\"\n",
    "    Compute check matmul.\n",
    "    \n",
    "    Args:\n",
    "        A: Input parameter.\n",
    "        B: Bias values or vector.\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    print(\"A shape:\", A.shape)\n",
    "    print(\"B shape:\", B.shape)\n",
    "    assert A.shape[-1] == B.shape[-2], \"matmul shape mismatch!\"\n",
    "    print(\"OK: A @ B is valid. Output shape:\", (A.shape[0], B.shape[1]))\n",
    "\n",
    "check_matmul(X, W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771d3b6",
   "metadata": {},
   "source": [
    "## 6) `axis` and `keepdims`: batch reductions, normalization, attention utilities\n",
    "\n",
    "This is *critical* for reading loss functions, softmax, layernorm, etc.\n",
    "\n",
    "Key ideas:\n",
    "- `axis=0` reduces over batch dimension (across samples)\n",
    "- `axis=1` reduces over feature dimension (per sample)\n",
    "- `keepdims=True` keeps dimensions so that broadcasting back is safe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8c5ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rng.standard_normal((4, 6))  # (B, D)\n",
    "\n",
    "mu_feat = X.mean(axis=0)         # (D,)\n",
    "mu_sample = X.mean(axis=1)       # (B,)\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"mu_feat (axis=0) shape:\", mu_feat.shape)\n",
    "print(\"mu_sample (axis=1) shape:\", mu_sample.shape)\n",
    "\n",
    "# keepdims for safe broadcasting\n",
    "mu_feat_k = X.mean(axis=0, keepdims=True)  # (1, D)\n",
    "X_centered = X - mu_feat_k                  # broadcasts (1,D)->(B,D)\n",
    "\n",
    "print(\"\\nmu_feat_k shape:\", mu_feat_k.shape)\n",
    "print(\"X_centered shape:\", X_centered.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a6d447",
   "metadata": {},
   "source": [
    "## 7) Numerical stability: stable softmax + logsumexp (must-have)\n",
    "\n",
    "Naive `np.exp(logits)` can overflow.\n",
    "\n",
    "**Stable trick:** subtract max before exp:\n",
    "```python\n",
    "x_shift = x - x.max(axis=axis, keepdims=True)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88700f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=-1):\n",
    "    \"\"\"\n",
    "    Compute softmax probabilities.\n",
    "    \n",
    "    Args:\n",
    "        x: Input data.\n",
    "        axis: Input parameter.\n",
    "    Returns:\n",
    "        Softmax probabilities.\n",
    "    \"\"\"\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    x_shift = x - x_max\n",
    "    exp_x = np.exp(x_shift)\n",
    "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
    "\n",
    "def logsumexp(x, axis=-1):\n",
    "    \"\"\"\n",
    "    Compute logsumexp.\n",
    "    \n",
    "    Args:\n",
    "        x: Input data.\n",
    "        axis: Input parameter.\n",
    "    Returns:\n",
    "        Computed result.\n",
    "    \"\"\"\n",
    "    x_max = np.max(x, axis=axis, keepdims=True)\n",
    "    return np.log(np.sum(np.exp(x - x_max), axis=axis, keepdims=True)) + x_max\n",
    "\n",
    "# Demo stability\n",
    "logits = rng.standard_normal((4, 10)) * 5\n",
    "probs = softmax(logits, axis=1)\n",
    "\n",
    "print(\"probs shape:\", probs.shape)\n",
    "print(\"row sums (should be ~1):\", probs.sum(axis=1))\n",
    "\n",
    "# Extreme values test\n",
    "x = np.array([[1000.0, 1001.0, 999.0]])\n",
    "print(\"\\nStable logsumexp:\", logsumexp(x, axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2431a364",
   "metadata": {},
   "source": [
    "## 8) Random initialization: reproducible weights\n",
    "\n",
    "Most pure NumPy DL repos initialize weights directly.\n",
    "The main building blocks are:\n",
    "- `rng.standard_normal(shape)` for Gaussian\n",
    "- `rng.uniform(low, high, size=shape)` for uniform\n",
    "- zeros for bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd2b262",
   "metadata": {},
   "outputs": [],
   "source": [
    "Din, Dout = 128, 64\n",
    "\n",
    "# Small Gaussian init\n",
    "W = rng.standard_normal((Din, Dout)) * 0.01\n",
    "b = np.zeros((Dout,), dtype=np.float32)\n",
    "\n",
    "# Xavier/Glorot uniform (simple version)\n",
    "limit = np.sqrt(6 / (Din + Dout))\n",
    "W_xavier = rng.uniform(-limit, limit, size=(Din, Dout))\n",
    "\n",
    "print(\"W std:\", W.std())\n",
    "print(\"W_xavier range approx:\", (W_xavier.min(), W_xavier.max()))\n",
    "print(\"b shape:\", b.shape, \"dtype:\", b.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfcecd8",
   "metadata": {},
   "source": [
    "## 9) Mini forward + cross-entropy loss (no backprop needed)\n",
    "\n",
    "Even when a repo provides backprop, the following should be readable:\n",
    "- logits computation: `X @ W + b`\n",
    "- stable softmax\n",
    "- cross-entropy indexing trick\n",
    "\n",
    "**Shapes:**\n",
    "- `X`: `(B, Din)`\n",
    "- `W`: `(Din, C)`\n",
    "- `logits`: `(B, C)`\n",
    "- `y`: `(B,)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40699c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, Din, C = 4, 6, 3\n",
    "X = rng.standard_normal((B, Din))\n",
    "y = rng.integers(low=0, high=C, size=(B,))\n",
    "\n",
    "W = rng.standard_normal((Din, C)) * 0.1\n",
    "b = np.zeros((C,))\n",
    "\n",
    "logits = X @ W + b\n",
    "probs = softmax(logits, axis=1)\n",
    "\n",
    "# cross-entropy loss (stable-ish): -mean(log p(correct))\n",
    "eps = 1e-12\n",
    "log_probs = np.log(probs + eps)\n",
    "\n",
    "loss = -np.mean(log_probs[np.arange(B), y])\n",
    "\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"logits shape:\", logits.shape)\n",
    "print(\"probs shape:\", probs.shape)\n",
    "print(\"y:\", y)\n",
    "print(\"loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc441727",
   "metadata": {},
   "source": [
    "## 10) Debug checklist\n",
    "\n",
    "When encountering errors or wrong outputs:\n",
    "1. Print **all shapes**: `print(X.shape, W.shape, b.shape)`\n",
    "2. For `A @ B`: check `A.shape[-1] == B.shape[-2]`\n",
    "3. Broadcasting confusion:\n",
    "   - per-sample scaling should use `(B,1)` not `(B,)`\n",
    "   - use `keepdims=True` when intending to broadcast back\n",
    "4. Numerical issues:\n",
    "   - softmax: subtract max\n",
    "   - log: add eps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba13d53",
   "metadata": {},
   "source": [
    "## 11) High-dimensional arrays: 3D and batch operations\n",
    "\n",
    "Beyond 2D arrays of shape `(B, D)`, deep learning implementations frequently use **3D tensors**, e.g. sequences of vectors with shape `(B, S, D)` (batch size, sequence length, feature dimension) or batches of matrices. This section introduces the corresponding shape conventions, batch matrix multiplication, multi-axis reduction, and broadcasting in 3D. The aim is to keep new syntax minimal so that code using such tensors can be read and written with confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71fcc96",
   "metadata": {},
   "source": [
    "### 11.1) 3D arrays: shape and indexing\n",
    "\n",
    "A 3D array has shape `(N0, N1, N2)`. In sequence-based code the convention is often `(B, S, D)`: batch size, sequence length, feature dimension. The same indexing and slicing rules as in 2D apply: a single integer index removes that dimension; a slice preserves it. The examples below illustrate selecting one sample, the last time step for all samples, and a single (sample, time) vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d5ecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, S, D = 2, 3, 4\n",
    "seq = rng.standard_normal((B, S, D))\n",
    "print(\"seq shape:\", seq.shape, \"  (B, S, D)\")\n",
    "print(\"ndim:\", seq.ndim)\n",
    "\n",
    "# Select first sample (all time steps) -> shape (S, D)\n",
    "first_sample = seq[0]\n",
    "print(\"\\nseq[0] shape:\", first_sample.shape)\n",
    "\n",
    "# Select last time step for all samples -> shape (B, D)\n",
    "last_step = seq[:, -1, :]\n",
    "print(\"seq[:, -1, :] shape:\", last_step.shape)\n",
    "\n",
    "# Select one (sample, time) vector -> shape (D,)\n",
    "single = seq[0, 1, :]\n",
    "print(\"seq[0, 1, :] shape:\", single.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed822779",
   "metadata": {},
   "source": [
    "### 11.2) Batch matrix multiplication\n",
    "\n",
    "In NumPy, the `@` operator treats the **last two dimensions** as the matrix dimensions; any leading dimensions are interpreted as a batch. Concretely:\n",
    "- `(B, M, K) @ (K, N)` → `(B, M, N)`: each of the `B` matrices of shape `(M, K)` is multiplied by the same matrix of shape `(K, N)`.\n",
    "- `(B, M, K) @ (B, K, N)` → `(B, M, N)`: for each batch index `b`, the matrix `A[b]` of shape `(M, K)` is multiplied by the matrix `B[b]` of shape `(K, N)`.\n",
    "\n",
    "**Shape rule:** For `A @ B`, the last two dimensions must satisfy `A.shape[-1] == B.shape[-2]`. Leading dimensions, if present, must either match on both sides or be absent on one side (a single matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6073d639",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, M, K, N = 2, 3, 4, 5\n",
    "A_batch = rng.standard_normal((B, M, K))\n",
    "W = rng.standard_normal((K, N))\n",
    "\n",
    "# (B, M, K) @ (K, N) -> (B, M, N): shared matrix W applied to each batch element\n",
    "Y1 = A_batch @ W\n",
    "print(\"A_batch shape:\", A_batch.shape)\n",
    "print(\"W shape:\", W.shape)\n",
    "print(\"A_batch @ W shape:\", Y1.shape)\n",
    "\n",
    "# (B, M, K) @ (B, K, N) -> (B, M, N): different right-hand matrix per batch element\n",
    "B_batch = rng.standard_normal((B, K, N))\n",
    "Y2 = A_batch @ B_batch\n",
    "print(\"\\nB_batch shape:\", B_batch.shape)\n",
    "print(\"A_batch @ B_batch shape:\", Y2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2bb4e",
   "metadata": {},
   "source": [
    "### 11.3) Reduction over multiple axes\n",
    "\n",
    "Reduction functions such as `sum`, `mean`, and `max` accept a **tuple of axes** `axis=(ax0, ax1, ...)`, so that reduction is performed over several dimensions at once. Using `keepdims=True` yields a result whose shape is suitable for broadcasting when subtracting means or normalizing over multiple dimensions (e.g. over both the sequence and feature axes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0373a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rng.standard_normal((2, 3, 4))  # (B, S, D)\n",
    "\n",
    "# Reduce over axes 1 and 2 -> shape (2,) (one scalar per batch element)\n",
    "global_per_batch = X.sum(axis=(1, 2))\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"X.sum(axis=(1,2)) shape:\", global_per_batch.shape)\n",
    "\n",
    "# Same reduction with keepdims -> shape (2, 1, 1), suitable for broadcasting\n",
    "global_k = X.sum(axis=(1, 2), keepdims=True)\n",
    "X_centered = X - global_k\n",
    "print(\"\\nglobal_k shape:\", global_k.shape)\n",
    "print(\"X_centered mean over (1,2):\", X_centered.sum(axis=(1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e07c790",
   "metadata": {},
   "source": [
    "### 11.4) Broadcasting in 3D\n",
    "\n",
    "The same broadcasting rules as in 2D apply: dimensions are aligned from the **right**, and a dimension of size 1 is broadcast to match the other array. Thus an array of shape `(B, S, D)` is compatible with `(D,)`, `(1, D)`, `(1, 1, D)`, `(B, 1, D)`, and similar shapes where dimensions either match or are 1. Common use cases: adding a shared bias of shape `(D,)` to every position, or applying a per-step or per-feature scale using shapes such as `(1, S, 1)` or `(1, 1, D)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f026c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, S, D = 2, 3, 4\n",
    "seq = rng.standard_normal((B, S, D))\n",
    "\n",
    "# Bias of shape (D,) is broadcast to every (batch, time, feature) position\n",
    "bias = rng.standard_normal((D,))\n",
    "out1 = seq + bias\n",
    "print(\"seq shape:\", seq.shape, \"+ bias shape:\", bias.shape, \"->\", out1.shape)\n",
    "\n",
    "# Scale of shape (1, S, 1): one scale per time step, broadcast over batch and feature\n",
    "scale = rng.standard_normal((1, S, 1))\n",
    "out2 = seq * scale\n",
    "print(\"seq shape:\", seq.shape, \"* scale shape:\", scale.shape, \"->\", out2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c1d97e",
   "metadata": {},
   "source": [
    "### 11.5) Example: shared linear layer over a sequence\n",
    "\n",
    "A standard pattern in sequence models is to apply a single linear layer to every time step: input shape `(B, S, Din)`, weight matrix `(Din, Dout)`, output shape `(B, S, Dout)`. Two equivalent approaches: (1) reshape the input to `(B*S, Din)`, compute `X @ W + b`, then reshape the result to `(B, S, Dout)`; (2) use batch matmul directly: `(B, S, Din) @ (Din, Dout)` yields `(B, S, Dout)` without explicit reshaping. The code below demonstrates both and verifies that the outputs coincide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b01ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, S, Din, Dout = 2, 3, 4, 5\n",
    "X_seq = rng.standard_normal((B, S, Din))\n",
    "W = rng.standard_normal((Din, Dout)) * 0.1\n",
    "b = np.zeros((Dout,))\n",
    "\n",
    "# Direct batch matmul: (B, S, Din) @ (Din, Dout) -> (B, S, Dout); bias (Dout,) broadcasts\n",
    "Y_seq = X_seq @ W + b\n",
    "print(\"X_seq shape:\", X_seq.shape)\n",
    "print(\"W shape:\", W.shape)\n",
    "print(\"Y_seq shape:\", Y_seq.shape)\n",
    "\n",
    "# Equivalent via reshape: flatten batch and sequence, matmul, reshape to (B, S, Dout)\n",
    "X_flat = X_seq.reshape(-1, Din)\n",
    "Y_flat = X_flat @ W + b\n",
    "Y_seq_alt = Y_flat.reshape(B, S, Dout)\n",
    "print(\"\\nReshape path Y_seq_alt shape:\", Y_seq_alt.shape)\n",
    "print(\"Results match:\", np.allclose(Y_seq, Y_seq_alt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afce7388",
   "metadata": {},
   "source": [
    "---\n",
    "## Part II: PyTorch basics (correspondence with NumPy above)\n",
    "\n",
    "The following sections introduce the minimal PyTorch needed to read and run typical deep learning code. The same **shape conventions** (e.g. `(B, D)`, `(B, S, D)`) and **concepts** (linear layer = matmul + bias, cross-entropy, batch operations) apply; PyTorch mainly provides a higher-level API (built-in layers, automatic differentiation) and tensor type that can run on GPU. No advanced usage (e.g. custom autograd, distributed training) is covered—focus is on basics only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a0589a",
   "metadata": {},
   "source": [
    "## 12) PyTorch setup and tensors\n",
    "\n",
    "PyTorch uses **tensors** instead of NumPy arrays. Tensors have the same notions of `shape`, `dtype`, and indexing/slicing; the main extra idea is that tensors can live on **CPU or GPU** (device) and can record operations for **automatic differentiation** (gradients). For reading code, it is enough to know: create tensors with `torch.tensor(...)` or `torch.randn(...)`, and check `x.shape`, `x.dtype`, `x.device`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a8e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Optional: set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Create a tensor (default: float32, CPU)\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [4.0, 5.0, 6.0]])\n",
    "print(\"x:\\n\", x)\n",
    "print(\"shape:\", x.shape)\n",
    "print(\"dtype:\", x.dtype)\n",
    "print(\"device:\", x.device)\n",
    "\n",
    "# Random tensor, same shape convention as NumPy: (B, D)\n",
    "B, D = 4, 6\n",
    "X = torch.randn(B, D)\n",
    "print(\"\\nX shape:\", X.shape, \"  (B, D)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197f34ee",
   "metadata": {},
   "source": [
    "### 12.1) NumPy ↔ PyTorch (optional)\n",
    "\n",
    "When data comes from NumPy (e.g. datasets), convert with `torch.from_numpy(arr)`; to get a NumPy array from a tensor, use `.numpy()` (only for CPU tensors). Shapes and indexing are the same, so the NumPy intuition from Part I carries over directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fba3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy -> PyTorch (shares memory if possible)\n",
    "a_np = np.arange(6).reshape(2, 3)\n",
    "a_pt = torch.from_numpy(a_np)\n",
    "print(\"NumPy shape:\", a_np.shape, \"-> PyTorch shape:\", a_pt.shape)\n",
    "\n",
    "# PyTorch -> NumPy (CPU only)\n",
    "b_pt = torch.randn(2, 3)\n",
    "b_np = b_pt.numpy()\n",
    "print(\"PyTorch shape:\", b_pt.shape, \"-> NumPy shape:\", b_np.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aacf1b0",
   "metadata": {},
   "source": [
    "## 13) Shape, indexing, and matmul in PyTorch\n",
    "\n",
    "The same rules as in NumPy apply: indexing and slicing are identical (`X[i]`, `X[:, -1]`, `X[i:i+1]`), and matrix multiplication is `@` or `torch.matmul`. So the mental model from sections 2–5 (batch selection, feature selection, `X @ W + b`) is unchanged; only the type is `torch.Tensor` instead of `np.ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(4, 6)\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"X[0] shape:\", X[0].shape)\n",
    "print(\"X[:, [0,2,4]] shape:\", X[:, [0, 2, 4]].shape)\n",
    "\n",
    "# Linear layer \"by hand\": same as NumPy\n",
    "B, Din, Dout = 4, 6, 3\n",
    "X = torch.randn(B, Din)\n",
    "W = torch.randn(Din, Dout)\n",
    "b = torch.randn(Dout)\n",
    "Y = X @ W + b\n",
    "print(\"\\nX @ W + b shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671fb1b3",
   "metadata": {},
   "source": [
    "## 14) Linear layer and loss in PyTorch (high-level API)\n",
    "\n",
    "Instead of manually defining `W` and `b`, PyTorch provides **modules** that encapsulate parameters and forward logic. `nn.Linear(Din, Dout)` stores weight `(Dout, Din)` and bias `(Dout,)` and computes `Y = X @ W.T + b` (note: the stored weight is `(Dout, Din)` so that the matmul is written as `X @ W.T` in raw form; the module hides this). For classification, **cross-entropy loss** is implemented as `F.cross_entropy(logits, labels)`: it expects `logits` of shape `(B, C)` and integer labels of shape `(B,)`, and does not require softmax to be applied first (it fuses log-softmax and negative log-likelihood for numerical stability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2040a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "B, Din, C = 4, 6, 3\n",
    "X = torch.randn(B, Din)\n",
    "y = torch.randint(0, C, (B,))\n",
    "\n",
    "# Built-in linear layer: replaces manual W, b\n",
    "linear = nn.Linear(Din, C)\n",
    "logits = linear(X)\n",
    "print(\"logits shape:\", logits.shape)\n",
    "\n",
    "# Cross-entropy: input logits (B, C), labels (B,); no need to apply softmax first\n",
    "loss = F.cross_entropy(logits, y)\n",
    "print(\"loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59beb2a6",
   "metadata": {},
   "source": [
    "## 15) Forward, loss, backward, and one optimizer step\n",
    "\n",
    "Training typically repeats: forward pass → compute loss → call `loss.backward()` to fill gradients → optimizer step to update parameters. The following shows one such step. Parameters must be in a module (e.g. `nn.Linear`); the optimizer is given the list of parameters to update. After `backward()`, gradients are accumulated in `param.grad`; `optimizer.step()` applies the update (e.g. SGD); `optimizer.zero_grad()` clears old gradients so the next `backward()` does not add to them. This pattern is the basis of most training loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd804f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(Din, C)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# One training step\n",
    "optimizer.zero_grad()\n",
    "logits = model(X)\n",
    "loss = F.cross_entropy(logits, y)\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(\"Loss after one step:\", F.cross_entropy(model(X), y).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5fcb0",
   "metadata": {},
   "source": [
    "## 16) 3D / sequence in PyTorch (correspondence with §11)\n",
    "\n",
    "The same shape conventions apply: a sequence batch has shape `(B, S, D)`. Applying a shared linear layer to every time step is done by passing a tensor of shape `(B, S, Din)` into `nn.Linear(Din, Dout)`; PyTorch’s linear layer accepts any number of leading dimensions and applies the same `(Din, Dout)` transformation to the last dimension. So the output is `(B, S, Dout)` without any explicit reshape—this corresponds to the batch matmul `(B, S, Din) @ (Din, Dout)` from section 11.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770e860b",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, S, Din, Dout = 2, 3, 4, 5\n",
    "X_seq = torch.randn(B, S, Din)\n",
    "linear = nn.Linear(Din, Dout)\n",
    "Y_seq = linear(X_seq)\n",
    "print(\"X_seq shape:\", X_seq.shape)\n",
    "print(\"Y_seq shape:\", Y_seq.shape)\n",
    "# Same as NumPy: (B, S, Din) -> (B, S, Dout) in one call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd414b5a",
   "metadata": {},
   "source": [
    "### Quick reference: NumPy ↔ PyTorch\n",
    "\n",
    "| Concept | NumPy (Part I) | PyTorch (Part II) |\n",
    "|--------|-----------------|-------------------|\n",
    "| Array / tensor | `np.ndarray`, `x.shape` | `torch.Tensor`, `x.shape` |\n",
    "| Linear layer | `Y = X @ W + b` (manual W, b) | `nn.Linear(Din, Dout)`, `Y = linear(X)` |\n",
    "| Cross-entropy | Manual softmax + `log_probs[np.arange(B), y]` | `F.cross_entropy(logits, y)` |\n",
    "| 3D / sequence | `(B, S, Din) @ (Din, Dout)` or reshape | `nn.Linear(Din, Dout)(X_seq)` → `(B, S, Dout)` |\n",
    "| Gradients / training | Hand-written backprop | `loss.backward()`, `optimizer.step()` |\n",
    "\n",
    "The same **shape conventions** (`(B, D)`, `(B, S, D)`, etc.) apply in both; PyTorch adds layers, automatic differentiation, and (optionally) GPU execution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemistry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}